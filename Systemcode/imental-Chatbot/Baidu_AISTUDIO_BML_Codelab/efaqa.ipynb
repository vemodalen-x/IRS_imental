{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cb1c578-0155-405d-9758-002b114620c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-25T04:14:36.792350Z",
     "iopub.status.busy": "2021-10-25T04:14:36.791963Z",
     "iopub.status.idle": "2021-10-25T04:14:41.976984Z",
     "shell.execute_reply": "2021-10-25T04:14:41.975833Z",
     "shell.execute_reply.started": "2021-10-25T04:14:36.792298Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting efaqa_corpus_zh\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/e9/bd/6c98a6d542805d1af6927a30e2a76ca8051cfa4c58ad22a0c21e8605505e/efaqa_corpus_zh-0.2.tar.gz\n",
      "Building wheels for collected packages: efaqa-corpus-zh\n",
      "  Building wheel for efaqa-corpus-zh (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for efaqa-corpus-zh: filename=efaqa_corpus_zh-0.2-cp37-none-any.whl size=8667 sha256=67dba4de54c845f81fad7a2de656f2ed0d07ca3f8c014c748aa7b5ba76dbea6f\n",
      "  Stored in directory: /home/aistudio/.cache/pip/wheels/2e/ca/eb/c6e12293e82bd4ea4b1865e835b0ad202cf8d468d8981cb386\n",
      "Successfully built efaqa-corpus-zh\n",
      "Installing collected packages: efaqa-corpus-zh\n",
      "Successfully installed efaqa-corpus-zh-0.2\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting gensim\n",
      "\u001b[?25l  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/9f/44/985c6291f160aca1257dae9b5bb62d91d0f61f12014297a2fa80e6464be1/gensim-4.1.2-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (24.1MB)\n",
      "\u001b[K     |████████████████████████████████| 24.1MB 24.4MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting smart-open>=1.8.1 (from gensim)\n",
      "\u001b[?25l  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/cd/11/05f68ea934c24ade38e95ac30a38407767787c4e3db1776eae4886ad8c95/smart_open-5.2.1-py3-none-any.whl (58kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 35.8MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy>=0.18.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from gensim) (1.6.3)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from gensim) (1.20.3)\n",
      "Installing collected packages: smart-open, gensim\n",
      "Successfully installed gensim-4.1.2 smart-open-5.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install efaqa_corpus_zh\n",
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d4c4a45-fcb6-4189-8c27-5f29a95e6fd9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-25T04:14:49.267790Z",
     "iopub.status.busy": "2021-10-25T04:14:49.267337Z",
     "iopub.status.idle": "2021-10-25T04:14:51.196055Z",
     "shell.execute_reply": "2021-10-25T04:14:51.195019Z",
     "shell.execute_reply.started": "2021-10-25T04:14:49.267727Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import os\n",
    "import efaqa_corpus_zh\n",
    "#dataset from https://github.com/chatopera/efaqa-corpus-zh\n",
    "data = list(efaqa_corpus_zh.load('dataset/efaqa-corpus-zh.utf8.gz'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc18bc8a-1abc-41cd-b813-97e971971f5b",
   "metadata": {},
   "source": [
    "# Explore the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "42795632-9d1e-49c3-8341-c6013f1a9835",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-24T12:59:17.931982Z",
     "iopub.status.busy": "2021-10-24T12:59:17.931600Z",
     "iopub.status.idle": "2021-10-24T12:59:18.050014Z",
     "shell.execute_reply": "2021-10-24T12:59:18.047997Z",
     "shell.execute_reply.started": "2021-10-24T12:59:17.931926Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_post 20000\n",
      "num_sentence 207745\n",
      "each 10.38725\n",
      "average 17.68224987364317\n"
     ]
    }
   ],
   "source": [
    "num_post = len(data)\n",
    "num_sentence = sum([len(post[\"chats\"]) for post in data])\n",
    "avg_num_word = sum(len(chat[\"value\"]) for post in data for chat in post[\"chats\"]) / num_sentence\n",
    "\n",
    "print(\"num_post\", num_post)\n",
    "print(\"num_sentence\", num_sentence)\n",
    "print(\"each\", num_sentence / num_post)\n",
    "print(\"average\", avg_num_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e6f749bc-f3b6-45da-85af-80b75c97c188",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-24T12:59:20.556881Z",
     "iopub.status.busy": "2021-10-24T12:59:20.556513Z",
     "iopub.status.idle": "2021-10-24T12:59:20.565357Z",
     "shell.execute_reply": "2021-10-24T12:59:20.564162Z",
     "shell.execute_reply.started": "2021-10-24T12:59:20.556827Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'chats': [{'label': {'knowledge': False, 'negative': False, 'question': True}, 'sender': 'audience', 'time': '11:02:45', 'type': 'textMessage', 'value': '这样的议论是针对谁呢？'}, {'label': {'knowledge': False, 'negative': False, 'question': False}, 'sender': 'audience', 'time': '11:08:38', 'type': 'textMessage', 'value': '我也是一个从小被这样训到大的女生哦，总会被指责缺心少肺、没心眼儿、没眼力见儿、看不出来眉眼高低等等。不过在我成长一段时间之后，发现这件事情其实很简单，也没有什么大的问题。如果你愿意的话，可以找我聊聊，倾诉一下你遇到的事情，希望能够帮到你。我是树洞小太阳，欢迎你来找我玩❤'}, {'label': {'knowledge': False, 'negative': False, 'question': False}, 'sender': 'audience', 'time': '11:15:17', 'type': 'textMessage', 'value': '好惨'}, {'label': {'knowledge': False, 'negative': False, 'question': False}, 'sender': 'audience', 'time': '11:15:35', 'type': 'textMessage', 'value': '原生家庭也这么对你吗'}], 'date': '2020-03-02 11:01:08', 'label': {'s1': '1.13', 's2': '2.7', 's3': '3.4'}, 'owner': '匿名', 'title': '女 听过别人最多的议论就是干啥啥不行不长心眼没有脑子'}\n",
      "dict_keys(['chats', 'date', 'label', 'owner', 'title'])\n",
      "{'knowledge': False, 'negative': False, 'question': True}\n"
     ]
    }
   ],
   "source": [
    "print(data[0])\n",
    "print(data[500].keys())\n",
    "print(data[0][\"chats\"][0]['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9c657a-9e4f-42b6-9689-ccb453268c93",
   "metadata": {},
   "source": [
    "# Try ML to do classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bc02e1c5-5be8-414f-97c2-07bb922601c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-24T12:11:40.133891Z",
     "iopub.status.busy": "2021-10-24T12:11:40.133543Z",
     "iopub.status.idle": "2021-10-24T12:11:42.147916Z",
     "shell.execute_reply": "2021-10-24T12:11:42.146896Z",
     "shell.execute_reply.started": "2021-10-24T12:11:40.133835Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正克隆到 'stopwords'...\n",
      "remote: Enumerating objects: 22, done.\n",
      "remote: Total 22 (delta 0), reused 0 (delta 0), pack-reused 22\n",
      "展开对象中: 100% (22/22), 完成.\n",
      "检查连接... 完成。\n"
     ]
    }
   ],
   "source": [
    "#stop wrods for chinese\n",
    "!git clone https://github.com/goto456/stopwords.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "cf868d72-5af8-4ca2-8d89-b260ee990ffc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-24T13:00:01.334215Z",
     "iopub.status.busy": "2021-10-24T13:00:01.333827Z",
     "iopub.status.idle": "2021-10-24T13:01:34.180184Z",
     "shell.execute_reply": "2021-10-24T13:01:34.179323Z",
     "shell.execute_reply.started": "2021-10-24T13:00:01.334158Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/20000 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      "  0%|          | 26/20000 [00:00<01:17, 259.21it/s]\n",
      "\n",
      "\n",
      "  0%|          | 63/20000 [00:00<01:11, 279.81it/s]\n",
      "\n",
      "\n",
      "  0%|          | 87/20000 [00:00<01:16, 261.99it/s]\n",
      "\n",
      "\n",
      "  1%|          | 112/20000 [00:00<01:17, 258.17it/s]\n",
      "\n",
      "\n",
      "  1%|          | 148/20000 [00:00<01:10, 282.05it/s]\n",
      "\n",
      "\n",
      "  1%|          | 174/20000 [00:00<01:12, 272.39it/s]\n",
      "\n",
      "\n",
      "  1%|          | 199/20000 [00:00<01:17, 255.98it/s]\n",
      "\n",
      "\n",
      "  1%|          | 224/20000 [00:00<01:20, 246.71it/s]\n",
      "\n",
      "\n",
      "  1%|▏         | 252/20000 [00:00<01:17, 255.53it/s]\n",
      "\n",
      "\n",
      "  1%|▏         | 279/20000 [00:01<01:23, 237.55it/s]\n",
      "\n",
      "\n",
      "  2%|▏         | 303/20000 [00:01<01:31, 215.70it/s]\n",
      "\n",
      "\n",
      "  2%|▏         | 327/20000 [00:01<01:29, 219.11it/s]\n",
      "\n",
      "\n",
      "  2%|▏         | 355/20000 [00:01<01:25, 229.02it/s]\n",
      "\n",
      "\n",
      "  2%|▏         | 380/20000 [00:01<01:24, 231.33it/s]\n",
      "\n",
      "\n",
      "  2%|▏         | 404/20000 [00:01<01:32, 211.03it/s]\n",
      "\n",
      "\n",
      "  2%|▏         | 431/20000 [00:01<01:28, 222.12it/s]\n",
      "\n",
      "\n",
      "  2%|▏         | 455/20000 [00:01<01:26, 227.17it/s]\n",
      "\n",
      "\n",
      "  2%|▏         | 484/20000 [00:01<01:20, 242.92it/s]\n",
      "\n",
      "\n",
      "  3%|▎         | 512/20000 [00:02<01:18, 249.65it/s]\n",
      "\n",
      "\n",
      "  3%|▎         | 538/20000 [00:02<01:27, 221.79it/s]\n",
      "\n",
      "\n",
      "  3%|▎         | 563/20000 [00:02<01:25, 228.30it/s]\n",
      "\n",
      "\n",
      "  3%|▎         | 592/20000 [00:02<01:19, 243.62it/s]\n",
      "\n",
      "\n",
      "  3%|▎         | 618/20000 [00:02<01:27, 220.64it/s]\n",
      "\n",
      "\n",
      "  3%|▎         | 642/20000 [00:02<01:39, 193.82it/s]\n",
      "\n",
      "\n",
      "  3%|▎         | 664/20000 [00:02<01:44, 184.62it/s]\n",
      "\n",
      "\n",
      "  3%|▎         | 687/20000 [00:02<01:39, 195.01it/s]\n",
      "\n",
      "\n",
      "  4%|▎         | 708/20000 [00:03<01:42, 188.23it/s]\n",
      "\n",
      "\n",
      "  4%|▎         | 734/20000 [00:03<01:34, 204.70it/s]\n",
      "\n",
      "\n",
      "  4%|▍         | 758/20000 [00:03<01:38, 195.68it/s]\n",
      "\n",
      "\n",
      "  4%|▍         | 779/20000 [00:03<01:36, 198.69it/s]\n",
      "\n",
      "\n",
      "  4%|▍         | 805/20000 [00:03<01:30, 213.27it/s]\n",
      "\n",
      "\n",
      "  4%|▍         | 827/20000 [00:03<01:39, 192.37it/s]\n",
      "\n",
      "\n",
      "  4%|▍         | 848/20000 [00:03<01:42, 186.29it/s]\n",
      "\n",
      "\n",
      "  4%|▍         | 868/20000 [00:03<01:51, 171.27it/s]\n",
      "\n",
      "\n",
      "  4%|▍         | 890/20000 [00:04<01:46, 179.12it/s]\n",
      "\n",
      "\n",
      "  5%|▍         | 912/20000 [00:04<01:41, 188.15it/s]\n",
      "\n",
      "\n",
      "  5%|▍         | 938/20000 [00:04<01:33, 204.71it/s]\n",
      "\n",
      "\n",
      "  5%|▍         | 960/20000 [00:04<01:36, 197.63it/s]\n",
      "\n",
      "\n",
      "  5%|▍         | 981/20000 [00:04<01:44, 181.62it/s]\n",
      "\n",
      "\n",
      "  5%|▌         | 1001/20000 [00:04<01:44, 182.46it/s]\n",
      "\n",
      "\n",
      "  5%|▌         | 1023/20000 [00:04<01:40, 189.24it/s]\n",
      "\n",
      "\n",
      "  5%|▌         | 1043/20000 [00:04<01:41, 186.67it/s]\n",
      "\n",
      "\n",
      "  5%|▌         | 1065/20000 [00:04<01:38, 193.17it/s]\n",
      "\n",
      "\n",
      "  5%|▌         | 1085/20000 [00:05<01:39, 190.91it/s]\n",
      "\n",
      "\n",
      "  6%|▌         | 1105/20000 [00:05<01:53, 167.13it/s]\n",
      "\n",
      "\n",
      "  6%|▌         | 1123/20000 [00:05<01:57, 160.50it/s]\n",
      "\n",
      "\n",
      "  6%|▌         | 1140/20000 [00:05<02:13, 140.75it/s]\n",
      "\n",
      "\n",
      "  6%|▌         | 1158/20000 [00:05<02:05, 149.85it/s]\n",
      "\n",
      "\n",
      "  6%|▌         | 1174/20000 [00:05<02:17, 136.44it/s]\n",
      "\n",
      "\n",
      "  6%|▌         | 1189/20000 [00:05<02:18, 136.30it/s]\n",
      "\n",
      "\n",
      "  6%|▌         | 1204/20000 [00:05<02:23, 130.99it/s]\n",
      "\n",
      "\n",
      "  6%|▌         | 1218/20000 [00:06<02:21, 133.01it/s]\n",
      "\n",
      "\n",
      "  6%|▌         | 1232/20000 [00:06<02:36, 120.06it/s]\n",
      "\n",
      "\n",
      "  6%|▌         | 1245/20000 [00:06<02:32, 122.63it/s]\n",
      "\n",
      "\n",
      "  6%|▋         | 1259/20000 [00:06<02:27, 127.34it/s]\n",
      "\n",
      "\n",
      "  6%|▋         | 1273/20000 [00:06<02:24, 129.16it/s]\n",
      "\n",
      "\n",
      "  6%|▋         | 1290/20000 [00:06<02:17, 136.02it/s]\n",
      "\n",
      "\n",
      "  7%|▋         | 1304/20000 [00:06<02:18, 134.58it/s]\n",
      "\n",
      "\n",
      "  7%|▋         | 1318/20000 [00:06<02:17, 135.43it/s]\n",
      "\n",
      "\n",
      "  7%|▋         | 1333/20000 [00:06<02:14, 138.75it/s]\n",
      "\n",
      "\n",
      "  7%|▋         | 1347/20000 [00:07<02:27, 126.87it/s]\n",
      "\n",
      "\n",
      "  7%|▋         | 1360/20000 [00:07<02:35, 120.17it/s]\n",
      "\n",
      "\n",
      "  7%|▋         | 1373/20000 [00:07<02:33, 121.38it/s]\n",
      "\n",
      "\n",
      "  7%|▋         | 1386/20000 [00:07<02:33, 121.50it/s]\n",
      "\n",
      "\n",
      "  7%|▋         | 1399/20000 [00:07<02:45, 112.60it/s]\n",
      "\n",
      "\n",
      "  7%|▋         | 1417/20000 [00:07<02:26, 126.59it/s]\n",
      "\n",
      "\n",
      "  7%|▋         | 1435/20000 [00:07<02:14, 138.10it/s]\n",
      "\n",
      "\n",
      "  7%|▋         | 1452/20000 [00:07<02:06, 146.13it/s]\n",
      "\n",
      "\n",
      "  7%|▋         | 1470/20000 [00:07<02:04, 148.77it/s]\n",
      "\n",
      "\n",
      "  7%|▋         | 1495/20000 [00:08<01:49, 168.98it/s]\n",
      "\n",
      "\n",
      "  8%|▊         | 1517/20000 [00:08<01:42, 180.83it/s]\n",
      "\n",
      "\n",
      "  8%|▊         | 1537/20000 [00:08<01:53, 162.46it/s]\n",
      "\n",
      "\n",
      "  8%|▊         | 1555/20000 [00:08<01:56, 158.36it/s]\n",
      "\n",
      "\n",
      "  8%|▊         | 1572/20000 [00:08<01:57, 157.02it/s]\n",
      "\n",
      "\n",
      "  8%|▊         | 1591/20000 [00:08<01:55, 159.41it/s]\n",
      "\n",
      "\n",
      "  8%|▊         | 1615/20000 [00:08<01:43, 176.81it/s]\n",
      "\n",
      "\n",
      "  8%|▊         | 1634/20000 [00:08<01:42, 179.31it/s]\n",
      "\n",
      "\n",
      "  8%|▊         | 1653/20000 [00:08<01:44, 175.17it/s]\n",
      "\n",
      "\n",
      "  8%|▊         | 1672/20000 [00:09<01:43, 177.50it/s]\n",
      "\n",
      "\n",
      "  8%|▊         | 1691/20000 [00:09<01:45, 174.22it/s]\n",
      "\n",
      "\n",
      "  9%|▊         | 1709/20000 [00:09<01:47, 169.45it/s]\n",
      "\n",
      "\n",
      "  9%|▊         | 1727/20000 [00:09<01:53, 161.31it/s]\n",
      "\n",
      "\n",
      "  9%|▊         | 1744/20000 [00:09<01:51, 163.54it/s]\n",
      "\n",
      "\n",
      "  9%|▉         | 1761/20000 [00:09<01:56, 156.54it/s]\n",
      "\n",
      "\n",
      "  9%|▉         | 1783/20000 [00:09<01:47, 170.02it/s]\n",
      "\n",
      "\n",
      "  9%|▉         | 1803/20000 [00:09<01:43, 175.73it/s]\n",
      "\n",
      "\n",
      "  9%|▉         | 1821/20000 [00:09<02:01, 149.41it/s]\n",
      "\n",
      "\n",
      "  9%|▉         | 1837/20000 [00:10<02:04, 145.39it/s]\n",
      "\n",
      "\n",
      "  9%|▉         | 1858/20000 [00:10<01:55, 157.36it/s]\n",
      "\n",
      "\n",
      "  9%|▉         | 1875/20000 [00:10<01:53, 159.62it/s]\n",
      "\n",
      "\n",
      "  9%|▉         | 1892/20000 [00:10<01:58, 153.15it/s]\n",
      "\n",
      "\n",
      " 10%|▉         | 1908/20000 [00:10<02:01, 148.90it/s]\n",
      "\n",
      "\n",
      " 10%|▉         | 1926/20000 [00:10<01:57, 154.16it/s]\n",
      "\n",
      "\n",
      " 10%|▉         | 1942/20000 [00:10<02:00, 149.88it/s]\n",
      "\n",
      "\n",
      " 10%|▉         | 1959/20000 [00:10<01:56, 154.93it/s]\n",
      "\n",
      "\n",
      " 10%|▉         | 1976/20000 [00:10<01:54, 157.67it/s]\n",
      "\n",
      "\n",
      " 10%|▉         | 1992/20000 [00:11<01:55, 155.33it/s]\n",
      "\n",
      "\n",
      " 10%|█         | 2008/20000 [00:11<02:03, 145.57it/s]\n",
      "\n",
      "\n",
      " 10%|█         | 2024/20000 [00:11<02:02, 147.18it/s]\n",
      "\n",
      "\n",
      " 10%|█         | 2039/20000 [00:11<02:07, 140.92it/s]\n",
      "\n",
      "\n",
      " 10%|█         | 2058/20000 [00:11<01:57, 152.37it/s]\n",
      "\n",
      "\n",
      " 10%|█         | 2076/20000 [00:11<01:53, 157.72it/s]\n",
      "\n",
      "\n",
      " 10%|█         | 2094/20000 [00:11<01:54, 156.99it/s]\n",
      "\n",
      "\n",
      " 11%|█         | 2110/20000 [00:11<02:01, 146.72it/s]\n",
      "\n",
      "\n",
      " 11%|█         | 2133/20000 [00:11<01:48, 163.98it/s]\n",
      "\n",
      "\n",
      " 11%|█         | 2154/20000 [00:12<01:43, 173.02it/s]\n",
      "\n",
      "\n",
      " 11%|█         | 2185/20000 [00:12<01:30, 195.85it/s]\n",
      "\n",
      "\n",
      " 11%|█         | 2222/20000 [00:12<01:18, 227.08it/s]\n",
      "\n",
      "\n",
      " 11%|█▏        | 2258/20000 [00:12<01:09, 255.33it/s]\n",
      "\n",
      "\n",
      " 11%|█▏        | 2292/20000 [00:12<01:04, 275.83it/s]\n",
      "\n",
      "\n",
      " 12%|█▏        | 2323/20000 [00:12<01:07, 260.59it/s]\n",
      "\n",
      "\n",
      " 12%|█▏        | 2352/20000 [00:12<01:06, 264.84it/s]\n",
      "\n",
      "\n",
      " 12%|█▏        | 2380/20000 [00:12<01:09, 252.62it/s]\n",
      "\n",
      "\n",
      " 12%|█▏        | 2409/20000 [00:12<01:07, 261.39it/s]\n",
      "\n",
      "\n",
      " 12%|█▏        | 2438/20000 [00:13<01:05, 267.92it/s]\n",
      "\n",
      "\n",
      " 12%|█▏        | 2469/20000 [00:13<01:02, 278.34it/s]\n",
      "\n",
      "\n",
      " 12%|█▏        | 2498/20000 [00:13<01:03, 276.45it/s]\n",
      "\n",
      "\n",
      " 13%|█▎        | 2533/20000 [00:13<00:59, 294.41it/s]\n",
      "\n",
      "\n",
      " 13%|█▎        | 2571/20000 [00:13<00:55, 315.07it/s]\n",
      "\n",
      "\n",
      " 13%|█▎        | 2604/20000 [00:13<01:00, 286.00it/s]\n",
      "\n",
      "\n",
      " 13%|█▎        | 2634/20000 [00:13<01:06, 262.54it/s]\n",
      "\n",
      "\n",
      " 13%|█▎        | 2667/20000 [00:13<01:02, 276.28it/s]\n",
      "\n",
      "\n",
      " 13%|█▎        | 2696/20000 [00:13<01:05, 263.06it/s]\n",
      "\n",
      "\n",
      " 14%|█▎        | 2728/20000 [00:14<01:02, 277.09it/s]\n",
      "\n",
      "\n",
      " 14%|█▍        | 2760/20000 [00:14<00:59, 287.40it/s]\n",
      "\n",
      "\n",
      " 14%|█▍        | 2790/20000 [00:14<00:59, 288.40it/s]\n",
      "\n",
      "\n",
      " 14%|█▍        | 2820/20000 [00:14<01:00, 281.66it/s]\n",
      "\n",
      "\n",
      " 14%|█▍        | 2849/20000 [00:14<01:00, 283.03it/s]\n",
      "\n",
      "\n",
      " 14%|█▍        | 2878/20000 [00:14<01:03, 268.40it/s]\n",
      "\n",
      "\n",
      " 15%|█▍        | 2906/20000 [00:14<01:07, 252.71it/s]\n",
      "\n",
      "\n",
      " 15%|█▍        | 2933/20000 [00:14<01:08, 249.33it/s]\n",
      "\n",
      "\n",
      " 15%|█▍        | 2963/20000 [00:14<01:05, 261.12it/s]\n",
      "\n",
      "\n",
      " 15%|█▍        | 2992/20000 [00:15<01:03, 268.81it/s]\n",
      "\n",
      "\n",
      " 15%|█▌        | 3032/20000 [00:15<00:57, 296.89it/s]\n",
      "\n",
      "\n",
      " 15%|█▌        | 3066/20000 [00:15<00:55, 307.77it/s]\n",
      "\n",
      "\n",
      " 16%|█▌        | 3101/20000 [00:15<00:53, 316.35it/s]\n",
      "\n",
      "\n",
      " 16%|█▌        | 3134/20000 [00:15<00:57, 291.23it/s]\n",
      "\n",
      "\n",
      " 16%|█▌        | 3164/20000 [00:15<01:02, 269.74it/s]\n",
      "\n",
      "\n",
      " 16%|█▌        | 3192/20000 [00:15<01:11, 235.58it/s]\n",
      "\n",
      "\n",
      " 16%|█▌        | 3217/20000 [00:15<01:13, 227.88it/s]\n",
      "\n",
      "\n",
      " 16%|█▌        | 3241/20000 [00:16<01:15, 222.86it/s]\n",
      "\n",
      "\n",
      " 16%|█▋        | 3265/20000 [00:16<01:13, 227.20it/s]\n",
      "\n",
      "\n",
      " 16%|█▋        | 3289/20000 [00:16<01:15, 221.36it/s]\n",
      "\n",
      "\n",
      " 17%|█▋        | 3312/20000 [00:16<01:20, 208.16it/s]\n",
      "\n",
      "\n",
      " 17%|█▋        | 3334/20000 [00:16<01:22, 200.89it/s]\n",
      "\n",
      "\n",
      " 17%|█▋        | 3360/20000 [00:16<01:17, 214.32it/s]\n",
      "\n",
      "\n",
      " 17%|█▋        | 3382/20000 [00:16<01:19, 209.38it/s]\n",
      "\n",
      "\n",
      " 17%|█▋        | 3404/20000 [00:16<01:23, 199.10it/s]\n",
      "\n",
      "\n",
      " 17%|█▋        | 3432/20000 [00:16<01:16, 217.80it/s]\n",
      "\n",
      "\n",
      " 17%|█▋        | 3455/20000 [00:17<01:18, 210.59it/s]\n",
      "\n",
      "\n",
      " 17%|█▋        | 3481/20000 [00:17<01:14, 221.31it/s]\n",
      "\n",
      "\n",
      " 18%|█▊        | 3504/20000 [00:17<01:21, 203.17it/s]\n",
      "\n",
      "\n",
      " 18%|█▊        | 3532/20000 [00:17<01:14, 219.95it/s]\n",
      "\n",
      "\n",
      " 18%|█▊        | 3556/20000 [00:17<01:13, 222.42it/s]\n",
      "\n",
      "\n",
      " 18%|█▊        | 3579/20000 [00:17<01:16, 214.11it/s]\n",
      "\n",
      "\n",
      " 18%|█▊        | 3601/20000 [00:17<01:18, 210.19it/s]\n",
      "\n",
      "\n",
      " 18%|█▊        | 3623/20000 [00:17<01:18, 207.97it/s]\n",
      "\n",
      "\n",
      " 18%|█▊        | 3645/20000 [00:17<01:20, 204.39it/s]\n",
      "\n",
      "\n",
      " 18%|█▊        | 3669/20000 [00:18<01:18, 208.39it/s]\n",
      "\n",
      "\n",
      " 18%|█▊        | 3700/20000 [00:18<01:10, 230.75it/s]\n",
      "\n",
      "\n",
      " 19%|█▊        | 3730/20000 [00:18<01:05, 247.56it/s]\n",
      "\n",
      "\n",
      " 19%|█▉        | 3763/20000 [00:18<01:01, 265.22it/s]\n",
      "\n",
      "\n",
      " 19%|█▉        | 3791/20000 [00:18<01:01, 263.85it/s]\n",
      "\n",
      "\n",
      " 19%|█▉        | 3819/20000 [00:18<01:01, 263.81it/s]\n",
      "\n",
      "\n",
      " 19%|█▉        | 3846/20000 [00:18<01:03, 254.83it/s]\n",
      "\n",
      "\n",
      " 19%|█▉        | 3872/20000 [00:18<01:08, 235.15it/s]\n",
      "\n",
      "\n",
      " 19%|█▉        | 3897/20000 [00:18<01:07, 237.41it/s]\n",
      "\n",
      "\n",
      " 20%|█▉        | 3922/20000 [00:19<01:07, 237.57it/s]\n",
      "\n",
      "\n",
      " 20%|█▉        | 3948/20000 [00:19<01:09, 229.55it/s]\n",
      "\n",
      "\n",
      " 20%|█▉        | 3972/20000 [00:19<01:15, 213.47it/s]\n",
      "\n",
      "\n",
      " 20%|█▉        | 3994/20000 [00:19<01:21, 195.73it/s]\n",
      "\n",
      "\n",
      " 20%|██        | 4015/20000 [00:19<01:23, 192.09it/s]\n",
      "\n",
      "\n",
      " 20%|██        | 4035/20000 [00:19<01:23, 190.34it/s]\n",
      "\n",
      "\n",
      " 20%|██        | 4055/20000 [00:19<01:30, 175.78it/s]\n",
      "\n",
      "\n",
      " 20%|██        | 4076/20000 [00:19<01:30, 176.35it/s]\n",
      "\n",
      "\n",
      " 20%|██        | 4094/20000 [00:19<01:30, 176.71it/s]\n",
      "\n",
      "\n",
      " 21%|██        | 4116/20000 [00:20<01:24, 187.50it/s]\n",
      "\n",
      "\n",
      " 21%|██        | 4136/20000 [00:20<01:23, 190.04it/s]\n",
      "\n",
      "\n",
      " 21%|██        | 4156/20000 [00:20<01:30, 174.59it/s]\n",
      "\n",
      "\n",
      " 21%|██        | 4174/20000 [00:20<01:34, 166.71it/s]\n",
      "\n",
      "\n",
      " 21%|██        | 4196/20000 [00:20<01:29, 176.08it/s]\n",
      "\n",
      "\n",
      " 21%|██        | 4215/20000 [00:20<01:40, 157.32it/s]\n",
      "\n",
      "\n",
      " 21%|██        | 4234/20000 [00:20<01:35, 164.24it/s]\n",
      "\n",
      "\n",
      " 21%|██▏       | 4265/20000 [00:20<01:22, 189.82it/s]\n",
      "\n",
      "\n",
      " 21%|██▏       | 4288/20000 [00:21<01:19, 198.78it/s]\n",
      "\n",
      "\n",
      " 22%|██▏       | 4317/20000 [00:21<01:12, 217.17it/s]\n",
      "\n",
      "\n",
      " 22%|██▏       | 4341/20000 [00:21<01:10, 223.04it/s]\n",
      "\n",
      "\n",
      " 22%|██▏       | 4367/20000 [00:21<01:10, 223.06it/s]\n",
      "\n",
      "\n",
      " 22%|██▏       | 4391/20000 [00:21<01:09, 223.72it/s]\n",
      "\n",
      "\n",
      " 22%|██▏       | 4414/20000 [00:21<01:09, 224.98it/s]\n",
      "\n",
      "\n",
      " 22%|██▏       | 4444/20000 [00:21<01:04, 242.53it/s]\n",
      "\n",
      "\n",
      " 22%|██▏       | 4469/20000 [00:21<01:07, 229.81it/s]\n",
      "\n",
      "\n",
      " 22%|██▏       | 4493/20000 [00:21<01:07, 230.48it/s]\n",
      "\n",
      "\n",
      " 23%|██▎       | 4517/20000 [00:21<01:08, 225.93it/s]\n",
      "\n",
      "\n",
      " 23%|██▎       | 4540/20000 [00:22<01:12, 213.71it/s]\n",
      "\n",
      "\n",
      " 23%|██▎       | 4562/20000 [00:22<01:15, 205.54it/s]\n",
      "\n",
      "\n",
      " 23%|██▎       | 4590/20000 [00:22<01:09, 222.64it/s]\n",
      "\n",
      "\n",
      " 23%|██▎       | 4619/20000 [00:22<01:04, 238.96it/s]\n",
      "\n",
      "\n",
      " 23%|██▎       | 4646/20000 [00:22<01:02, 246.91it/s]\n",
      "\n",
      "\n",
      " 23%|██▎       | 4672/20000 [00:22<01:04, 238.77it/s]\n",
      "\n",
      "\n",
      " 23%|██▎       | 4697/20000 [00:22<01:06, 231.83it/s]\n",
      "\n",
      "\n",
      " 24%|██▎       | 4721/20000 [00:22<01:05, 234.07it/s]\n",
      "\n",
      "\n",
      " 24%|██▎       | 4747/20000 [00:22<01:03, 240.37it/s]\n",
      "\n",
      "\n",
      " 24%|██▍       | 4772/20000 [00:23<01:04, 235.20it/s]\n",
      "\n",
      "\n",
      " 24%|██▍       | 4796/20000 [00:23<01:06, 228.27it/s]\n",
      "\n",
      "\n",
      " 24%|██▍       | 4825/20000 [00:23<01:02, 243.77it/s]\n",
      "\n",
      "\n",
      " 24%|██▍       | 4851/20000 [00:23<01:01, 247.18it/s]\n",
      "\n",
      "\n",
      " 24%|██▍       | 4877/20000 [00:23<01:01, 244.71it/s]\n",
      "\n",
      "\n",
      " 25%|██▍       | 4902/20000 [00:23<01:05, 230.84it/s]\n",
      "\n",
      "\n",
      " 25%|██▍       | 4926/20000 [00:23<01:08, 220.70it/s]\n",
      "\n",
      "\n",
      " 25%|██▍       | 4954/20000 [00:23<01:03, 235.25it/s]\n",
      "\n",
      "\n",
      " 25%|██▍       | 4979/20000 [00:23<01:09, 216.79it/s]\n",
      "\n",
      "\n",
      " 25%|██▌       | 5002/20000 [00:24<01:09, 216.58it/s]\n",
      "\n",
      "\n",
      " 25%|██▌       | 5025/20000 [00:24<01:10, 213.38it/s]\n",
      "\n",
      "\n",
      " 25%|██▌       | 5047/20000 [00:24<01:11, 210.50it/s]\n",
      "\n",
      "\n",
      " 25%|██▌       | 5069/20000 [00:24<01:11, 208.20it/s]\n",
      "\n",
      "\n",
      " 25%|██▌       | 5090/20000 [00:24<01:17, 192.77it/s]\n",
      "\n",
      "\n",
      " 26%|██▌       | 5110/20000 [00:24<01:26, 171.56it/s]\n",
      "\n",
      "\n",
      " 26%|██▌       | 5128/20000 [00:24<01:36, 153.45it/s]\n",
      "\n",
      "\n",
      " 26%|██▌       | 5146/20000 [00:24<01:32, 160.14it/s]\n",
      "\n",
      "\n",
      " 26%|██▌       | 5163/20000 [00:25<01:35, 155.94it/s]\n",
      "\n",
      "\n",
      " 26%|██▌       | 5186/20000 [00:25<01:27, 169.81it/s]\n",
      "\n",
      "\n",
      " 26%|██▌       | 5213/20000 [00:25<01:18, 189.16it/s]\n",
      "\n",
      "\n",
      " 26%|██▌       | 5235/20000 [00:25<01:14, 197.13it/s]\n",
      "\n",
      "\n",
      " 26%|██▋       | 5259/20000 [00:25<01:11, 205.74it/s]\n",
      "\n",
      "\n",
      " 26%|██▋       | 5285/20000 [00:25<01:07, 219.01it/s]\n",
      "\n",
      "\n",
      " 27%|██▋       | 5312/20000 [00:25<01:03, 231.49it/s]\n",
      "\n",
      "\n",
      " 27%|██▋       | 5340/20000 [00:25<01:00, 243.61it/s]\n",
      "\n",
      "\n",
      " 27%|██▋       | 5366/20000 [00:25<00:58, 248.05it/s]\n",
      "\n",
      "\n",
      " 27%|██▋       | 5392/20000 [00:26<01:14, 195.63it/s]\n",
      "\n",
      "\n",
      " 27%|██▋       | 5422/20000 [00:26<01:07, 214.88it/s]\n",
      "\n",
      "\n",
      " 27%|██▋       | 5450/20000 [00:26<01:08, 210.99it/s]\n",
      "\n",
      "\n",
      " 27%|██▋       | 5479/20000 [00:26<01:04, 226.60it/s]\n",
      "\n",
      "\n",
      " 28%|██▊       | 5506/20000 [00:26<01:01, 236.97it/s]\n",
      "\n",
      "\n",
      " 28%|██▊       | 5531/20000 [00:26<01:01, 234.71it/s]\n",
      "\n",
      "\n",
      " 28%|██▊       | 5556/20000 [00:26<01:06, 216.91it/s]\n",
      "\n",
      "\n",
      " 28%|██▊       | 5579/20000 [00:26<01:06, 217.11it/s]\n",
      "\n",
      "\n",
      " 28%|██▊       | 5602/20000 [00:26<01:08, 210.47it/s]\n",
      "\n",
      "\n",
      " 28%|██▊       | 5624/20000 [00:27<01:15, 190.15it/s]\n",
      "\n",
      "\n",
      " 28%|██▊       | 5645/20000 [00:27<01:14, 193.74it/s]\n",
      "\n",
      "\n",
      " 28%|██▊       | 5665/20000 [00:27<01:16, 188.27it/s]\n",
      "\n",
      "\n",
      " 28%|██▊       | 5685/20000 [00:27<01:16, 187.28it/s]\n",
      "\n",
      "\n",
      " 29%|██▊       | 5709/20000 [00:27<01:11, 200.24it/s]\n",
      "\n",
      "\n",
      " 29%|██▊       | 5733/20000 [00:27<01:07, 210.61it/s]\n",
      "\n",
      "\n",
      " 29%|██▉       | 5762/20000 [00:27<01:02, 229.15it/s]\n",
      "\n",
      "\n",
      " 29%|██▉       | 5786/20000 [00:27<01:02, 225.70it/s]\n",
      "\n",
      "\n",
      " 29%|██▉       | 5810/20000 [00:27<01:07, 209.00it/s]\n",
      "\n",
      "\n",
      " 29%|██▉       | 5834/20000 [00:28<01:05, 215.71it/s]\n",
      "\n",
      "\n",
      " 29%|██▉       | 5857/20000 [00:28<01:10, 201.15it/s]\n",
      "\n",
      "\n",
      " 29%|██▉       | 5878/20000 [00:28<01:18, 180.79it/s]\n",
      "\n",
      "\n",
      " 29%|██▉       | 5897/20000 [00:28<01:22, 170.14it/s]\n",
      "\n",
      "\n",
      " 30%|██▉       | 5916/20000 [00:28<01:22, 170.18it/s]\n",
      "\n",
      "\n",
      " 30%|██▉       | 5936/20000 [00:28<01:19, 177.54it/s]\n",
      "\n",
      "\n",
      " 30%|██▉       | 5955/20000 [00:28<01:18, 179.30it/s]\n",
      "\n",
      "\n",
      " 30%|██▉       | 5977/20000 [00:28<01:14, 188.55it/s]\n",
      "\n",
      "\n",
      " 30%|██▉       | 5997/20000 [00:29<01:15, 185.56it/s]\n",
      "\n",
      "\n",
      " 30%|███       | 6025/20000 [00:29<01:08, 204.46it/s]\n",
      "\n",
      "\n",
      " 30%|███       | 6047/20000 [00:29<01:08, 203.75it/s]\n",
      "\n",
      "\n",
      " 30%|███       | 6068/20000 [00:29<01:08, 202.01it/s]\n",
      "\n",
      "\n",
      " 30%|███       | 6096/20000 [00:29<01:04, 216.55it/s]\n",
      "\n",
      "\n",
      " 31%|███       | 6119/20000 [00:29<01:05, 212.68it/s]\n",
      "\n",
      "\n",
      " 31%|███       | 6141/20000 [00:29<01:06, 208.00it/s]\n",
      "\n",
      "\n",
      " 31%|███       | 6164/20000 [00:29<01:05, 209.81it/s]\n",
      "\n",
      "\n",
      " 31%|███       | 6186/20000 [00:29<01:09, 199.73it/s]\n",
      "\n",
      "\n",
      " 31%|███       | 6207/20000 [00:30<01:25, 162.05it/s]\n",
      "\n",
      "\n",
      " 31%|███       | 6225/20000 [00:30<01:27, 157.53it/s]\n",
      "\n",
      "\n",
      " 31%|███       | 6243/20000 [00:30<01:24, 163.44it/s]\n",
      "\n",
      "\n",
      " 31%|███▏      | 6263/20000 [00:30<01:20, 170.34it/s]\n",
      "\n",
      "\n",
      " 31%|███▏      | 6281/20000 [00:30<01:43, 133.14it/s]\n",
      "\n",
      "\n",
      " 31%|███▏      | 6296/20000 [00:30<01:44, 130.96it/s]\n",
      "\n",
      "\n",
      " 32%|███▏      | 6311/20000 [00:30<01:45, 130.21it/s]\n",
      "\n",
      "\n",
      " 32%|███▏      | 6327/20000 [00:30<01:39, 137.78it/s]\n",
      "\n",
      "\n",
      " 32%|███▏      | 6345/20000 [00:31<01:35, 143.33it/s]\n",
      "\n",
      "\n",
      " 32%|███▏      | 6362/20000 [00:31<01:32, 147.64it/s]\n",
      "\n",
      "\n",
      " 32%|███▏      | 6378/20000 [00:31<01:32, 147.04it/s]\n",
      "\n",
      "\n",
      " 32%|███▏      | 6393/20000 [00:31<01:38, 138.65it/s]\n",
      "\n",
      "\n",
      " 32%|███▏      | 6408/20000 [00:31<01:40, 134.60it/s]\n",
      "\n",
      "\n",
      " 32%|███▏      | 6422/20000 [00:31<01:43, 130.98it/s]\n",
      "\n",
      "\n",
      " 32%|███▏      | 6436/20000 [00:31<02:21, 96.18it/s] \n",
      "\n",
      "\n",
      " 32%|███▏      | 6451/20000 [00:31<02:05, 107.72it/s]\n",
      "\n",
      "\n",
      " 32%|███▏      | 6467/20000 [00:32<01:55, 117.48it/s]\n",
      "\n",
      "\n",
      " 32%|███▏      | 6488/20000 [00:32<01:40, 134.01it/s]\n",
      "\n",
      "\n",
      " 33%|███▎      | 6504/20000 [00:32<01:51, 120.58it/s]\n",
      "\n",
      "\n",
      " 33%|███▎      | 6523/20000 [00:32<01:43, 130.17it/s]\n",
      "\n",
      "\n",
      " 33%|███▎      | 6542/20000 [00:32<01:34, 141.96it/s]\n",
      "\n",
      "\n",
      " 33%|███▎      | 6558/20000 [00:32<01:33, 143.15it/s]\n",
      "\n",
      "\n",
      " 33%|███▎      | 6574/20000 [00:32<01:35, 140.33it/s]\n",
      "\n",
      "\n",
      " 33%|███▎      | 6590/20000 [00:32<01:32, 144.94it/s]\n",
      "\n",
      "\n",
      " 33%|███▎      | 6609/20000 [00:33<01:26, 155.58it/s]\n",
      "\n",
      "\n",
      " 33%|███▎      | 6626/20000 [00:33<01:26, 154.93it/s]\n",
      "\n",
      "\n",
      " 33%|███▎      | 6643/20000 [00:33<01:24, 158.99it/s]\n",
      "\n",
      "\n",
      " 33%|███▎      | 6660/20000 [00:33<01:31, 145.56it/s]\n",
      "\n",
      "\n",
      " 33%|███▎      | 6675/20000 [00:33<01:37, 136.20it/s]\n",
      "\n",
      "\n",
      " 33%|███▎      | 6690/20000 [00:33<01:44, 126.84it/s]\n",
      "\n",
      "\n",
      " 34%|███▎      | 6707/20000 [00:33<01:38, 134.85it/s]\n",
      "\n",
      "\n",
      " 34%|███▎      | 6721/20000 [00:33<01:44, 127.38it/s]\n",
      "\n",
      "\n",
      " 34%|███▎      | 6743/20000 [00:33<01:32, 143.90it/s]\n",
      "\n",
      "\n",
      " 34%|███▍      | 6771/20000 [00:34<01:18, 167.81it/s]\n",
      "\n",
      "\n",
      " 34%|███▍      | 6796/20000 [00:34<01:11, 185.26it/s]\n",
      "\n",
      "\n",
      " 34%|███▍      | 6817/20000 [00:34<01:10, 186.65it/s]\n",
      "\n",
      "\n",
      " 34%|███▍      | 6840/20000 [00:34<01:07, 194.40it/s]\n",
      "\n",
      "\n",
      " 34%|███▍      | 6861/20000 [00:34<01:07, 193.47it/s]\n",
      "\n",
      "\n",
      " 34%|███▍      | 6882/20000 [00:34<01:09, 187.83it/s]\n",
      "\n",
      "\n",
      " 35%|███▍      | 6904/20000 [00:34<01:06, 196.44it/s]\n",
      "\n",
      "\n",
      " 35%|███▍      | 6928/20000 [00:34<01:03, 207.28it/s]\n",
      "\n",
      "\n",
      " 35%|███▍      | 6954/20000 [00:34<00:59, 220.51it/s]\n",
      "\n",
      "\n",
      " 35%|███▍      | 6977/20000 [00:35<00:59, 218.15it/s]\n",
      "\n",
      "\n",
      " 35%|███▌      | 7000/20000 [00:35<01:02, 206.80it/s]\n",
      "\n",
      "\n",
      " 35%|███▌      | 7022/20000 [00:35<01:02, 209.14it/s]\n",
      "\n",
      "\n",
      " 35%|███▌      | 7044/20000 [00:35<01:02, 206.77it/s]\n",
      "\n",
      "\n",
      " 35%|███▌      | 7066/20000 [00:35<01:02, 206.41it/s]\n",
      "\n",
      "\n",
      " 35%|███▌      | 7087/20000 [00:35<01:05, 197.26it/s]\n",
      "\n",
      "\n",
      " 36%|███▌      | 7109/20000 [00:35<01:03, 202.15it/s]\n",
      "\n",
      "\n",
      " 36%|███▌      | 7136/20000 [00:35<01:00, 211.38it/s]\n",
      "\n",
      "\n",
      " 36%|███▌      | 7158/20000 [00:35<01:02, 204.94it/s]\n",
      "\n",
      "\n",
      " 36%|███▌      | 7179/20000 [00:36<01:06, 193.78it/s]\n",
      "\n",
      "\n",
      " 36%|███▌      | 7199/20000 [00:36<01:05, 195.34it/s]\n",
      "\n",
      "\n",
      " 36%|███▌      | 7219/20000 [00:36<01:05, 196.42it/s]\n",
      "\n",
      "\n",
      " 36%|███▌      | 7239/20000 [00:36<01:19, 160.64it/s]\n",
      "\n",
      "\n",
      " 36%|███▋      | 7258/20000 [00:36<01:15, 167.92it/s]\n",
      "\n",
      "\n",
      " 36%|███▋      | 7277/20000 [00:36<01:14, 170.71it/s]\n",
      "\n",
      "\n",
      " 36%|███▋      | 7295/20000 [00:36<01:17, 163.22it/s]\n",
      "\n",
      "\n",
      " 37%|███▋      | 7323/20000 [00:36<01:08, 184.64it/s]\n",
      "\n",
      "\n",
      " 37%|███▋      | 7343/20000 [00:37<01:16, 165.72it/s]\n",
      "\n",
      "\n",
      " 37%|███▋      | 7363/20000 [00:37<01:12, 173.48it/s]\n",
      "\n",
      "\n",
      " 37%|███▋      | 7382/20000 [00:37<01:13, 170.80it/s]\n",
      "\n",
      "\n",
      " 37%|███▋      | 7406/20000 [00:37<01:10, 179.43it/s]\n",
      "\n",
      "\n",
      " 37%|███▋      | 7425/20000 [00:37<01:09, 181.54it/s]\n",
      "\n",
      "\n",
      " 37%|███▋      | 7448/20000 [00:37<01:05, 190.96it/s]\n",
      "\n",
      "\n",
      " 37%|███▋      | 7468/20000 [00:37<01:08, 183.87it/s]\n",
      "\n",
      "\n",
      " 37%|███▋      | 7487/20000 [00:37<01:07, 185.05it/s]\n",
      "\n",
      "\n",
      " 38%|███▊      | 7506/20000 [00:37<01:14, 168.57it/s]\n",
      "\n",
      "\n",
      " 38%|███▊      | 7525/20000 [00:38<01:11, 173.74it/s]\n",
      "\n",
      "\n",
      " 38%|███▊      | 7543/20000 [00:38<01:11, 175.16it/s]\n",
      "\n",
      "\n",
      " 38%|███▊      | 7565/20000 [00:38<01:07, 184.24it/s]\n",
      "\n",
      "\n",
      " 38%|███▊      | 7587/20000 [00:38<01:04, 192.83it/s]\n",
      "\n",
      "\n",
      " 38%|███▊      | 7607/20000 [00:38<01:15, 163.59it/s]\n",
      "\n",
      "\n",
      " 38%|███▊      | 7628/20000 [00:38<01:10, 174.26it/s]\n",
      "\n",
      "\n",
      " 38%|███▊      | 7647/20000 [00:38<01:14, 166.54it/s]\n",
      "\n",
      "\n",
      " 38%|███▊      | 7665/20000 [00:38<01:14, 166.01it/s]\n",
      "\n",
      "\n",
      " 38%|███▊      | 7683/20000 [00:38<01:13, 167.03it/s]\n",
      "\n",
      "\n",
      " 39%|███▊      | 7705/20000 [00:39<01:08, 179.87it/s]\n",
      "\n",
      "\n",
      " 39%|███▊      | 7724/20000 [00:39<01:07, 182.30it/s]\n",
      "\n",
      "\n",
      " 39%|███▊      | 7743/20000 [00:39<01:13, 165.97it/s]\n",
      "\n",
      "\n",
      " 39%|███▉      | 7761/20000 [00:39<01:16, 159.48it/s]\n",
      "\n",
      "\n",
      " 39%|███▉      | 7778/20000 [00:39<01:23, 146.62it/s]\n",
      "\n",
      "\n",
      " 39%|███▉      | 7794/20000 [00:39<01:26, 140.70it/s]\n",
      "\n",
      "\n",
      " 39%|███▉      | 7816/20000 [00:39<01:17, 157.62it/s]\n",
      "\n",
      "\n",
      " 39%|███▉      | 7842/20000 [00:39<01:08, 176.39it/s]\n",
      "\n",
      "\n",
      " 39%|███▉      | 7864/20000 [00:39<01:04, 187.10it/s]\n",
      "\n",
      "\n",
      " 39%|███▉      | 7884/20000 [00:40<01:08, 177.44it/s]\n",
      "\n",
      "\n",
      " 40%|███▉      | 7907/20000 [00:40<01:03, 189.34it/s]\n",
      "\n",
      "\n",
      " 40%|███▉      | 7927/20000 [00:40<01:02, 192.33it/s]\n",
      "\n",
      "\n",
      " 40%|███▉      | 7961/20000 [00:40<00:54, 220.25it/s]\n",
      "\n",
      "\n",
      " 40%|███▉      | 7986/20000 [00:40<00:53, 224.11it/s]\n",
      "\n",
      "\n",
      " 40%|████      | 8010/20000 [00:40<00:53, 223.47it/s]\n",
      "\n",
      "\n",
      " 40%|████      | 8034/20000 [00:40<00:53, 222.75it/s]\n",
      "\n",
      "\n",
      " 40%|████      | 8057/20000 [00:40<00:58, 203.17it/s]\n",
      "\n",
      "\n",
      " 40%|████      | 8079/20000 [00:40<01:01, 192.76it/s]\n",
      "\n",
      "\n",
      " 41%|████      | 8106/20000 [00:41<00:56, 210.65it/s]\n",
      "\n",
      "\n",
      " 41%|████      | 8129/20000 [00:41<01:02, 189.71it/s]\n",
      "\n",
      "\n",
      " 41%|████      | 8152/20000 [00:41<00:59, 197.97it/s]\n",
      "\n",
      "\n",
      " 41%|████      | 8173/20000 [00:41<01:02, 190.27it/s]\n",
      "\n",
      "\n",
      " 41%|████      | 8195/20000 [00:41<00:59, 197.31it/s]\n",
      "\n",
      "\n",
      " 41%|████      | 8217/20000 [00:41<00:58, 202.32it/s]\n",
      "\n",
      "\n",
      " 41%|████      | 8243/20000 [00:41<00:54, 216.40it/s]\n",
      "\n",
      "\n",
      " 41%|████▏     | 8268/20000 [00:41<00:52, 224.60it/s]\n",
      "\n",
      "\n",
      " 41%|████▏     | 8292/20000 [00:41<00:51, 228.27it/s]\n",
      "\n",
      "\n",
      " 42%|████▏     | 8316/20000 [00:42<00:54, 212.54it/s]\n",
      "\n",
      "\n",
      " 42%|████▏     | 8338/20000 [00:42<00:56, 207.26it/s]\n",
      "\n",
      "\n",
      " 42%|████▏     | 8360/20000 [00:42<00:55, 209.17it/s]\n",
      "\n",
      "\n",
      " 42%|████▏     | 8382/20000 [00:42<00:57, 203.48it/s]\n",
      "\n",
      "\n",
      " 42%|████▏     | 8403/20000 [00:42<00:57, 200.28it/s]\n",
      "\n",
      "\n",
      " 42%|████▏     | 8424/20000 [00:42<01:00, 192.03it/s]\n",
      "\n",
      "\n",
      " 42%|████▏     | 8444/20000 [00:43<02:36, 73.69it/s] \n",
      "\n",
      "\n",
      " 42%|████▏     | 8461/20000 [00:43<02:10, 88.32it/s]\n",
      "\n",
      "\n",
      " 42%|████▏     | 8480/20000 [00:43<01:49, 105.19it/s]\n",
      "\n",
      "\n",
      " 42%|████▎     | 8500/20000 [00:43<01:35, 120.70it/s]\n",
      "\n",
      "\n",
      " 43%|████▎     | 8518/20000 [00:43<01:26, 132.01it/s]\n",
      "\n",
      "\n",
      " 43%|████▎     | 8535/20000 [00:43<01:27, 130.82it/s]\n",
      "\n",
      "\n",
      " 43%|████▎     | 8558/20000 [00:43<01:16, 149.63it/s]\n",
      "\n",
      "\n",
      " 43%|████▎     | 8580/20000 [00:44<01:09, 164.65it/s]\n",
      "\n",
      "\n",
      " 43%|████▎     | 8599/20000 [00:44<01:09, 164.37it/s]\n",
      "\n",
      "\n",
      " 43%|████▎     | 8618/20000 [00:44<01:07, 167.59it/s]\n",
      "\n",
      "\n",
      " 43%|████▎     | 8636/20000 [00:44<01:12, 156.79it/s]\n",
      "\n",
      "\n",
      " 43%|████▎     | 8653/20000 [00:44<01:18, 144.56it/s]\n",
      "\n",
      "\n",
      " 43%|████▎     | 8671/20000 [00:44<01:14, 153.03it/s]\n",
      "\n",
      "\n",
      " 43%|████▎     | 8692/20000 [00:44<01:08, 165.80it/s]\n",
      "\n",
      "\n",
      " 44%|████▎     | 8714/20000 [00:44<01:03, 177.43it/s]\n",
      "\n",
      "\n",
      " 44%|████▎     | 8747/20000 [00:44<00:56, 197.90it/s]\n",
      "\n",
      "\n",
      " 44%|████▍     | 8772/20000 [00:45<00:53, 210.29it/s]\n",
      "\n",
      "\n",
      " 44%|████▍     | 8796/20000 [00:45<00:51, 218.04it/s]\n",
      "\n",
      "\n",
      " 44%|████▍     | 8824/20000 [00:45<00:48, 230.18it/s]\n",
      "\n",
      "\n",
      " 44%|████▍     | 8852/20000 [00:45<00:45, 242.96it/s]\n",
      "\n",
      "\n",
      " 44%|████▍     | 8878/20000 [00:45<00:45, 245.53it/s]\n",
      "\n",
      "\n",
      " 45%|████▍     | 8904/20000 [00:45<00:48, 226.82it/s]\n",
      "\n",
      "\n",
      " 45%|████▍     | 8928/20000 [00:45<00:50, 220.46it/s]\n",
      "\n",
      "\n",
      " 45%|████▍     | 8957/20000 [00:45<00:46, 236.85it/s]\n",
      "\n",
      "\n",
      " 45%|████▍     | 8982/20000 [00:45<00:47, 232.04it/s]\n",
      "\n",
      "\n",
      " 45%|████▌     | 9009/20000 [00:46<00:45, 241.98it/s]\n",
      "\n",
      "\n",
      " 45%|████▌     | 9034/20000 [00:46<00:47, 230.66it/s]\n",
      "\n",
      "\n",
      " 45%|████▌     | 9059/20000 [00:46<00:46, 234.55it/s]\n",
      "\n",
      "\n",
      " 45%|████▌     | 9083/20000 [00:46<00:47, 231.56it/s]\n",
      "\n",
      "\n",
      " 46%|████▌     | 9107/20000 [00:46<00:47, 229.50it/s]\n",
      "\n",
      "\n",
      " 46%|████▌     | 9131/20000 [00:46<00:54, 200.91it/s]\n",
      "\n",
      "\n",
      " 46%|████▌     | 9152/20000 [00:46<00:54, 198.18it/s]\n",
      "\n",
      "\n",
      " 46%|████▌     | 9173/20000 [00:46<00:55, 196.12it/s]\n",
      "\n",
      "\n",
      " 46%|████▌     | 9193/20000 [00:46<00:56, 191.60it/s]\n",
      "\n",
      "\n",
      " 46%|████▌     | 9216/20000 [00:47<00:53, 200.43it/s]\n",
      "\n",
      "\n",
      " 46%|████▌     | 9237/20000 [00:47<00:54, 199.04it/s]\n",
      "\n",
      "\n",
      " 46%|████▋     | 9258/20000 [00:47<00:55, 193.10it/s]\n",
      "\n",
      "\n",
      " 46%|████▋     | 9278/20000 [00:47<00:55, 191.69it/s]\n",
      "\n",
      "\n",
      " 46%|████▋     | 9298/20000 [00:47<00:58, 183.48it/s]\n",
      "\n",
      "\n",
      " 47%|████▋     | 9322/20000 [00:47<00:54, 196.67it/s]\n",
      "\n",
      "\n",
      " 47%|████▋     | 9343/20000 [00:47<00:53, 200.31it/s]\n",
      "\n",
      "\n",
      " 47%|████▋     | 9364/20000 [00:47<00:56, 188.05it/s]\n",
      "\n",
      "\n",
      " 47%|████▋     | 9384/20000 [00:47<00:56, 189.24it/s]\n",
      "\n",
      "\n",
      " 47%|████▋     | 9404/20000 [00:48<01:00, 175.56it/s]\n",
      "\n",
      "\n",
      " 47%|████▋     | 9423/20000 [00:48<01:00, 174.89it/s]\n",
      "\n",
      "\n",
      " 47%|████▋     | 9448/20000 [00:48<00:55, 191.54it/s]\n",
      "\n",
      "\n",
      " 47%|████▋     | 9468/20000 [00:48<00:56, 185.04it/s]\n",
      "\n",
      "\n",
      " 47%|████▋     | 9488/20000 [00:48<01:00, 174.55it/s]\n",
      "\n",
      "\n",
      " 48%|████▊     | 9506/20000 [00:48<01:01, 169.81it/s]\n",
      "\n",
      "\n",
      " 48%|████▊     | 9524/20000 [00:48<01:03, 164.99it/s]\n",
      "\n",
      "\n",
      " 48%|████▊     | 9541/20000 [00:48<01:03, 164.14it/s]\n",
      "\n",
      "\n",
      " 48%|████▊     | 9562/20000 [00:48<01:00, 173.47it/s]\n",
      "\n",
      "\n",
      " 48%|████▊     | 9583/20000 [00:49<00:58, 178.86it/s]\n",
      "\n",
      "\n",
      " 48%|████▊     | 9602/20000 [00:49<00:57, 180.37it/s]\n",
      "\n",
      "\n",
      " 48%|████▊     | 9621/20000 [00:49<00:57, 181.04it/s]\n",
      "\n",
      "\n",
      " 48%|████▊     | 9643/20000 [00:49<00:54, 190.45it/s]\n",
      "\n",
      "\n",
      " 48%|████▊     | 9663/20000 [00:49<00:59, 172.93it/s]\n",
      "\n",
      "\n",
      " 48%|████▊     | 9681/20000 [00:49<01:00, 169.87it/s]\n",
      "\n",
      "\n",
      " 48%|████▊     | 9699/20000 [00:49<01:09, 149.10it/s]\n",
      "\n",
      "\n",
      " 49%|████▊     | 9715/20000 [00:49<01:10, 146.55it/s]\n",
      "\n",
      "\n",
      " 49%|████▊     | 9731/20000 [00:50<01:11, 143.07it/s]\n",
      "\n",
      "\n",
      " 49%|████▉     | 9752/20000 [00:50<01:04, 157.89it/s]\n",
      "\n",
      "\n",
      " 49%|████▉     | 9769/20000 [00:50<01:05, 157.35it/s]\n",
      "\n",
      "\n",
      " 49%|████▉     | 9786/20000 [00:50<01:05, 156.68it/s]\n",
      "\n",
      "\n",
      " 49%|████▉     | 9803/20000 [00:50<01:10, 144.41it/s]\n",
      "\n",
      "\n",
      " 49%|████▉     | 9821/20000 [00:50<01:06, 152.63it/s]\n",
      "\n",
      "\n",
      " 49%|████▉     | 9837/20000 [00:50<01:12, 139.36it/s]\n",
      "\n",
      "\n",
      " 49%|████▉     | 9854/20000 [00:50<01:09, 146.95it/s]\n",
      "\n",
      "\n",
      " 49%|████▉     | 9870/20000 [00:50<01:14, 135.90it/s]\n",
      "\n",
      "\n",
      " 49%|████▉     | 9885/20000 [00:51<01:22, 123.03it/s]\n",
      "\n",
      "\n",
      " 50%|████▉     | 9914/20000 [00:51<01:07, 148.61it/s]\n",
      "\n",
      "\n",
      " 50%|████▉     | 9944/20000 [00:51<00:57, 174.60it/s]\n",
      "\n",
      "\n",
      " 50%|████▉     | 9966/20000 [00:51<00:54, 183.50it/s]\n",
      "\n",
      "\n",
      " 50%|████▉     | 9990/20000 [00:51<00:50, 197.40it/s]\n",
      "\n",
      "\n",
      " 50%|█████     | 10018/20000 [00:51<00:46, 216.15it/s]\n",
      "\n",
      "\n",
      " 50%|█████     | 10042/20000 [00:51<00:46, 213.57it/s]\n",
      "\n",
      "\n",
      " 50%|█████     | 10085/20000 [00:51<00:39, 248.73it/s]\n",
      "\n",
      "\n",
      " 51%|█████     | 10116/20000 [00:51<00:37, 263.21it/s]\n",
      "\n",
      "\n",
      " 51%|█████     | 10145/20000 [00:52<00:36, 266.77it/s]\n",
      "\n",
      "\n",
      " 51%|█████     | 10174/20000 [00:52<00:37, 263.08it/s]\n",
      "\n",
      "\n",
      " 51%|█████     | 10202/20000 [00:52<00:38, 255.55it/s]\n",
      "\n",
      "\n",
      " 51%|█████     | 10234/20000 [00:52<00:36, 268.93it/s]\n",
      "\n",
      "\n",
      " 51%|█████▏    | 10262/20000 [00:52<00:36, 269.92it/s]\n",
      "\n",
      "\n",
      " 51%|█████▏    | 10290/20000 [00:52<00:36, 264.48it/s]\n",
      "\n",
      "\n",
      " 52%|█████▏    | 10321/20000 [00:52<00:35, 274.63it/s]\n",
      "\n",
      "\n",
      " 52%|█████▏    | 10349/20000 [00:52<00:36, 266.26it/s]\n",
      "\n",
      "\n",
      " 52%|█████▏    | 10376/20000 [00:52<00:36, 265.66it/s]\n",
      "\n",
      "\n",
      " 52%|█████▏    | 10403/20000 [00:53<00:40, 239.86it/s]\n",
      "\n",
      "\n",
      " 52%|█████▏    | 10428/20000 [00:53<00:42, 222.96it/s]\n",
      "\n",
      "\n",
      " 52%|█████▏    | 10451/20000 [00:53<00:43, 218.81it/s]\n",
      "\n",
      "\n",
      " 52%|█████▏    | 10478/20000 [00:53<00:41, 229.53it/s]\n",
      "\n",
      "\n",
      " 53%|█████▎    | 10502/20000 [00:53<00:42, 221.83it/s]\n",
      "\n",
      "\n",
      " 53%|█████▎    | 10527/20000 [00:53<00:41, 229.13it/s]\n",
      "\n",
      "\n",
      " 53%|█████▎    | 10551/20000 [00:53<00:43, 217.70it/s]\n",
      "\n",
      "\n",
      " 53%|█████▎    | 10574/20000 [00:53<00:43, 216.91it/s]\n",
      "\n",
      "\n",
      " 53%|█████▎    | 10605/20000 [00:53<00:39, 238.16it/s]\n",
      "\n",
      "\n",
      " 53%|█████▎    | 10630/20000 [00:54<00:41, 227.31it/s]\n",
      "\n",
      "\n",
      " 53%|█████▎    | 10654/20000 [00:54<00:46, 201.94it/s]\n",
      "\n",
      "\n",
      " 53%|█████▎    | 10676/20000 [00:54<00:46, 201.24it/s]\n",
      "\n",
      "\n",
      " 53%|█████▎    | 10697/20000 [00:54<00:47, 195.33it/s]\n",
      "\n",
      "\n",
      " 54%|█████▎    | 10718/20000 [00:54<00:50, 182.48it/s]\n",
      "\n",
      "\n",
      " 54%|█████▎    | 10740/20000 [00:54<00:48, 191.39it/s]\n",
      "\n",
      "\n",
      " 54%|█████▍    | 10760/20000 [00:54<00:49, 187.21it/s]\n",
      "\n",
      "\n",
      " 54%|█████▍    | 10788/20000 [00:54<00:44, 207.53it/s]\n",
      "\n",
      "\n",
      " 54%|█████▍    | 10813/20000 [00:55<00:42, 218.42it/s]\n",
      "\n",
      "\n",
      " 54%|█████▍    | 10836/20000 [00:55<00:42, 214.09it/s]\n",
      "\n",
      "\n",
      " 54%|█████▍    | 10859/20000 [00:55<00:42, 217.48it/s]\n",
      "\n",
      "\n",
      " 54%|█████▍    | 10882/20000 [00:55<00:45, 198.57it/s]\n",
      "\n",
      "\n",
      " 55%|█████▍    | 10904/20000 [00:55<00:45, 201.02it/s]\n",
      "\n",
      "\n",
      " 55%|█████▍    | 10927/20000 [00:55<00:43, 207.60it/s]\n",
      "\n",
      "\n",
      " 55%|█████▍    | 10952/20000 [00:55<00:41, 217.77it/s]\n",
      "\n",
      "\n",
      " 55%|█████▍    | 10977/20000 [00:55<00:39, 225.85it/s]\n",
      "\n",
      "\n",
      " 55%|█████▌    | 11000/20000 [00:55<00:41, 216.28it/s]\n",
      "\n",
      "\n",
      " 55%|█████▌    | 11022/20000 [00:56<00:41, 214.97it/s]\n",
      "\n",
      "\n",
      " 55%|█████▌    | 11045/20000 [00:56<00:41, 217.08it/s]\n",
      "\n",
      "\n",
      " 55%|█████▌    | 11069/20000 [00:56<00:40, 221.48it/s]\n",
      "\n",
      "\n",
      " 55%|█████▌    | 11093/20000 [00:56<00:39, 223.77it/s]\n",
      "\n",
      "\n",
      " 56%|█████▌    | 11116/20000 [00:56<00:40, 217.07it/s]\n",
      "\n",
      "\n",
      " 56%|█████▌    | 11138/20000 [00:56<00:41, 213.87it/s]\n",
      "\n",
      "\n",
      " 56%|█████▌    | 11165/20000 [00:56<00:38, 228.05it/s]\n",
      "\n",
      "\n",
      " 56%|█████▌    | 11189/20000 [00:56<00:39, 221.62it/s]\n",
      "\n",
      "\n",
      " 56%|█████▌    | 11212/20000 [00:56<00:39, 223.66it/s]\n",
      "\n",
      "\n",
      " 56%|█████▌    | 11235/20000 [00:56<00:40, 213.91it/s]\n",
      "\n",
      "\n",
      " 56%|█████▋    | 11257/20000 [00:57<00:41, 211.63it/s]\n",
      "\n",
      "\n",
      " 56%|█████▋    | 11279/20000 [00:57<00:41, 209.34it/s]\n",
      "\n",
      "\n",
      " 57%|█████▋    | 11301/20000 [00:57<00:41, 211.51it/s]\n",
      "\n",
      "\n",
      " 57%|█████▋    | 11323/20000 [00:57<00:40, 213.97it/s]\n",
      "\n",
      "\n",
      " 57%|█████▋    | 11347/20000 [00:57<00:40, 214.93it/s]\n",
      "\n",
      "\n",
      " 57%|█████▋    | 11369/20000 [00:57<00:40, 211.64it/s]\n",
      "\n",
      "\n",
      " 57%|█████▋    | 11391/20000 [00:57<00:40, 213.29it/s]\n",
      "\n",
      "\n",
      " 57%|█████▋    | 11416/20000 [00:57<00:38, 221.35it/s]\n",
      "\n",
      "\n",
      " 57%|█████▋    | 11439/20000 [00:57<00:39, 215.05it/s]\n",
      "\n",
      "\n",
      " 57%|█████▋    | 11461/20000 [00:58<00:41, 207.43it/s]\n",
      "\n",
      "\n",
      " 57%|█████▋    | 11484/20000 [00:58<00:39, 213.36it/s]\n",
      "\n",
      "\n",
      " 58%|█████▊    | 11508/20000 [00:58<00:38, 220.30it/s]\n",
      "\n",
      "\n",
      " 58%|█████▊    | 11531/20000 [00:58<00:41, 202.65it/s]\n",
      "\n",
      "\n",
      " 58%|█████▊    | 11552/20000 [00:58<00:41, 203.83it/s]\n",
      "\n",
      "\n",
      " 58%|█████▊    | 11576/20000 [00:58<00:40, 209.89it/s]\n",
      "\n",
      "\n",
      " 58%|█████▊    | 11598/20000 [00:58<00:45, 185.46it/s]\n",
      "\n",
      "\n",
      " 58%|█████▊    | 11618/20000 [00:58<00:44, 188.17it/s]\n",
      "\n",
      "\n",
      " 58%|█████▊    | 11638/20000 [00:58<00:44, 186.51it/s]\n",
      "\n",
      "\n",
      " 58%|█████▊    | 11658/20000 [00:59<00:44, 187.68it/s]\n",
      "\n",
      "\n",
      " 58%|█████▊    | 11678/20000 [00:59<00:44, 188.25it/s]\n",
      "\n",
      "\n",
      " 58%|█████▊    | 11698/20000 [00:59<00:43, 190.31it/s]\n",
      "\n",
      "\n",
      " 59%|█████▊    | 11720/20000 [00:59<00:41, 197.99it/s]\n",
      "\n",
      "\n",
      " 59%|█████▊    | 11740/20000 [00:59<00:44, 184.19it/s]\n",
      "\n",
      "\n",
      " 59%|█████▉    | 11760/20000 [00:59<00:43, 188.36it/s]\n",
      "\n",
      "\n",
      " 59%|█████▉    | 11780/20000 [00:59<00:42, 191.38it/s]\n",
      "\n",
      "\n",
      " 59%|█████▉    | 11800/20000 [00:59<00:44, 183.60it/s]\n",
      "\n",
      "\n",
      " 59%|█████▉    | 11819/20000 [00:59<00:45, 179.33it/s]\n",
      "\n",
      "\n",
      " 59%|█████▉    | 11838/20000 [01:00<00:47, 172.53it/s]\n",
      "\n",
      "\n",
      " 59%|█████▉    | 11856/20000 [01:00<00:49, 165.15it/s]\n",
      "\n",
      "\n",
      " 59%|█████▉    | 11885/20000 [01:00<00:43, 188.24it/s]\n",
      "\n",
      "\n",
      " 60%|█████▉    | 11914/20000 [01:00<00:38, 209.14it/s]\n",
      "\n",
      "\n",
      " 60%|█████▉    | 11947/20000 [01:00<00:34, 233.71it/s]\n",
      "\n",
      "\n",
      " 60%|█████▉    | 11979/20000 [01:00<00:31, 253.08it/s]\n",
      "\n",
      "\n",
      " 60%|██████    | 12008/20000 [01:00<00:30, 262.51it/s]\n",
      "\n",
      "\n",
      " 60%|██████    | 12038/20000 [01:00<00:29, 270.20it/s]\n",
      "\n",
      "\n",
      " 60%|██████    | 12067/20000 [01:00<00:31, 255.07it/s]\n",
      "\n",
      "\n",
      " 61%|██████    | 12101/20000 [01:01<00:28, 274.50it/s]\n",
      "\n",
      "\n",
      " 61%|██████    | 12130/20000 [01:01<00:29, 267.05it/s]\n",
      "\n",
      "\n",
      " 61%|██████    | 12158/20000 [01:01<00:29, 269.01it/s]\n",
      "\n",
      "\n",
      " 61%|██████    | 12188/20000 [01:01<00:28, 276.67it/s]\n",
      "\n",
      "\n",
      " 61%|██████    | 12217/20000 [01:01<00:28, 277.76it/s]\n",
      "\n",
      "\n",
      " 61%|██████    | 12248/20000 [01:01<00:27, 283.96it/s]\n",
      "\n",
      "\n",
      " 61%|██████▏   | 12279/20000 [01:01<00:26, 288.27it/s]\n",
      "\n",
      "\n",
      " 62%|██████▏   | 12311/20000 [01:01<00:25, 297.08it/s]\n",
      "\n",
      "\n",
      " 62%|██████▏   | 12341/20000 [01:01<00:25, 294.98it/s]\n",
      "\n",
      "\n",
      " 62%|██████▏   | 12371/20000 [01:01<00:26, 289.63it/s]\n",
      "\n",
      "\n",
      " 62%|██████▏   | 12402/20000 [01:02<00:25, 294.09it/s]\n",
      "\n",
      "\n",
      " 62%|██████▏   | 12432/20000 [01:02<00:29, 252.81it/s]\n",
      "\n",
      "\n",
      " 62%|██████▏   | 12459/20000 [01:02<00:30, 244.24it/s]\n",
      "\n",
      "\n",
      " 62%|██████▏   | 12485/20000 [01:02<00:30, 248.70it/s]\n",
      "\n",
      "\n",
      " 63%|██████▎   | 12511/20000 [01:02<00:30, 249.14it/s]\n",
      "\n",
      "\n",
      " 63%|██████▎   | 12538/20000 [01:02<00:29, 254.26it/s]\n",
      "\n",
      "\n",
      " 63%|██████▎   | 12564/20000 [01:02<00:29, 254.17it/s]\n",
      "\n",
      "\n",
      " 63%|██████▎   | 12590/20000 [01:02<00:30, 244.27it/s]\n",
      "\n",
      "\n",
      " 63%|██████▎   | 12617/20000 [01:02<00:29, 250.46it/s]\n",
      "\n",
      "\n",
      " 63%|██████▎   | 12645/20000 [01:03<00:28, 254.22it/s]\n",
      "\n",
      "\n",
      " 63%|██████▎   | 12671/20000 [01:03<00:30, 243.37it/s]\n",
      "\n",
      "\n",
      " 63%|██████▎   | 12697/20000 [01:03<00:29, 244.31it/s]\n",
      "\n",
      "\n",
      " 64%|██████▎   | 12722/20000 [01:03<00:30, 238.88it/s]\n",
      "\n",
      "\n",
      " 64%|██████▍   | 12750/20000 [01:03<00:29, 249.77it/s]\n",
      "\n",
      "\n",
      " 64%|██████▍   | 12776/20000 [01:03<00:29, 241.25it/s]\n",
      "\n",
      "\n",
      " 64%|██████▍   | 12801/20000 [01:03<00:32, 219.57it/s]\n",
      "\n",
      "\n",
      " 64%|██████▍   | 12826/20000 [01:03<00:31, 227.55it/s]\n",
      "\n",
      "\n",
      " 64%|██████▍   | 12850/20000 [01:03<00:31, 226.35it/s]\n",
      "\n",
      "\n",
      " 64%|██████▍   | 12873/20000 [01:04<00:32, 217.58it/s]\n",
      "\n",
      "\n",
      " 64%|██████▍   | 12899/20000 [01:04<00:31, 228.35it/s]\n",
      "\n",
      "\n",
      " 65%|██████▍   | 12923/20000 [01:04<00:31, 227.51it/s]\n",
      "\n",
      "\n",
      " 65%|██████▍   | 12947/20000 [01:04<00:30, 230.81it/s]\n",
      "\n",
      "\n",
      " 65%|██████▍   | 12971/20000 [01:04<00:31, 222.50it/s]\n",
      "\n",
      "\n",
      " 65%|██████▍   | 12996/20000 [01:04<00:30, 228.49it/s]\n",
      "\n",
      "\n",
      " 65%|██████▌   | 13020/20000 [01:04<00:30, 228.57it/s]\n",
      "\n",
      "\n",
      " 65%|██████▌   | 13043/20000 [01:04<00:33, 205.40it/s]\n",
      "\n",
      "\n",
      " 65%|██████▌   | 13067/20000 [01:04<00:32, 213.34it/s]\n",
      "\n",
      "\n",
      " 65%|██████▌   | 13089/20000 [01:05<00:33, 205.40it/s]\n",
      "\n",
      "\n",
      " 66%|██████▌   | 13113/20000 [01:05<00:33, 208.53it/s]\n",
      "\n",
      "\n",
      " 66%|██████▌   | 13135/20000 [01:05<00:32, 210.57it/s]\n",
      "\n",
      "\n",
      " 66%|██████▌   | 13157/20000 [01:05<00:32, 209.81it/s]\n",
      "\n",
      "\n",
      " 66%|██████▌   | 13179/20000 [01:05<00:33, 201.46it/s]\n",
      "\n",
      "\n",
      " 66%|██████▌   | 13202/20000 [01:05<00:32, 208.92it/s]\n",
      "\n",
      "\n",
      " 66%|██████▌   | 13224/20000 [01:05<00:33, 204.25it/s]\n",
      "\n",
      "\n",
      " 66%|██████▌   | 13247/20000 [01:05<00:32, 211.00it/s]\n",
      "\n",
      "\n",
      " 66%|██████▋   | 13270/20000 [01:05<00:31, 212.29it/s]\n",
      "\n",
      "\n",
      " 66%|██████▋   | 13298/20000 [01:06<00:29, 226.82it/s]\n",
      "\n",
      "\n",
      " 67%|██████▋   | 13322/20000 [01:06<00:29, 227.90it/s]\n",
      "\n",
      "\n",
      " 67%|██████▋   | 13347/20000 [01:06<00:28, 232.31it/s]\n",
      "\n",
      "\n",
      " 67%|██████▋   | 13371/20000 [01:06<00:29, 223.75it/s]\n",
      "\n",
      "\n",
      " 67%|██████▋   | 13394/20000 [01:06<00:30, 213.50it/s]\n",
      "\n",
      "\n",
      " 67%|██████▋   | 13416/20000 [01:06<00:30, 214.22it/s]\n",
      "\n",
      "\n",
      " 67%|██████▋   | 13438/20000 [01:06<00:30, 212.05it/s]\n",
      "\n",
      "\n",
      " 67%|██████▋   | 13460/20000 [01:06<00:31, 205.14it/s]\n",
      "\n",
      "\n",
      " 67%|██████▋   | 13487/20000 [01:06<00:29, 217.88it/s]\n",
      "\n",
      "\n",
      " 68%|██████▊   | 13510/20000 [01:06<00:30, 214.43it/s]\n",
      "\n",
      "\n",
      " 68%|██████▊   | 13540/20000 [01:07<00:27, 231.45it/s]\n",
      "\n",
      "\n",
      " 68%|██████▊   | 13564/20000 [01:07<00:27, 230.97it/s]\n",
      "\n",
      "\n",
      " 68%|██████▊   | 13588/20000 [01:07<00:28, 221.17it/s]\n",
      "\n",
      "\n",
      " 68%|██████▊   | 13611/20000 [01:07<00:31, 205.77it/s]\n",
      "\n",
      "\n",
      " 68%|██████▊   | 13633/20000 [01:07<00:30, 205.86it/s]\n",
      "\n",
      "\n",
      " 68%|██████▊   | 13654/20000 [01:07<00:30, 206.58it/s]\n",
      "\n",
      "\n",
      " 68%|██████▊   | 13679/20000 [01:07<00:29, 217.25it/s]\n",
      "\n",
      "\n",
      " 69%|██████▊   | 13702/20000 [01:07<00:32, 191.80it/s]\n",
      "\n",
      "\n",
      " 69%|██████▊   | 13724/20000 [01:08<00:31, 198.03it/s]\n",
      "\n",
      "\n",
      " 69%|██████▊   | 13745/20000 [01:08<00:31, 201.28it/s]\n",
      "\n",
      "\n",
      " 69%|██████▉   | 13766/20000 [01:08<00:33, 185.33it/s]\n",
      "\n",
      "\n",
      " 69%|██████▉   | 13787/20000 [01:08<00:32, 190.98it/s]\n",
      "\n",
      "\n",
      " 69%|██████▉   | 13807/20000 [01:08<00:32, 188.04it/s]\n",
      "\n",
      "\n",
      " 69%|██████▉   | 13827/20000 [01:08<00:33, 184.62it/s]\n",
      "\n",
      "\n",
      " 69%|██████▉   | 13846/20000 [01:08<00:33, 183.64it/s]\n",
      "\n",
      "\n",
      " 69%|██████▉   | 13868/20000 [01:08<00:31, 192.11it/s]\n",
      "\n",
      "\n",
      " 69%|██████▉   | 13889/20000 [01:08<00:31, 196.04it/s]\n",
      "\n",
      "\n",
      " 70%|██████▉   | 13911/20000 [01:08<00:30, 202.06it/s]\n",
      "\n",
      "\n",
      " 70%|██████▉   | 13945/20000 [01:09<00:26, 228.98it/s]\n",
      "\n",
      "\n",
      " 70%|██████▉   | 13984/20000 [01:09<00:23, 261.06it/s]\n",
      "\n",
      "\n",
      " 70%|███████   | 14013/20000 [01:09<00:22, 261.98it/s]\n",
      "\n",
      "\n",
      " 70%|███████   | 14045/20000 [01:09<00:21, 276.30it/s]\n",
      "\n",
      "\n",
      " 70%|███████   | 14075/20000 [01:09<00:22, 262.96it/s]\n",
      "\n",
      "\n",
      " 71%|███████   | 14103/20000 [01:09<00:22, 265.43it/s]\n",
      "\n",
      "\n",
      " 71%|███████   | 14131/20000 [01:09<00:22, 266.32it/s]\n",
      "\n",
      "\n",
      " 71%|███████   | 14159/20000 [01:09<00:23, 251.55it/s]\n",
      "\n",
      "\n",
      " 71%|███████   | 14185/20000 [01:09<00:23, 250.03it/s]\n",
      "\n",
      "\n",
      " 71%|███████   | 14220/20000 [01:10<00:21, 271.81it/s]\n",
      "\n",
      "\n",
      " 71%|███████   | 14249/20000 [01:10<00:21, 263.40it/s]\n",
      "\n",
      "\n",
      " 71%|███████▏  | 14278/20000 [01:10<00:21, 269.17it/s]\n",
      "\n",
      "\n",
      " 72%|███████▏  | 14308/20000 [01:10<00:20, 272.53it/s]\n",
      "\n",
      "\n",
      " 72%|███████▏  | 14340/20000 [01:10<00:20, 278.76it/s]\n",
      "\n",
      "\n",
      " 72%|███████▏  | 14375/20000 [01:10<00:19, 293.48it/s]\n",
      "\n",
      "\n",
      " 72%|███████▏  | 14415/20000 [01:10<00:17, 316.87it/s]\n",
      "\n",
      "\n",
      " 72%|███████▏  | 14448/20000 [01:10<00:19, 291.35it/s]\n",
      "\n",
      "\n",
      " 72%|███████▏  | 14479/20000 [01:10<00:19, 281.47it/s]\n",
      "\n",
      "\n",
      " 73%|███████▎  | 14508/20000 [01:11<00:20, 272.88it/s]\n",
      "\n",
      "\n",
      " 73%|███████▎  | 14536/20000 [01:11<00:20, 270.86it/s]\n",
      "\n",
      "\n",
      " 73%|███████▎  | 14564/20000 [01:11<00:21, 250.11it/s]\n",
      "\n",
      "\n",
      " 73%|███████▎  | 14593/20000 [01:11<00:21, 256.67it/s]\n",
      "\n",
      "\n",
      " 73%|███████▎  | 14627/20000 [01:11<00:19, 275.05it/s]\n",
      "\n",
      "\n",
      " 73%|███████▎  | 14657/20000 [01:11<00:19, 278.50it/s]\n",
      "\n",
      "\n",
      " 73%|███████▎  | 14686/20000 [01:11<00:19, 271.97it/s]\n",
      "\n",
      "\n",
      " 74%|███████▎  | 14717/20000 [01:11<00:18, 278.56it/s]\n",
      "\n",
      "\n",
      " 74%|███████▎  | 14748/20000 [01:11<00:18, 278.73it/s]\n",
      "\n",
      "\n",
      " 74%|███████▍  | 14781/20000 [01:12<00:18, 285.57it/s]\n",
      "\n",
      "\n",
      " 74%|███████▍  | 14810/20000 [01:12<00:18, 282.04it/s]\n",
      "\n",
      "\n",
      " 74%|███████▍  | 14839/20000 [01:12<00:18, 276.49it/s]\n",
      "\n",
      "\n",
      " 74%|███████▍  | 14867/20000 [01:12<00:19, 266.96it/s]\n",
      "\n",
      "\n",
      " 74%|███████▍  | 14894/20000 [01:12<00:20, 252.03it/s]\n",
      "\n",
      "\n",
      " 75%|███████▍  | 14924/20000 [01:12<00:19, 261.59it/s]\n",
      "\n",
      "\n",
      " 75%|███████▍  | 14951/20000 [01:12<00:19, 259.76it/s]\n",
      "\n",
      "\n",
      " 75%|███████▍  | 14980/20000 [01:12<00:18, 266.76it/s]\n",
      "\n",
      "\n",
      " 75%|███████▌  | 15007/20000 [01:12<00:18, 266.21it/s]\n",
      "\n",
      "\n",
      " 75%|███████▌  | 15034/20000 [01:13<00:19, 253.37it/s]\n",
      "\n",
      "\n",
      " 75%|███████▌  | 15060/20000 [01:13<00:19, 249.87it/s]\n",
      "\n",
      "\n",
      " 75%|███████▌  | 15086/20000 [01:13<00:19, 248.24it/s]\n",
      "\n",
      "\n",
      " 76%|███████▌  | 15111/20000 [01:13<00:20, 240.63it/s]\n",
      "\n",
      "\n",
      " 76%|███████▌  | 15140/20000 [01:13<00:19, 253.36it/s]\n",
      "\n",
      "\n",
      " 76%|███████▌  | 15167/20000 [01:13<00:18, 257.05it/s]\n",
      "\n",
      "\n",
      " 76%|███████▌  | 15193/20000 [01:13<00:19, 246.94it/s]\n",
      "\n",
      "\n",
      " 76%|███████▌  | 15220/20000 [01:13<00:18, 253.30it/s]\n",
      "\n",
      "\n",
      " 76%|███████▌  | 15246/20000 [01:13<00:18, 252.77it/s]\n",
      "\n",
      "\n",
      " 76%|███████▋  | 15274/20000 [01:14<00:18, 259.03it/s]\n",
      "\n",
      "\n",
      " 77%|███████▋  | 15301/20000 [01:14<00:18, 250.89it/s]\n",
      "\n",
      "\n",
      " 77%|███████▋  | 15327/20000 [01:14<00:19, 244.54it/s]\n",
      "\n",
      "\n",
      " 77%|███████▋  | 15353/20000 [01:14<00:18, 246.51it/s]\n",
      "\n",
      "\n",
      " 77%|███████▋  | 15378/20000 [01:14<00:19, 239.67it/s]\n",
      "\n",
      "\n",
      " 77%|███████▋  | 15403/20000 [01:14<00:20, 228.04it/s]\n",
      "\n",
      "\n",
      " 77%|███████▋  | 15427/20000 [01:14<00:20, 227.69it/s]\n",
      "\n",
      "\n",
      " 77%|███████▋  | 15450/20000 [01:14<00:20, 224.16it/s]\n",
      "\n",
      "\n",
      " 77%|███████▋  | 15473/20000 [01:14<00:20, 222.05it/s]\n",
      "\n",
      "\n",
      " 77%|███████▋  | 15496/20000 [01:15<00:21, 207.27it/s]\n",
      "\n",
      "\n",
      " 78%|███████▊  | 15522/20000 [01:15<00:20, 220.64it/s]\n",
      "\n",
      "\n",
      " 78%|███████▊  | 15545/20000 [01:15<00:20, 222.00it/s]\n",
      "\n",
      "\n",
      " 78%|███████▊  | 15572/20000 [01:15<00:18, 233.26it/s]\n",
      "\n",
      "\n",
      " 78%|███████▊  | 15596/20000 [01:15<00:19, 222.07it/s]\n",
      "\n",
      "\n",
      " 78%|███████▊  | 15623/20000 [01:15<00:18, 231.64it/s]\n",
      "\n",
      "\n",
      " 78%|███████▊  | 15649/20000 [01:15<00:18, 236.65it/s]\n",
      "\n",
      "\n",
      " 78%|███████▊  | 15675/20000 [01:15<00:17, 241.14it/s]\n",
      "\n",
      "\n",
      " 78%|███████▊  | 15700/20000 [01:15<00:18, 230.95it/s]\n",
      "\n",
      "\n",
      " 79%|███████▊  | 15724/20000 [01:15<00:19, 222.06it/s]\n",
      "\n",
      "\n",
      " 79%|███████▊  | 15747/20000 [01:16<00:19, 217.19it/s]\n",
      "\n",
      "\n",
      " 79%|███████▉  | 15771/20000 [01:16<00:18, 222.92it/s]\n",
      "\n",
      "\n",
      " 79%|███████▉  | 15794/20000 [01:16<00:19, 218.54it/s]\n",
      "\n",
      "\n",
      " 79%|███████▉  | 15816/20000 [01:16<00:19, 217.98it/s]\n",
      "\n",
      "\n",
      " 79%|███████▉  | 15838/20000 [01:16<00:19, 215.66it/s]\n",
      "\n",
      "\n",
      " 79%|███████▉  | 15867/20000 [01:16<00:17, 229.93it/s]\n",
      "\n",
      "\n",
      " 79%|███████▉  | 15891/20000 [01:16<00:17, 229.46it/s]\n",
      "\n",
      "\n",
      " 80%|███████▉  | 15915/20000 [01:16<00:18, 217.46it/s]\n",
      "\n",
      "\n",
      " 80%|███████▉  | 15938/20000 [01:16<00:19, 205.91it/s]\n",
      "\n",
      "\n",
      " 80%|███████▉  | 15960/20000 [01:17<00:19, 208.24it/s]\n",
      "\n",
      "\n",
      " 80%|███████▉  | 15984/20000 [01:17<00:18, 214.39it/s]\n",
      "\n",
      "\n",
      " 80%|████████  | 16006/20000 [01:17<00:18, 212.62it/s]\n",
      "\n",
      "\n",
      " 80%|████████  | 16030/20000 [01:17<00:18, 218.94it/s]\n",
      "\n",
      "\n",
      " 80%|████████  | 16061/20000 [01:17<00:16, 239.10it/s]\n",
      "\n",
      "\n",
      " 80%|████████  | 16086/20000 [01:17<00:18, 214.60it/s]\n",
      "\n",
      "\n",
      " 81%|████████  | 16109/20000 [01:17<00:18, 211.45it/s]\n",
      "\n",
      "\n",
      " 81%|████████  | 16132/20000 [01:17<00:18, 213.27it/s]\n",
      "\n",
      "\n",
      " 81%|████████  | 16155/20000 [01:17<00:17, 216.68it/s]\n",
      "\n",
      "\n",
      " 81%|████████  | 16180/20000 [01:18<00:17, 223.80it/s]\n",
      "\n",
      "\n",
      " 81%|████████  | 16206/20000 [01:18<00:16, 232.84it/s]\n",
      "\n",
      "\n",
      " 81%|████████  | 16230/20000 [01:18<00:16, 224.71it/s]\n",
      "\n",
      "\n",
      " 81%|████████▏ | 16253/20000 [01:18<00:17, 218.12it/s]\n",
      "\n",
      "\n",
      " 81%|████████▏ | 16276/20000 [01:18<00:17, 216.78it/s]\n",
      "\n",
      "\n",
      " 82%|████████▏ | 16300/20000 [01:18<00:16, 219.80it/s]\n",
      "\n",
      "\n",
      " 82%|████████▏ | 16323/20000 [01:18<00:17, 215.54it/s]\n",
      "\n",
      "\n",
      " 82%|████████▏ | 16348/20000 [01:18<00:16, 223.19it/s]\n",
      "\n",
      "\n",
      " 82%|████████▏ | 16371/20000 [01:18<00:16, 220.09it/s]\n",
      "\n",
      "\n",
      " 82%|████████▏ | 16394/20000 [01:19<00:16, 218.40it/s]\n",
      "\n",
      "\n",
      " 82%|████████▏ | 16416/20000 [01:19<00:16, 211.23it/s]\n",
      "\n",
      "\n",
      " 82%|████████▏ | 16438/20000 [01:19<00:16, 210.06it/s]\n",
      "\n",
      "\n",
      " 82%|████████▏ | 16463/20000 [01:19<00:16, 219.77it/s]\n",
      "\n",
      "\n",
      " 82%|████████▏ | 16486/20000 [01:19<00:16, 208.85it/s]\n",
      "\n",
      "\n",
      " 83%|████████▎ | 16508/20000 [01:19<00:17, 202.67it/s]\n",
      "\n",
      "\n",
      " 83%|████████▎ | 16529/20000 [01:19<00:17, 195.97it/s]\n",
      "\n",
      "\n",
      " 83%|████████▎ | 16554/20000 [01:19<00:16, 209.28it/s]\n",
      "\n",
      "\n",
      " 83%|████████▎ | 16576/20000 [01:19<00:16, 207.21it/s]\n",
      "\n",
      "\n",
      " 83%|████████▎ | 16601/20000 [01:20<00:15, 216.85it/s]\n",
      "\n",
      "\n",
      " 83%|████████▎ | 16623/20000 [01:20<00:15, 211.94it/s]\n",
      "\n",
      "\n",
      " 83%|████████▎ | 16645/20000 [01:20<00:16, 206.32it/s]\n",
      "\n",
      "\n",
      " 83%|████████▎ | 16668/20000 [01:20<00:15, 211.76it/s]\n",
      "\n",
      "\n",
      " 83%|████████▎ | 16690/20000 [01:20<00:16, 196.18it/s]\n",
      "\n",
      "\n",
      " 84%|████████▎ | 16712/20000 [01:20<00:16, 202.13it/s]\n",
      "\n",
      "\n",
      " 84%|████████▎ | 16733/20000 [01:20<00:16, 196.53it/s]\n",
      "\n",
      "\n",
      " 84%|████████▍ | 16753/20000 [01:20<00:16, 191.35it/s]\n",
      "\n",
      "\n",
      " 84%|████████▍ | 16773/20000 [01:20<00:16, 192.44it/s]\n",
      "\n",
      "\n",
      " 84%|████████▍ | 16797/20000 [01:21<00:15, 203.17it/s]\n",
      "\n",
      "\n",
      " 84%|████████▍ | 16818/20000 [01:21<00:16, 189.35it/s]\n",
      "\n",
      "\n",
      " 84%|████████▍ | 16838/20000 [01:21<00:17, 180.01it/s]\n",
      "\n",
      "\n",
      " 84%|████████▍ | 16857/20000 [01:21<00:18, 171.47it/s]\n",
      "\n",
      "\n",
      " 84%|████████▍ | 16891/20000 [01:21<00:15, 201.39it/s]\n",
      "\n",
      "\n",
      " 85%|████████▍ | 16919/20000 [01:21<00:14, 218.84it/s]\n",
      "\n",
      "\n",
      " 85%|████████▍ | 16958/20000 [01:21<00:12, 251.74it/s]\n",
      "\n",
      "\n",
      " 85%|████████▍ | 16987/20000 [01:21<00:11, 261.30it/s]\n",
      "\n",
      "\n",
      " 85%|████████▌ | 17024/20000 [01:21<00:10, 277.08it/s]\n",
      "\n",
      "\n",
      " 85%|████████▌ | 17057/20000 [01:22<00:10, 288.97it/s]\n",
      "\n",
      "\n",
      " 85%|████████▌ | 17091/20000 [01:22<00:09, 302.04it/s]\n",
      "\n",
      "\n",
      " 86%|████████▌ | 17123/20000 [01:22<00:09, 303.51it/s]\n",
      "\n",
      "\n",
      " 86%|████████▌ | 17163/20000 [01:22<00:08, 322.68it/s]\n",
      "\n",
      "\n",
      " 86%|████████▌ | 17199/20000 [01:22<00:08, 331.18it/s]\n",
      "\n",
      "\n",
      " 86%|████████▌ | 17236/20000 [01:22<00:08, 340.84it/s]\n",
      "\n",
      "\n",
      " 86%|████████▋ | 17271/20000 [01:22<00:08, 318.36it/s]\n",
      "\n",
      "\n",
      " 87%|████████▋ | 17304/20000 [01:22<00:08, 308.23it/s]\n",
      "\n",
      "\n",
      " 87%|████████▋ | 17339/20000 [01:22<00:08, 318.35it/s]\n",
      "\n",
      "\n",
      " 87%|████████▋ | 17372/20000 [01:22<00:08, 302.71it/s]\n",
      "\n",
      "\n",
      " 87%|████████▋ | 17410/20000 [01:23<00:08, 322.18it/s]\n",
      "\n",
      "\n",
      " 87%|████████▋ | 17445/20000 [01:23<00:07, 328.21it/s]\n",
      "\n",
      "\n",
      " 87%|████████▋ | 17479/20000 [01:23<00:08, 302.65it/s]\n",
      "\n",
      "\n",
      " 88%|████████▊ | 17511/20000 [01:23<00:08, 301.33it/s]\n",
      "\n",
      "\n",
      " 88%|████████▊ | 17545/20000 [01:23<00:07, 308.15it/s]\n",
      "\n",
      "\n",
      " 88%|████████▊ | 17580/20000 [01:23<00:07, 319.06it/s]\n",
      "\n",
      "\n",
      " 88%|████████▊ | 17613/20000 [01:23<00:07, 321.75it/s]\n",
      "\n",
      "\n",
      " 88%|████████▊ | 17646/20000 [01:23<00:07, 313.69it/s]\n",
      "\n",
      "\n",
      " 88%|████████▊ | 17678/20000 [01:23<00:07, 306.48it/s]\n",
      "\n",
      "\n",
      " 89%|████████▊ | 17709/20000 [01:24<00:07, 301.62it/s]\n",
      "\n",
      "\n",
      " 89%|████████▊ | 17744/20000 [01:24<00:07, 314.59it/s]\n",
      "\n",
      "\n",
      " 89%|████████▉ | 17777/20000 [01:24<00:07, 317.45it/s]\n",
      "\n",
      "\n",
      " 89%|████████▉ | 17809/20000 [01:24<00:07, 294.81it/s]\n",
      "\n",
      "\n",
      " 89%|████████▉ | 17839/20000 [01:24<00:07, 295.04it/s]\n",
      "\n",
      "\n",
      " 89%|████████▉ | 17869/20000 [01:24<00:07, 280.59it/s]\n",
      "\n",
      "\n",
      " 89%|████████▉ | 17899/20000 [01:24<00:07, 283.32it/s]\n",
      "\n",
      "\n",
      " 90%|████████▉ | 17928/20000 [01:24<00:07, 281.31it/s]\n",
      "\n",
      "\n",
      " 90%|████████▉ | 17957/20000 [01:24<00:07, 273.43it/s]\n",
      "\n",
      "\n",
      " 90%|████████▉ | 17990/20000 [01:25<00:06, 287.32it/s]\n",
      "\n",
      "\n",
      " 90%|█████████ | 18021/20000 [01:25<00:06, 288.82it/s]\n",
      "\n",
      "\n",
      " 90%|█████████ | 18051/20000 [01:25<00:07, 270.62it/s]\n",
      "\n",
      "\n",
      " 90%|█████████ | 18080/20000 [01:25<00:07, 273.87it/s]\n",
      "\n",
      "\n",
      " 91%|█████████ | 18114/20000 [01:25<00:06, 289.21it/s]\n",
      "\n",
      "\n",
      " 91%|█████████ | 18144/20000 [01:25<00:06, 288.35it/s]\n",
      "\n",
      "\n",
      " 91%|█████████ | 18175/20000 [01:25<00:06, 293.21it/s]\n",
      "\n",
      "\n",
      " 91%|█████████ | 18205/20000 [01:25<00:06, 280.05it/s]\n",
      "\n",
      "\n",
      " 91%|█████████ | 18234/20000 [01:25<00:06, 276.80it/s]\n",
      "\n",
      "\n",
      " 91%|█████████▏| 18262/20000 [01:26<00:06, 261.33it/s]\n",
      "\n",
      "\n",
      " 91%|█████████▏| 18289/20000 [01:26<00:06, 255.95it/s]\n",
      "\n",
      "\n",
      " 92%|█████████▏| 18315/20000 [01:26<00:06, 255.76it/s]\n",
      "\n",
      "\n",
      " 92%|█████████▏| 18349/20000 [01:26<00:05, 275.94it/s]\n",
      "\n",
      "\n",
      " 92%|█████████▏| 18378/20000 [01:26<00:05, 271.94it/s]\n",
      "\n",
      "\n",
      " 92%|█████████▏| 18407/20000 [01:26<00:05, 275.97it/s]\n",
      "\n",
      "\n",
      " 92%|█████████▏| 18443/20000 [01:26<00:05, 296.07it/s]\n",
      "\n",
      "\n",
      " 92%|█████████▏| 18474/20000 [01:26<00:05, 295.19it/s]\n",
      "\n",
      "\n",
      " 93%|█████████▎| 18504/20000 [01:26<00:05, 287.91it/s]\n",
      "\n",
      "\n",
      " 93%|█████████▎| 18534/20000 [01:26<00:05, 287.82it/s]\n",
      "\n",
      "\n",
      " 93%|█████████▎| 18564/20000 [01:27<00:05, 285.29it/s]\n",
      "\n",
      "\n",
      " 93%|█████████▎| 18593/20000 [01:27<00:05, 281.13it/s]\n",
      "\n",
      "\n",
      " 93%|█████████▎| 18624/20000 [01:27<00:04, 288.19it/s]\n",
      "\n",
      "\n",
      " 93%|█████████▎| 18653/20000 [01:27<00:04, 275.67it/s]\n",
      "\n",
      "\n",
      " 93%|█████████▎| 18681/20000 [01:27<00:04, 270.30it/s]\n",
      "\n",
      "\n",
      " 94%|█████████▎| 18714/20000 [01:27<00:04, 285.50it/s]\n",
      "\n",
      "\n",
      " 94%|█████████▎| 18743/20000 [01:27<00:04, 259.44it/s]\n",
      "\n",
      "\n",
      " 94%|█████████▍| 18770/20000 [01:27<00:04, 262.21it/s]\n",
      "\n",
      "\n",
      " 94%|█████████▍| 18799/20000 [01:27<00:04, 269.67it/s]\n",
      "\n",
      "\n",
      " 94%|█████████▍| 18827/20000 [01:28<00:04, 250.23it/s]\n",
      "\n",
      "\n",
      " 94%|█████████▍| 18853/20000 [01:28<00:04, 252.58it/s]\n",
      "\n",
      "\n",
      " 94%|█████████▍| 18881/20000 [01:28<00:04, 259.57it/s]\n",
      "\n",
      "\n",
      " 95%|█████████▍| 18912/20000 [01:28<00:04, 270.86it/s]\n",
      "\n",
      "\n",
      " 95%|█████████▍| 18941/20000 [01:28<00:03, 276.31it/s]\n",
      "\n",
      "\n",
      " 95%|█████████▍| 18969/20000 [01:28<00:03, 264.15it/s]\n",
      "\n",
      "\n",
      " 95%|█████████▍| 18996/20000 [01:28<00:03, 251.82it/s]\n",
      "\n",
      "\n",
      " 95%|█████████▌| 19022/20000 [01:28<00:03, 249.66it/s]\n",
      "\n",
      "\n",
      " 95%|█████████▌| 19048/20000 [01:28<00:03, 250.90it/s]\n",
      "\n",
      "\n",
      " 95%|█████████▌| 19074/20000 [01:29<00:04, 227.00it/s]\n",
      "\n",
      "\n",
      " 95%|█████████▌| 19099/20000 [01:29<00:03, 233.43it/s]\n",
      "\n",
      "\n",
      " 96%|█████████▌| 19125/20000 [01:29<00:03, 238.51it/s]\n",
      "\n",
      "\n",
      " 96%|█████████▌| 19154/20000 [01:29<00:03, 250.25it/s]\n",
      "\n",
      "\n",
      " 96%|█████████▌| 19181/20000 [01:29<00:03, 253.79it/s]\n",
      "\n",
      "\n",
      " 96%|█████████▌| 19207/20000 [01:29<00:03, 250.37it/s]\n",
      "\n",
      "\n",
      " 96%|█████████▌| 19234/20000 [01:29<00:03, 252.97it/s]\n",
      "\n",
      "\n",
      " 96%|█████████▋| 19262/20000 [01:29<00:02, 257.35it/s]\n",
      "\n",
      "\n",
      " 96%|█████████▋| 19288/20000 [01:29<00:02, 245.62it/s]\n",
      "\n",
      "\n",
      " 97%|█████████▋| 19315/20000 [01:30<00:02, 249.37it/s]\n",
      "\n",
      "\n",
      " 97%|█████████▋| 19341/20000 [01:30<00:02, 232.15it/s]\n",
      "\n",
      "\n",
      " 97%|█████████▋| 19365/20000 [01:30<00:02, 232.97it/s]\n",
      "\n",
      "\n",
      " 97%|█████████▋| 19390/20000 [01:30<00:02, 236.31it/s]\n",
      "\n",
      "\n",
      " 97%|█████████▋| 19414/20000 [01:30<00:02, 232.15it/s]\n",
      "\n",
      "\n",
      " 97%|█████████▋| 19443/20000 [01:30<00:02, 245.67it/s]\n",
      "\n",
      "\n",
      " 97%|█████████▋| 19470/20000 [01:30<00:02, 246.50it/s]\n",
      "\n",
      "\n",
      " 97%|█████████▋| 19496/20000 [01:30<00:02, 247.43it/s]\n",
      "\n",
      "\n",
      " 98%|█████████▊| 19523/20000 [01:30<00:01, 252.72it/s]\n",
      "\n",
      "\n",
      " 98%|█████████▊| 19549/20000 [01:30<00:01, 245.16it/s]\n",
      "\n",
      "\n",
      " 98%|█████████▊| 19576/20000 [01:31<00:01, 243.82it/s]\n",
      "\n",
      "\n",
      " 98%|█████████▊| 19601/20000 [01:31<00:01, 245.02it/s]\n",
      "\n",
      "\n",
      " 98%|█████████▊| 19628/20000 [01:31<00:01, 247.54it/s]\n",
      "\n",
      "\n",
      " 98%|█████████▊| 19657/20000 [01:31<00:01, 255.99it/s]\n",
      "\n",
      "\n",
      " 98%|█████████▊| 19683/20000 [01:31<00:01, 254.51it/s]\n",
      "\n",
      "\n",
      " 99%|█████████▊| 19709/20000 [01:31<00:01, 240.42it/s]\n",
      "\n",
      "\n",
      " 99%|█████████▊| 19738/20000 [01:31<00:01, 251.70it/s]\n",
      "\n",
      "\n",
      " 99%|█████████▉| 19764/20000 [01:31<00:00, 253.69it/s]\n",
      "\n",
      "\n",
      " 99%|█████████▉| 19792/20000 [01:31<00:00, 259.23it/s]\n",
      "\n",
      "\n",
      " 99%|█████████▉| 19819/20000 [01:32<00:00, 244.14it/s]\n",
      "\n",
      "\n",
      " 99%|█████████▉| 19844/20000 [01:32<00:00, 240.93it/s]\n",
      "\n",
      "\n",
      " 99%|█████████▉| 19869/20000 [01:32<00:00, 235.51it/s]\n",
      "\n",
      "\n",
      " 99%|█████████▉| 19893/20000 [01:32<00:00, 224.94it/s]\n",
      "\n",
      "\n",
      "100%|█████████▉| 19919/20000 [01:32<00:00, 233.51it/s]\n",
      "\n",
      "\n",
      "100%|█████████▉| 19946/20000 [01:32<00:00, 242.47it/s]\n",
      "\n",
      "\n",
      "100%|█████████▉| 19971/20000 [01:32<00:00, 235.85it/s]\n",
      "\n",
      "\n",
      "100%|██████████| 20000/20000 [01:32<00:00, 215.44it/s]\n"
     ]
    }
   ],
   "source": [
    "#get 20000 post test\n",
    "from tqdm import tqdm\n",
    "stop_word_path = \"stopwords/baidu_stopwords.txt\"\n",
    "# read stop words\n",
    "stopwords = [line.strip() for line in open(stop_word_path, 'r', encoding='utf-8').readlines()]\n",
    "\n",
    "stopwords += [\" \", \"，\", \"。\"]\n",
    "\n",
    "len(stopwords)\n",
    "lines = [[post[\"title\"]] + [chat[\"value\"] for chat in post[\"chats\"]] for post in data]\n",
    "\n",
    "import jieba\n",
    "from functools import reduce\n",
    "from tqdm import tqdm\n",
    "\n",
    "# seperate word\n",
    "x = []\n",
    "for cluster in tqdm(lines):\n",
    "    x_line = []\n",
    "    for line in cluster:\n",
    "        tmp = [char for char in jieba.lcut(line) if char not in stopwords]\n",
    "        x_line.append(tmp)\n",
    "    x.append(reduce(lambda a, b: a+b, x_line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6229a17c-37dc-428f-aa88-5ea5aa1d362c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-24T13:15:22.333980Z",
     "iopub.status.busy": "2021-10-24T13:15:22.333632Z",
     "iopub.status.idle": "2021-10-24T13:15:22.460932Z",
     "shell.execute_reply": "2021-10-24T13:15:22.460017Z",
     "shell.execute_reply.started": "2021-10-24T13:15:22.333926Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [00:00<00:00, 338145.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1.13': 0, '1.16': 1, '1.6': 2, '1.9': 3, '1.14': 4, '1.7': 5, '1.12': 6, '1.3': 7, '1.15': 8, '1.8': 9, '1.2': 10, '1.1': 11, '1.10': 12, '1.11': 13, '1.4': 14, '1.5': 15, '1.18': 16, '1.17': 17, '1.19': 18, '2.7': 19, '2.1': 20, '2.2': 21, '2.8': 22, '2.3': 23, '2.4': 24, '2.5': 25, '2.6': 26, '3.4': 27, '3.2': 28, '3.3': 29, '3.6': 30, '3.5': 31}\n",
      "20000 20000 20000\n",
      "[0, 1, 1, 1, 1]\n",
      "Counter({1: 7156, 3: 4968, 7: 1979, 5: 879, 9: 817, 13: 728, 11: 659, 0: 511, 6: 444, 2: 406, 12: 403, 10: 294, 8: 202, 4: 195, 14: 104, 18: 87, 16: 71, 17: 53, 15: 44})\n",
      "Counter({19: 17799, 20: 907, 21: 716, 24: 236, 22: 146, 23: 94, 25: 69, 26: 33})\n",
      "Counter({27: 19612, 28: 247, 29: 88, 30: 49, 31: 4})\n"
     ]
    }
   ],
   "source": [
    "#get label for 3 category of 20000 post\n",
    "from tqdm import tqdm\n",
    "\n",
    "# predict label\n",
    "y_s1_raw = []\n",
    "y_s2_raw = []\n",
    "y_s3_raw = []\n",
    "\n",
    "for post in tqdm(data):\n",
    "    cluster = {item[0]: item[1] for item in post[\"label\"].items()} \n",
    "    y_s1_raw.append(cluster[\"s1\"])\n",
    "    y_s2_raw.append(cluster[\"s2\"])\n",
    "    y_s3_raw.append(cluster[\"s3\"])\n",
    "\n",
    "#mapping\n",
    "y_map = {}\n",
    "for label in y_s1_raw + y_s2_raw + y_s3_raw:\n",
    "    if label not in y_map:\n",
    "        y_map[label] = len(y_map)\n",
    "\n",
    "print(y_map)\n",
    "\n",
    "y_s1 = [y_map[label] for label in y_s1_raw]\n",
    "y_s2 = [y_map[label] for label in y_s2_raw]\n",
    "y_s3 = [y_map[label] for label in y_s3_raw]\n",
    "print(len(y_s1),len(y_s2),len(y_s3))\n",
    "print(y_s1[0:5])\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "#imbalance\n",
    "print( Counter(y_s1)   )\n",
    "print( Counter(y_s2)   )\n",
    "print( Counter(y_s3)   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a11c40eb-ea1e-4612-ae80-aa9535f54a03",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-24T12:57:40.345449Z",
     "iopub.status.busy": "2021-10-24T12:57:40.345083Z",
     "iopub.status.idle": "2021-10-24T12:57:40.382192Z",
     "shell.execute_reply": "2021-10-24T12:57:40.381373Z",
     "shell.execute_reply.started": "2021-10-24T12:57:40.345395Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x1_train, x1_test, y1_train, y1_test \\\n",
    "        = train_test_split(x, y_s1, test_size=0.3)\n",
    "x2_train, x2_test, y2_train, y2_test \\\n",
    "        = train_test_split(x, y_s2, test_size=0.3)\n",
    "x3_train, x3_test, y3_train, y3_test \\\n",
    "        = train_test_split(x, y_s3, test_size=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ceb6c66-9685-4de6-9e31-020c80e3251d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "28b84198-d485-4e83-822d-4de693a8810c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-24T12:34:25.501521Z",
     "iopub.status.busy": "2021-10-24T12:34:25.501161Z",
     "iopub.status.idle": "2021-10-24T12:34:32.849127Z",
     "shell.execute_reply": "2021-10-24T12:34:32.848377Z",
     "shell.execute_reply.started": "2021-10-24T12:34:25.501466Z"
    }
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "# print(token_embedding.search(x_train[0]))\n",
    "word2vec_model = gensim.models.Word2Vec(x1_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "344a6f62-83f8-42af-85fb-f7cdb12cef54",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-24T12:34:32.867117Z",
     "iopub.status.busy": "2021-10-24T12:34:32.866884Z",
     "iopub.status.idle": "2021-10-24T12:35:09.609455Z",
     "shell.execute_reply": "2021-10-24T12:35:09.608502Z",
     "shell.execute_reply.started": "2021-10-24T12:34:32.867072Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/14000 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      "  0%|          | 38/14000 [00:00<00:36, 379.01it/s]\n",
      "\n",
      "\n",
      "  1%|          | 85/14000 [00:00<00:34, 400.07it/s]\n",
      "\n",
      "\n",
      "  1%|          | 143/14000 [00:00<00:31, 440.23it/s]\n",
      "\n",
      "\n",
      "  1%|▏         | 186/14000 [00:00<00:31, 436.16it/s]\n",
      "\n",
      "\n",
      "  2%|▏         | 242/14000 [00:00<00:29, 466.57it/s]\n",
      "\n",
      "\n",
      "  2%|▏         | 284/14000 [00:00<00:34, 399.10it/s]\n",
      "\n",
      "\n",
      "  2%|▏         | 342/14000 [00:00<00:31, 439.68it/s]\n",
      "\n",
      "\n",
      "  3%|▎         | 394/14000 [00:00<00:29, 458.01it/s]\n",
      "\n",
      "\n",
      "  3%|▎         | 447/14000 [00:00<00:28, 476.78it/s]\n",
      "\n",
      "\n",
      "  4%|▎         | 502/14000 [00:01<00:27, 495.08it/s]\n",
      "\n",
      "\n",
      "  4%|▍         | 562/14000 [00:01<00:25, 522.45it/s]\n",
      "\n",
      "\n",
      "  4%|▍         | 617/14000 [00:01<00:25, 530.09it/s]\n",
      "\n",
      "\n",
      "  5%|▍         | 671/14000 [00:01<00:25, 524.26it/s]\n",
      "\n",
      "\n",
      "  5%|▌         | 727/14000 [00:01<00:24, 532.29it/s]\n",
      "\n",
      "\n",
      "  6%|▌         | 781/14000 [00:01<00:24, 532.15it/s]\n",
      "\n",
      "\n",
      "  6%|▌         | 835/14000 [00:01<00:25, 525.71it/s]\n",
      "\n",
      "\n",
      "  6%|▋         | 888/14000 [00:01<00:25, 507.21it/s]\n",
      "\n",
      "\n",
      "  7%|▋         | 939/14000 [00:01<00:26, 492.23it/s]\n",
      "\n",
      "\n",
      "  7%|▋         | 993/14000 [00:01<00:25, 505.38it/s]\n",
      "\n",
      "\n",
      "  8%|▊         | 1052/14000 [00:02<00:24, 526.31it/s]\n",
      "\n",
      "\n",
      "  8%|▊         | 1113/14000 [00:02<00:23, 541.21it/s]\n",
      "\n",
      "\n",
      "  8%|▊         | 1173/14000 [00:02<00:23, 551.88it/s]\n",
      "\n",
      "\n",
      "  9%|▉         | 1239/14000 [00:02<00:22, 579.92it/s]\n",
      "\n",
      "\n",
      "  9%|▉         | 1298/14000 [00:02<00:21, 580.18it/s]\n",
      "\n",
      "\n",
      " 10%|▉         | 1357/14000 [00:02<00:22, 571.48it/s]\n",
      "\n",
      "\n",
      " 10%|█         | 1415/14000 [00:02<00:22, 571.07it/s]\n",
      "\n",
      "\n",
      " 11%|█         | 1473/14000 [00:02<00:22, 567.38it/s]\n",
      "\n",
      "\n",
      " 11%|█         | 1530/14000 [00:02<00:22, 562.61it/s]\n",
      "\n",
      "\n",
      " 11%|█▏        | 1587/14000 [00:03<00:23, 537.07it/s]\n",
      "\n",
      "\n",
      " 12%|█▏        | 1648/14000 [00:03<00:22, 555.66it/s]\n",
      "\n",
      "\n",
      " 12%|█▏        | 1704/14000 [00:03<00:22, 556.70it/s]\n",
      "\n",
      "\n",
      " 13%|█▎        | 1760/14000 [00:03<00:22, 553.15it/s]\n",
      "\n",
      "\n",
      " 13%|█▎        | 1816/14000 [00:03<00:22, 533.85it/s]\n",
      "\n",
      "\n",
      " 13%|█▎        | 1874/14000 [00:03<00:22, 546.72it/s]\n",
      "\n",
      "\n",
      " 14%|█▍        | 1929/14000 [00:03<00:22, 543.73it/s]\n",
      "\n",
      "\n",
      " 14%|█▍        | 1987/14000 [00:03<00:21, 550.97it/s]\n",
      "\n",
      "\n",
      " 15%|█▍        | 2043/14000 [00:03<00:21, 547.76it/s]\n",
      "\n",
      "\n",
      " 15%|█▍        | 2098/14000 [00:03<00:22, 531.56it/s]\n",
      "\n",
      "\n",
      " 15%|█▌        | 2154/14000 [00:04<00:21, 538.75it/s]\n",
      "\n",
      "\n",
      " 16%|█▌        | 2220/14000 [00:04<00:20, 564.40it/s]\n",
      "\n",
      "\n",
      " 16%|█▋        | 2277/14000 [00:04<00:20, 560.21it/s]\n",
      "\n",
      "\n",
      " 17%|█▋        | 2334/14000 [00:04<00:21, 546.06it/s]\n",
      "\n",
      "\n",
      " 17%|█▋        | 2395/14000 [00:04<00:20, 563.44it/s]\n",
      "\n",
      "\n",
      " 18%|█▊        | 2452/14000 [00:04<00:20, 557.59it/s]\n",
      "\n",
      "\n",
      " 18%|█▊        | 2514/14000 [00:04<00:20, 573.90it/s]\n",
      "\n",
      "\n",
      " 18%|█▊        | 2572/14000 [00:04<00:20, 567.97it/s]\n",
      "\n",
      "\n",
      " 19%|█▉        | 2630/14000 [00:04<00:19, 569.93it/s]\n",
      "\n",
      "\n",
      " 19%|█▉        | 2688/14000 [00:05<00:19, 570.78it/s]\n",
      "\n",
      "\n",
      " 20%|█▉        | 2746/14000 [00:05<00:19, 572.62it/s]\n",
      "\n",
      "\n",
      " 20%|██        | 2810/14000 [00:05<00:18, 590.16it/s]\n",
      "\n",
      "\n",
      " 20%|██        | 2870/14000 [00:05<00:19, 561.16it/s]\n",
      "\n",
      "\n",
      " 21%|██        | 2927/14000 [00:05<00:20, 545.60it/s]\n",
      "\n",
      "\n",
      " 21%|██▏       | 2983/14000 [00:05<00:20, 549.41it/s]\n",
      "\n",
      "\n",
      " 22%|██▏       | 3042/14000 [00:05<00:19, 558.15it/s]\n",
      "\n",
      "\n",
      " 22%|██▏       | 3099/14000 [00:05<00:19, 559.13it/s]\n",
      "\n",
      "\n",
      " 23%|██▎       | 3156/14000 [00:05<00:19, 554.70it/s]\n",
      "\n",
      "\n",
      " 23%|██▎       | 3221/14000 [00:05<00:18, 579.28it/s]\n",
      "\n",
      "\n",
      " 23%|██▎       | 3286/14000 [00:06<00:17, 598.22it/s]\n",
      "\n",
      "\n",
      " 24%|██▍       | 3347/14000 [00:06<00:17, 596.15it/s]\n",
      "\n",
      "\n",
      " 24%|██▍       | 3407/14000 [00:06<00:18, 573.69it/s]\n",
      "\n",
      "\n",
      " 25%|██▍       | 3465/14000 [00:06<00:18, 556.01it/s]\n",
      "\n",
      "\n",
      " 25%|██▌       | 3522/14000 [00:06<00:18, 557.26it/s]\n",
      "\n",
      "\n",
      " 26%|██▌       | 3578/14000 [00:06<00:19, 534.67it/s]\n",
      "\n",
      "\n",
      " 26%|██▌       | 3648/14000 [00:06<00:17, 575.14it/s]\n",
      "\n",
      "\n",
      " 26%|██▋       | 3707/14000 [00:06<00:19, 535.83it/s]\n",
      "\n",
      "\n",
      " 27%|██▋       | 3762/14000 [00:06<00:19, 524.83it/s]\n",
      "\n",
      "\n",
      " 27%|██▋       | 3820/14000 [00:07<00:18, 537.91it/s]\n",
      "\n",
      "\n",
      " 28%|██▊       | 3879/14000 [00:07<00:18, 546.20it/s]\n",
      "\n",
      "\n",
      " 28%|██▊       | 3935/14000 [00:07<00:19, 525.48it/s]\n",
      "\n",
      "\n",
      " 28%|██▊       | 3989/14000 [00:07<00:19, 524.12it/s]\n",
      "\n",
      "\n",
      " 29%|██▉       | 4044/14000 [00:07<00:18, 530.77it/s]\n",
      "\n",
      "\n",
      " 29%|██▉       | 4098/14000 [00:07<00:19, 496.54it/s]\n",
      "\n",
      "\n",
      " 30%|██▉       | 4159/14000 [00:07<00:18, 524.82it/s]\n",
      "\n",
      "\n",
      " 30%|███       | 4216/14000 [00:07<00:18, 537.04it/s]\n",
      "\n",
      "\n",
      " 31%|███       | 4273/14000 [00:07<00:17, 545.61it/s]\n",
      "\n",
      "\n",
      " 31%|███       | 4329/14000 [00:07<00:18, 528.47it/s]\n",
      "\n",
      "\n",
      " 31%|███▏      | 4386/14000 [00:08<00:17, 538.71it/s]\n",
      "\n",
      "\n",
      " 32%|███▏      | 4441/14000 [00:08<00:18, 518.85it/s]\n",
      "\n",
      "\n",
      " 32%|███▏      | 4503/14000 [00:08<00:17, 545.34it/s]\n",
      "\n",
      "\n",
      " 33%|███▎      | 4564/14000 [00:08<00:16, 562.60it/s]\n",
      "\n",
      "\n",
      " 33%|███▎      | 4622/14000 [00:08<00:16, 563.73it/s]\n",
      "\n",
      "\n",
      " 33%|███▎      | 4683/14000 [00:08<00:16, 574.74it/s]\n",
      "\n",
      "\n",
      " 34%|███▍      | 4741/14000 [00:08<00:16, 557.44it/s]\n",
      "\n",
      "\n",
      " 34%|███▍      | 4799/14000 [00:08<00:16, 560.50it/s]\n",
      "\n",
      "\n",
      " 35%|███▍      | 4857/14000 [00:08<00:16, 562.16it/s]\n",
      "\n",
      "\n",
      " 35%|███▌      | 4914/14000 [00:09<00:16, 546.50it/s]\n",
      "\n",
      "\n",
      " 36%|███▌      | 4974/14000 [00:09<00:16, 561.24it/s]\n",
      "\n",
      "\n",
      " 36%|███▌      | 5031/14000 [00:09<00:16, 558.76it/s]\n",
      "\n",
      "\n",
      " 36%|███▋      | 5088/14000 [00:09<00:16, 534.91it/s]\n",
      "\n",
      "\n",
      " 37%|███▋      | 5142/14000 [00:09<00:17, 518.70it/s]\n",
      "\n",
      "\n",
      " 37%|███▋      | 5200/14000 [00:09<00:16, 535.63it/s]\n",
      "\n",
      "\n",
      " 38%|███▊      | 5259/14000 [00:09<00:15, 549.49it/s]\n",
      "\n",
      "\n",
      " 38%|███▊      | 5315/14000 [00:09<00:15, 546.58it/s]\n",
      "\n",
      "\n",
      " 38%|███▊      | 5374/14000 [00:09<00:15, 555.69it/s]\n",
      "\n",
      "\n",
      " 39%|███▉      | 5432/14000 [00:09<00:15, 562.22it/s]\n",
      "\n",
      "\n",
      " 39%|███▉      | 5489/14000 [00:10<00:15, 554.96it/s]\n",
      "\n",
      "\n",
      " 40%|███▉      | 5545/14000 [00:10<00:16, 524.11it/s]\n",
      "\n",
      "\n",
      " 40%|████      | 5600/14000 [00:10<00:15, 529.56it/s]\n",
      "\n",
      "\n",
      " 40%|████      | 5654/14000 [00:10<00:16, 519.96it/s]\n",
      "\n",
      "\n",
      " 41%|████      | 5707/14000 [00:10<00:16, 516.55it/s]\n",
      "\n",
      "\n",
      " 41%|████      | 5771/14000 [00:10<00:15, 547.64it/s]\n",
      "\n",
      "\n",
      " 42%|████▏     | 5829/14000 [00:10<00:14, 549.79it/s]\n",
      "\n",
      "\n",
      " 42%|████▏     | 5885/14000 [00:10<00:15, 538.63it/s]\n",
      "\n",
      "\n",
      " 42%|████▏     | 5940/14000 [00:10<00:15, 525.08it/s]\n",
      "\n",
      "\n",
      " 43%|████▎     | 6005/14000 [00:11<00:14, 556.67it/s]\n",
      "\n",
      "\n",
      " 43%|████▎     | 6063/14000 [00:11<00:14, 559.17it/s]\n",
      "\n",
      "\n",
      " 44%|████▎     | 6120/14000 [00:11<00:14, 560.77it/s]\n",
      "\n",
      "\n",
      " 44%|████▍     | 6177/14000 [00:11<00:14, 551.24it/s]\n",
      "\n",
      "\n",
      " 45%|████▍     | 6233/14000 [00:11<00:14, 537.26it/s]\n",
      "\n",
      "\n",
      " 45%|████▍     | 6299/14000 [00:11<00:13, 567.84it/s]\n",
      "\n",
      "\n",
      " 45%|████▌     | 6357/14000 [00:11<00:13, 562.23it/s]\n",
      "\n",
      "\n",
      " 46%|████▌     | 6416/14000 [00:11<00:13, 566.88it/s]\n",
      "\n",
      "\n",
      " 46%|████▌     | 6474/14000 [00:11<00:13, 564.66it/s]\n",
      "\n",
      "\n",
      " 47%|████▋     | 6531/14000 [00:11<00:13, 558.89it/s]\n",
      "\n",
      "\n",
      " 47%|████▋     | 6588/14000 [00:12<00:13, 532.76it/s]\n",
      "\n",
      "\n",
      " 47%|████▋     | 6642/14000 [00:12<00:14, 513.52it/s]\n",
      "\n",
      "\n",
      " 48%|████▊     | 6701/14000 [00:12<00:13, 532.98it/s]\n",
      "\n",
      "\n",
      " 48%|████▊     | 6755/14000 [00:12<00:13, 531.65it/s]\n",
      "\n",
      "\n",
      " 49%|████▊     | 6816/14000 [00:12<00:13, 545.78it/s]\n",
      "\n",
      "\n",
      " 49%|████▉     | 6871/14000 [00:12<00:13, 531.24it/s]\n",
      "\n",
      "\n",
      " 50%|████▉     | 6931/14000 [00:12<00:12, 549.63it/s]\n",
      "\n",
      "\n",
      " 50%|████▉     | 6987/14000 [00:12<00:13, 524.43it/s]\n",
      "\n",
      "\n",
      " 50%|█████     | 7045/14000 [00:12<00:12, 538.49it/s]\n",
      "\n",
      "\n",
      " 51%|█████     | 7100/14000 [00:13<00:12, 538.88it/s]\n",
      "\n",
      "\n",
      " 51%|█████     | 7160/14000 [00:13<00:12, 552.23it/s]\n",
      "\n",
      "\n",
      " 52%|█████▏    | 7223/14000 [00:13<00:11, 572.78it/s]\n",
      "\n",
      "\n",
      " 52%|█████▏    | 7281/14000 [00:13<00:33, 197.94it/s]\n",
      "\n",
      "\n",
      " 52%|█████▏    | 7347/14000 [00:14<00:26, 250.32it/s]\n",
      "\n",
      "\n",
      " 53%|█████▎    | 7397/14000 [00:14<00:22, 290.86it/s]\n",
      "\n",
      "\n",
      " 53%|█████▎    | 7458/14000 [00:14<00:18, 344.78it/s]\n",
      "\n",
      "\n",
      " 54%|█████▎    | 7518/14000 [00:14<00:16, 394.73it/s]\n",
      "\n",
      "\n",
      " 54%|█████▍    | 7581/14000 [00:14<00:14, 443.49it/s]\n",
      "\n",
      "\n",
      " 55%|█████▍    | 7638/14000 [00:14<00:13, 472.15it/s]\n",
      "\n",
      "\n",
      " 55%|█████▌    | 7701/14000 [00:14<00:12, 510.13it/s]\n",
      "\n",
      "\n",
      " 55%|█████▌    | 7763/14000 [00:14<00:11, 522.00it/s]\n",
      "\n",
      "\n",
      " 56%|█████▌    | 7821/14000 [00:14<00:11, 530.87it/s]\n",
      "\n",
      "\n",
      " 56%|█████▋    | 7881/14000 [00:15<00:11, 547.46it/s]\n",
      "\n",
      "\n",
      " 57%|█████▋    | 7943/14000 [00:15<00:10, 564.95it/s]\n",
      "\n",
      "\n",
      " 57%|█████▋    | 8002/14000 [00:15<00:10, 560.43it/s]\n",
      "\n",
      "\n",
      " 58%|█████▊    | 8063/14000 [00:15<00:10, 572.99it/s]\n",
      "\n",
      "\n",
      " 58%|█████▊    | 8131/14000 [00:15<00:09, 600.56it/s]\n",
      "\n",
      "\n",
      " 59%|█████▊    | 8193/14000 [00:15<00:10, 571.90it/s]\n",
      "\n",
      "\n",
      " 59%|█████▉    | 8252/14000 [00:15<00:10, 547.80it/s]\n",
      "\n",
      "\n",
      " 59%|█████▉    | 8308/14000 [00:15<00:10, 542.07it/s]\n",
      "\n",
      "\n",
      " 60%|█████▉    | 8363/14000 [00:15<00:10, 540.60it/s]\n",
      "\n",
      "\n",
      " 60%|██████    | 8418/14000 [00:16<00:10, 507.70it/s]\n",
      "\n",
      "\n",
      " 60%|██████    | 8470/14000 [00:16<00:10, 510.49it/s]\n",
      "\n",
      "\n",
      " 61%|██████    | 8534/14000 [00:16<00:10, 540.65it/s]\n",
      "\n",
      "\n",
      " 61%|██████▏   | 8593/14000 [00:16<00:09, 551.85it/s]\n",
      "\n",
      "\n",
      " 62%|██████▏   | 8649/14000 [00:16<00:09, 549.35it/s]\n",
      "\n",
      "\n",
      " 62%|██████▏   | 8705/14000 [00:16<00:09, 545.18it/s]\n",
      "\n",
      "\n",
      " 63%|██████▎   | 8773/14000 [00:16<00:09, 579.52it/s]\n",
      "\n",
      "\n",
      " 63%|██████▎   | 8832/14000 [00:16<00:09, 550.87it/s]\n",
      "\n",
      "\n",
      " 63%|██████▎   | 8889/14000 [00:16<00:09, 555.30it/s]\n",
      "\n",
      "\n",
      " 64%|██████▍   | 8946/14000 [00:16<00:09, 543.08it/s]\n",
      "\n",
      "\n",
      " 64%|██████▍   | 9003/14000 [00:17<00:09, 549.59it/s]\n",
      "\n",
      "\n",
      " 65%|██████▍   | 9070/14000 [00:17<00:08, 579.08it/s]\n",
      "\n",
      "\n",
      " 65%|██████▌   | 9130/14000 [00:17<00:08, 580.60it/s]\n",
      "\n",
      "\n",
      " 66%|██████▌   | 9191/14000 [00:17<00:08, 587.64it/s]\n",
      "\n",
      "\n",
      " 66%|██████▌   | 9251/14000 [00:17<00:08, 587.08it/s]\n",
      "\n",
      "\n",
      " 67%|██████▋   | 9314/14000 [00:17<00:07, 598.72it/s]\n",
      "\n",
      "\n",
      " 67%|██████▋   | 9375/14000 [00:17<00:07, 598.48it/s]\n",
      "\n",
      "\n",
      " 67%|██████▋   | 9436/14000 [00:17<00:07, 583.53it/s]\n",
      "\n",
      "\n",
      " 68%|██████▊   | 9495/14000 [00:17<00:07, 570.08it/s]\n",
      "\n",
      "\n",
      " 68%|██████▊   | 9553/14000 [00:17<00:07, 570.65it/s]\n",
      "\n",
      "\n",
      " 69%|██████▊   | 9611/14000 [00:18<00:07, 559.10it/s]\n",
      "\n",
      "\n",
      " 69%|██████▉   | 9668/14000 [00:18<00:07, 553.18it/s]\n",
      "\n",
      "\n",
      " 69%|██████▉   | 9724/14000 [00:18<00:07, 549.70it/s]\n",
      "\n",
      "\n",
      " 70%|██████▉   | 9783/14000 [00:18<00:07, 560.65it/s]\n",
      "\n",
      "\n",
      " 70%|███████   | 9840/14000 [00:18<00:07, 543.22it/s]\n",
      "\n",
      "\n",
      " 71%|███████   | 9895/14000 [00:18<00:07, 539.66it/s]\n",
      "\n",
      "\n",
      " 71%|███████   | 9950/14000 [00:18<00:07, 534.27it/s]\n",
      "\n",
      "\n",
      " 72%|███████▏  | 10010/14000 [00:18<00:07, 551.62it/s]\n",
      "\n",
      "\n",
      " 72%|███████▏  | 10066/14000 [00:18<00:07, 531.72it/s]\n",
      "\n",
      "\n",
      " 72%|███████▏  | 10126/14000 [00:19<00:07, 549.47it/s]\n",
      "\n",
      "\n",
      " 73%|███████▎  | 10182/14000 [00:19<00:06, 551.04it/s]\n",
      "\n",
      "\n",
      " 73%|███████▎  | 10238/14000 [00:19<00:07, 535.41it/s]\n",
      "\n",
      "\n",
      " 74%|███████▎  | 10306/14000 [00:19<00:06, 569.56it/s]\n",
      "\n",
      "\n",
      " 74%|███████▍  | 10364/14000 [00:19<00:06, 545.81it/s]\n",
      "\n",
      "\n",
      " 74%|███████▍  | 10424/14000 [00:19<00:06, 558.81it/s]\n",
      "\n",
      "\n",
      " 75%|███████▍  | 10489/14000 [00:19<00:06, 582.69it/s]\n",
      "\n",
      "\n",
      " 75%|███████▌  | 10550/14000 [00:19<00:05, 588.13it/s]\n",
      "\n",
      "\n",
      " 76%|███████▌  | 10610/14000 [00:19<00:05, 578.50it/s]\n",
      "\n",
      "\n",
      " 76%|███████▌  | 10669/14000 [00:19<00:05, 567.57it/s]\n",
      "\n",
      "\n",
      " 77%|███████▋  | 10730/14000 [00:20<00:05, 576.85it/s]\n",
      "\n",
      "\n",
      " 77%|███████▋  | 10788/14000 [00:20<00:05, 558.44it/s]\n",
      "\n",
      "\n",
      " 77%|███████▋  | 10847/14000 [00:20<00:05, 566.24it/s]\n",
      "\n",
      "\n",
      " 78%|███████▊  | 10911/14000 [00:20<00:05, 585.48it/s]\n",
      "\n",
      "\n",
      " 78%|███████▊  | 10970/14000 [00:20<00:05, 580.71it/s]\n",
      "\n",
      "\n",
      " 79%|███████▉  | 11029/14000 [00:20<00:05, 581.86it/s]\n",
      "\n",
      "\n",
      " 79%|███████▉  | 11088/14000 [00:20<00:05, 571.07it/s]\n",
      "\n",
      "\n",
      " 80%|███████▉  | 11146/14000 [00:20<00:05, 558.41it/s]\n",
      "\n",
      "\n",
      " 80%|████████  | 11203/14000 [00:20<00:05, 555.91it/s]\n",
      "\n",
      "\n",
      " 80%|████████  | 11261/14000 [00:21<00:04, 561.93it/s]\n",
      "\n",
      "\n",
      " 81%|████████  | 11321/14000 [00:21<00:04, 572.63it/s]\n",
      "\n",
      "\n",
      " 81%|████████▏ | 11379/14000 [00:21<00:04, 570.79it/s]\n",
      "\n",
      "\n",
      " 82%|████████▏ | 11442/14000 [00:21<00:04, 585.70it/s]\n",
      "\n",
      "\n",
      " 82%|████████▏ | 11501/14000 [00:21<00:04, 576.65it/s]\n",
      "\n",
      "\n",
      " 83%|████████▎ | 11559/14000 [00:21<00:04, 575.99it/s]\n",
      "\n",
      "\n",
      " 83%|████████▎ | 11617/14000 [00:21<00:04, 552.23it/s]\n",
      "\n",
      "\n",
      " 83%|████████▎ | 11673/14000 [00:21<00:04, 554.31it/s]\n",
      "\n",
      "\n",
      " 84%|████████▍ | 11729/14000 [00:21<00:04, 548.95it/s]\n",
      "\n",
      "\n",
      " 84%|████████▍ | 11785/14000 [00:21<00:04, 544.22it/s]\n",
      "\n",
      "\n",
      " 85%|████████▍ | 11840/14000 [00:22<00:04, 530.86it/s]\n",
      "\n",
      "\n",
      " 85%|████████▍ | 11894/14000 [00:22<00:03, 527.53it/s]\n",
      "\n",
      "\n",
      " 85%|████████▌ | 11947/14000 [00:22<00:03, 523.06it/s]\n",
      "\n",
      "\n",
      " 86%|████████▌ | 12000/14000 [00:22<00:03, 500.73it/s]\n",
      "\n",
      "\n",
      " 86%|████████▌ | 12051/14000 [00:22<00:03, 501.09it/s]\n",
      "\n",
      "\n",
      " 86%|████████▋ | 12106/14000 [00:22<00:03, 508.36it/s]\n",
      "\n",
      "\n",
      " 87%|████████▋ | 12169/14000 [00:22<00:03, 535.80it/s]\n",
      "\n",
      "\n",
      " 87%|████████▋ | 12226/14000 [00:22<00:03, 544.76it/s]\n",
      "\n",
      "\n",
      " 88%|████████▊ | 12289/14000 [00:22<00:03, 567.52it/s]\n",
      "\n",
      "\n",
      " 88%|████████▊ | 12347/14000 [00:23<00:02, 559.90it/s]\n",
      "\n",
      "\n",
      " 89%|████████▊ | 12404/14000 [00:23<00:03, 530.58it/s]\n",
      "\n",
      "\n",
      " 89%|████████▉ | 12458/14000 [00:23<00:02, 528.48it/s]\n",
      "\n",
      "\n",
      " 89%|████████▉ | 12512/14000 [00:23<00:02, 524.09it/s]\n",
      "\n",
      "\n",
      " 90%|████████▉ | 12565/14000 [00:23<00:02, 515.65it/s]\n",
      "\n",
      "\n",
      " 90%|█████████ | 12620/14000 [00:23<00:02, 522.44it/s]\n",
      "\n",
      "\n",
      " 91%|█████████ | 12676/14000 [00:23<00:02, 530.63it/s]\n",
      "\n",
      "\n",
      " 91%|█████████ | 12734/14000 [00:23<00:02, 543.60it/s]\n",
      "\n",
      "\n",
      " 91%|█████████▏| 12789/14000 [00:23<00:02, 544.89it/s]\n",
      "\n",
      "\n",
      " 92%|█████████▏| 12850/14000 [00:23<00:02, 561.71it/s]\n",
      "\n",
      "\n",
      " 92%|█████████▏| 12909/14000 [00:24<00:01, 566.23it/s]\n",
      "\n",
      "\n",
      " 93%|█████████▎| 12967/14000 [00:24<00:01, 568.85it/s]\n",
      "\n",
      "\n",
      " 93%|█████████▎| 13025/14000 [00:24<00:01, 543.11it/s]\n",
      "\n",
      "\n",
      " 93%|█████████▎| 13085/14000 [00:24<00:01, 558.83it/s]\n",
      "\n",
      "\n",
      " 94%|█████████▍| 13142/14000 [00:24<00:01, 557.26it/s]\n",
      "\n",
      "\n",
      " 94%|█████████▍| 13198/14000 [00:24<00:01, 541.66it/s]\n",
      "\n",
      "\n",
      " 95%|█████████▍| 13253/14000 [00:24<00:01, 532.03it/s]\n",
      "\n",
      "\n",
      " 95%|█████████▌| 13308/14000 [00:24<00:01, 536.78it/s]\n",
      "\n",
      "\n",
      " 95%|█████████▌| 13362/14000 [00:24<00:01, 523.27it/s]\n",
      "\n",
      "\n",
      " 96%|█████████▌| 13415/14000 [00:25<00:01, 524.57it/s]\n",
      "\n",
      "\n",
      " 96%|█████████▌| 13468/14000 [00:25<00:01, 520.95it/s]\n",
      "\n",
      "\n",
      " 97%|█████████▋| 13521/14000 [00:25<00:00, 520.77it/s]\n",
      "\n",
      "\n",
      " 97%|█████████▋| 13574/14000 [00:25<00:00, 506.38it/s]\n",
      "\n",
      "\n",
      " 97%|█████████▋| 13627/14000 [00:25<00:00, 510.90it/s]\n",
      "\n",
      "\n",
      " 98%|█████████▊| 13679/14000 [00:25<00:00, 512.95it/s]\n",
      "\n",
      "\n",
      " 98%|█████████▊| 13732/14000 [00:25<00:00, 516.98it/s]\n",
      "\n",
      "\n",
      " 98%|█████████▊| 13787/14000 [00:25<00:00, 526.36it/s]\n",
      "\n",
      "\n",
      " 99%|█████████▉| 13840/14000 [00:25<00:00, 507.55it/s]\n",
      "\n",
      "\n",
      " 99%|█████████▉| 13895/14000 [00:25<00:00, 518.25it/s]\n",
      "\n",
      "\n",
      "100%|██████████| 14000/14000 [00:26<00:00, 535.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zero vector 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/6000 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      "  1%|          | 70/6000 [00:00<00:08, 694.79it/s]\n",
      "\n",
      "\n",
      "  2%|▏         | 135/6000 [00:00<00:08, 679.69it/s]\n",
      "\n",
      "\n",
      "  3%|▎         | 189/6000 [00:00<00:09, 630.58it/s]\n",
      "\n",
      "\n",
      "  4%|▍         | 250/6000 [00:00<00:09, 623.21it/s]\n",
      "\n",
      "\n",
      "  5%|▌         | 304/6000 [00:00<00:09, 593.27it/s]\n",
      "\n",
      "\n",
      "  6%|▌         | 358/6000 [00:00<00:09, 574.84it/s]\n",
      "\n",
      "\n",
      "  7%|▋         | 423/6000 [00:00<00:09, 592.62it/s]\n",
      "\n",
      "\n",
      "  8%|▊         | 478/6000 [00:00<00:09, 571.93it/s]\n",
      "\n",
      "\n",
      "  9%|▉         | 533/6000 [00:00<00:09, 562.24it/s]\n",
      "\n",
      "\n",
      " 10%|▉         | 592/6000 [00:01<00:09, 569.28it/s]\n",
      "\n",
      "\n",
      " 11%|█         | 648/6000 [00:01<00:09, 553.33it/s]\n",
      "\n",
      "\n",
      " 12%|█▏        | 703/6000 [00:01<00:09, 549.94it/s]\n",
      "\n",
      "\n",
      " 13%|█▎        | 759/6000 [00:01<00:09, 551.20it/s]\n",
      "\n",
      "\n",
      " 14%|█▎        | 814/6000 [00:01<00:09, 543.47it/s]\n",
      "\n",
      "\n",
      " 14%|█▍        | 869/6000 [00:01<00:09, 530.81it/s]\n",
      "\n",
      "\n",
      " 15%|█▌        | 926/6000 [00:01<00:09, 536.56it/s]\n",
      "\n",
      "\n",
      " 16%|█▋        | 980/6000 [00:01<00:09, 529.04it/s]\n",
      "\n",
      "\n",
      " 17%|█▋        | 1033/6000 [00:01<00:09, 521.97it/s]\n",
      "\n",
      "\n",
      " 18%|█▊        | 1095/6000 [00:01<00:08, 547.69it/s]\n",
      "\n",
      "\n",
      " 19%|█▉        | 1160/6000 [00:02<00:08, 573.26it/s]\n",
      "\n",
      "\n",
      " 20%|██        | 1218/6000 [00:02<00:08, 569.69it/s]\n",
      "\n",
      "\n",
      " 21%|██▏       | 1281/6000 [00:02<00:08, 584.21it/s]\n",
      "\n",
      "\n",
      " 22%|██▏       | 1340/6000 [00:02<00:08, 580.89it/s]\n",
      "\n",
      "\n",
      " 23%|██▎       | 1399/6000 [00:02<00:08, 574.72it/s]\n",
      "\n",
      "\n",
      " 24%|██▍       | 1458/6000 [00:02<00:07, 574.89it/s]\n",
      "\n",
      "\n",
      " 25%|██▌       | 1516/6000 [00:02<00:08, 551.87it/s]\n",
      "\n",
      "\n",
      " 26%|██▌       | 1573/6000 [00:02<00:07, 556.73it/s]\n",
      "\n",
      "\n",
      " 27%|██▋       | 1629/6000 [00:02<00:07, 550.48it/s]\n",
      "\n",
      "\n",
      " 28%|██▊       | 1689/6000 [00:02<00:07, 563.46it/s]\n",
      "\n",
      "\n",
      " 29%|██▉       | 1748/6000 [00:03<00:07, 565.58it/s]\n",
      "\n",
      "\n",
      " 30%|███       | 1805/6000 [00:03<00:07, 533.33it/s]\n",
      "\n",
      "\n",
      " 31%|███       | 1863/6000 [00:03<00:07, 544.50it/s]\n",
      "\n",
      "\n",
      " 32%|███▏      | 1924/6000 [00:03<00:07, 560.86it/s]\n",
      "\n",
      "\n",
      " 33%|███▎      | 1981/6000 [00:03<00:07, 547.85it/s]\n",
      "\n",
      "\n",
      " 34%|███▍      | 2039/6000 [00:03<00:07, 554.19it/s]\n",
      "\n",
      "\n",
      " 35%|███▍      | 2096/6000 [00:03<00:06, 558.02it/s]\n",
      "\n",
      "\n",
      " 36%|███▌      | 2152/6000 [00:03<00:07, 543.63it/s]\n",
      "\n",
      "\n",
      " 37%|███▋      | 2213/6000 [00:03<00:06, 556.40it/s]\n",
      "\n",
      "\n",
      " 38%|███▊      | 2269/6000 [00:04<00:06, 534.81it/s]\n",
      "\n",
      "\n",
      " 39%|███▉      | 2330/6000 [00:04<00:06, 553.61it/s]\n",
      "\n",
      "\n",
      " 40%|███▉      | 2390/6000 [00:04<00:06, 565.64it/s]\n",
      "\n",
      "\n",
      " 41%|████      | 2450/6000 [00:04<00:06, 573.24it/s]\n",
      "\n",
      "\n",
      " 42%|████▏     | 2513/6000 [00:04<00:05, 587.59it/s]\n",
      "\n",
      "\n",
      " 43%|████▎     | 2575/6000 [00:04<00:05, 595.30it/s]\n",
      "\n",
      "\n",
      " 44%|████▍     | 2635/6000 [00:04<00:05, 580.36it/s]\n",
      "\n",
      "\n",
      " 45%|████▍     | 2694/6000 [00:04<00:05, 582.15it/s]\n",
      "\n",
      "\n",
      " 46%|████▌     | 2757/6000 [00:04<00:05, 594.70it/s]\n",
      "\n",
      "\n",
      " 47%|████▋     | 2817/6000 [00:04<00:05, 574.17it/s]\n",
      "\n",
      "\n",
      " 48%|████▊     | 2875/6000 [00:05<00:05, 562.37it/s]\n",
      "\n",
      "\n",
      " 49%|████▉     | 2935/6000 [00:05<00:05, 571.65it/s]\n",
      "\n",
      "\n",
      " 50%|████▉     | 2993/6000 [00:05<00:05, 562.99it/s]\n",
      "\n",
      "\n",
      " 51%|█████     | 3050/6000 [00:05<00:05, 556.80it/s]\n",
      "\n",
      "\n",
      " 52%|█████▏    | 3106/6000 [00:05<00:05, 546.18it/s]\n",
      "\n",
      "\n",
      " 53%|█████▎    | 3161/6000 [00:05<00:05, 542.35it/s]\n",
      "\n",
      "\n",
      " 54%|█████▎    | 3219/6000 [00:05<00:05, 552.35it/s]\n",
      "\n",
      "\n",
      " 55%|█████▍    | 3275/6000 [00:05<00:05, 542.41it/s]\n",
      "\n",
      "\n",
      " 56%|█████▌    | 3341/6000 [00:05<00:04, 572.42it/s]\n",
      "\n",
      "\n",
      " 57%|█████▋    | 3399/6000 [00:06<00:04, 556.19it/s]\n",
      "\n",
      "\n",
      " 58%|█████▊    | 3463/6000 [00:06<00:04, 575.54it/s]\n",
      "\n",
      "\n",
      " 59%|█████▊    | 3522/6000 [00:06<00:04, 562.33it/s]\n",
      "\n",
      "\n",
      " 60%|█████▉    | 3579/6000 [00:06<00:04, 561.05it/s]\n",
      "\n",
      "\n",
      " 61%|██████    | 3639/6000 [00:06<00:04, 571.48it/s]\n",
      "\n",
      "\n",
      " 62%|██████▏   | 3697/6000 [00:06<00:04, 554.75it/s]\n",
      "\n",
      "\n",
      " 63%|██████▎   | 3757/6000 [00:06<00:03, 566.75it/s]\n",
      "\n",
      "\n",
      " 64%|██████▎   | 3814/6000 [00:06<00:03, 560.68it/s]\n",
      "\n",
      "\n",
      " 65%|██████▍   | 3875/6000 [00:06<00:03, 569.70it/s]\n",
      "\n",
      "\n",
      " 66%|██████▌   | 3935/6000 [00:06<00:03, 576.15it/s]\n",
      "\n",
      "\n",
      " 67%|██████▋   | 3999/6000 [00:07<00:03, 591.23it/s]\n",
      "\n",
      "\n",
      " 68%|██████▊   | 4063/6000 [00:07<00:03, 604.16it/s]\n",
      "\n",
      "\n",
      " 69%|██████▊   | 4124/6000 [00:07<00:03, 570.89it/s]\n",
      "\n",
      "\n",
      " 70%|██████▉   | 4187/6000 [00:07<00:03, 587.06it/s]\n",
      "\n",
      "\n",
      " 71%|███████   | 4247/6000 [00:07<00:03, 573.31it/s]\n",
      "\n",
      "\n",
      " 72%|███████▏  | 4305/6000 [00:07<00:03, 556.81it/s]\n",
      "\n",
      "\n",
      " 73%|███████▎  | 4368/6000 [00:07<00:02, 575.59it/s]\n",
      "\n",
      "\n",
      " 74%|███████▍  | 4426/6000 [00:07<00:02, 575.07it/s]\n",
      "\n",
      "\n",
      " 75%|███████▍  | 4492/6000 [00:07<00:02, 597.96it/s]\n",
      "\n",
      "\n",
      " 76%|███████▌  | 4553/6000 [00:08<00:02, 582.49it/s]\n",
      "\n",
      "\n",
      " 77%|███████▋  | 4616/6000 [00:08<00:02, 590.76it/s]\n",
      "\n",
      "\n",
      " 78%|███████▊  | 4676/6000 [00:08<00:02, 589.52it/s]\n",
      "\n",
      "\n",
      " 79%|███████▉  | 4736/6000 [00:08<00:02, 573.42it/s]\n",
      "\n",
      "\n",
      " 80%|███████▉  | 4797/6000 [00:08<00:02, 583.52it/s]\n",
      "\n",
      "\n",
      " 81%|████████  | 4856/6000 [00:08<00:01, 579.27it/s]\n",
      "\n",
      "\n",
      " 82%|████████▏ | 4919/6000 [00:08<00:01, 592.56it/s]\n",
      "\n",
      "\n",
      " 83%|████████▎ | 4979/6000 [00:08<00:01, 593.90it/s]\n",
      "\n",
      "\n",
      " 84%|████████▍ | 5039/6000 [00:08<00:01, 592.67it/s]\n",
      "\n",
      "\n",
      " 85%|████████▍ | 5099/6000 [00:08<00:01, 585.37it/s]\n",
      "\n",
      "\n",
      " 86%|████████▌ | 5158/6000 [00:09<00:01, 573.17it/s]\n",
      "\n",
      "\n",
      " 87%|████████▋ | 5216/6000 [00:09<00:01, 559.63it/s]\n",
      "\n",
      "\n",
      " 88%|████████▊ | 5273/6000 [00:09<00:01, 562.12it/s]\n",
      "\n",
      "\n",
      " 89%|████████▉ | 5335/6000 [00:09<00:01, 577.00it/s]\n",
      "\n",
      "\n",
      " 90%|████████▉ | 5397/6000 [00:09<00:01, 587.43it/s]\n",
      "\n",
      "\n",
      " 91%|█████████ | 5459/6000 [00:09<00:00, 595.23it/s]\n",
      "\n",
      "\n",
      " 92%|█████████▏| 5519/6000 [00:09<00:00, 577.81it/s]\n",
      "\n",
      "\n",
      " 93%|█████████▎| 5577/6000 [00:09<00:00, 559.74it/s]\n",
      "\n",
      "\n",
      " 94%|█████████▍| 5634/6000 [00:09<00:00, 562.08it/s]\n",
      "\n",
      "\n",
      " 95%|█████████▍| 5691/6000 [00:10<00:00, 551.37it/s]\n",
      "\n",
      "\n",
      " 96%|█████████▌| 5752/6000 [00:10<00:00, 566.45it/s]\n",
      "\n",
      "\n",
      " 97%|█████████▋| 5813/6000 [00:10<00:00, 577.98it/s]\n",
      "\n",
      "\n",
      " 98%|█████████▊| 5872/6000 [00:10<00:00, 563.99it/s]\n",
      "\n",
      "\n",
      " 99%|█████████▉| 5931/6000 [00:10<00:00, 567.83it/s]\n",
      "\n",
      "\n",
      "100%|██████████| 6000/6000 [00:10<00:00, 568.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zero vector 0.00016666666666666666\n"
     ]
    }
   ],
   "source": [
    "def get_doc_vec(x, word2vec_model):\n",
    "    doc_vec_s1 = []\n",
    "    zero_count = 0\n",
    "    for doc in tqdm(x):\n",
    "        tmp = [word2vec_model.wv[word] for word in doc if word in word2vec_model.wv]\n",
    "        if len(tmp) == 0:\n",
    "            avg = np.zeros(len(doc_vec_s1[0]))\n",
    "            zero_count += 1\n",
    "        else:\n",
    "            avg = [item/len(tmp) for item in reduce(lambda lt1, lt2: [lt1[index]+lt2[index] for index in range(len(lt1))], tmp)]\n",
    "        doc_vec_s1.append(avg)\n",
    "    print(\"zero vector\", zero_count/len(doc_vec_s1))\n",
    "    return doc_vec_s1\n",
    "\n",
    "X1_train = get_doc_vec(x1_train, word2vec_model)\n",
    "X1_test  = get_doc_vec(x1_test, word2vec_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1d5c7887-14eb-4f91-be6b-51bfaa3c189a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-24T12:36:39.025499Z",
     "iopub.status.busy": "2021-10-24T12:36:39.025145Z",
     "iopub.status.idle": "2021-10-24T12:36:39.031821Z",
     "shell.execute_reply": "2021-10-24T12:36:39.030719Z",
     "shell.execute_reply.started": "2021-10-24T12:36:39.025446Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.16836156564600327, 0.11222276968114517, -0.15600928138284123, 0.235136116252226, -0.04517208478030037, -0.431303949917064, 0.10600343872519101, 0.40251027836519127, -0.549279717838063, -0.15212563907398896, -0.07575958616593305, -0.1997673511505127, -0.2436065393335679, 0.12940189417670755, -0.11400430342730354, -0.030603380764231962, -0.081016645712011, 0.014301672577857971, -0.1463780683629653, -0.46513927684110756, 0.14229583740234375, 0.26988568025476795, 0.33950003455666933, -0.16803937799790325, -0.019632069503559786, -0.13160213302163518, 0.1048821491353652, 0.0620114242329317, -0.15555439275853775, 0.09759168063893038, 0.3518732014824362, -0.0008335030254195718, 0.2825167880338781, 0.002122400438084322, -0.22092147434459014, 0.4370293617248535, 0.29839176290175495, -0.1534134640413172, -0.17386193836436553, -0.4525494855992934, -0.08144584824057187, 0.18196762309354894, -0.017369648989509132, 0.21601191688986385, 0.19030141830444336, 0.15270019980037913, 0.020697572652031395, -0.15743106954237995, -0.0612260804456823, 0.22914812144111185, 0.277654255137724, -0.41333551967845245, 0.2830689374138327, -0.3184052635641659, -0.09669078798855052, -0.013419983141562519, 0.32191868389354034, -0.176085303811466, -0.1323717902688419, 0.28916002722347484, 0.16586826829349294, -0.2510894326602711, 0.14154675427605123, 0.08922818127800436, -0.4456345333772547, 0.4594690378974466, -0.012153751709881951, 0.41635479646570545, -0.3022693465737736, -0.09544189537272733, -0.2829187337089987, 0.17736387252807617, 0.23614788055419922, 0.21926405850578756, 0.361111556782442, 0.025886497076819923, -0.10898890214807846, 0.23799649406881893, -0.23094498409944422, -0.08871093217064352, -0.34253625308766084, 0.07255444105933695, -0.26376788756426645, 0.20833596061257756, 0.13102291612064138, -0.21048510775846593, 0.44607580409330483, -0.1248310173259062, 0.033355909235337204, 0.37010787515079274, 0.19431470422183766, 0.0618812967749203, -0.09238145631902359, -0.25675495933083925, 0.4766065373140223, 0.30961308759801526, 0.14463922556708841, -0.1925349095288445, 0.0965806526296279, -0.15410229739020853]\n"
     ]
    }
   ],
   "source": [
    "print(X1_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9015ce9e-7073-43ed-9149-407210c94654",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-24T12:19:05.122487Z",
     "iopub.status.busy": "2021-10-24T12:19:05.122110Z",
     "iopub.status.idle": "2021-10-24T12:19:05.214670Z",
     "shell.execute_reply": "2021-10-24T12:19:05.213825Z",
     "shell.execute_reply.started": "2021-10-24T12:19:05.122434Z"
    }
   },
   "outputs": [],
   "source": [
    "#LR\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "LogisticRegression = LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=1.0,\n",
    "    fit_intercept=True, intercept_scaling=1, class_weight=None,\n",
    "    random_state=None, solver='liblinear', max_iter=100, multi_class='ovr',\n",
    "    verbose=0, warm_start=False, n_jobs=1)\n",
    "\n",
    "#bayes\n",
    "from sklearn import naive_bayes\n",
    "GaussianNB=naive_bayes.GaussianNB() # 高斯贝叶斯\n",
    "\n",
    "MultinomialNB=naive_bayes.MultinomialNB(alpha=1.0, fit_prior=True, class_prior=None)\n",
    "\n",
    "BernoulliNB= naive_bayes.BernoulliNB(alpha=1.0, binarize=0.0, fit_prior=True, class_prior=None)\n",
    "\n",
    "#DT\n",
    "from sklearn import tree\n",
    "DecisionTree = tree.DecisionTreeClassifier(criterion='gini', max_depth=None,\n",
    "    min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0,\n",
    "    max_features=None, random_state=None, max_leaf_nodes=None,\n",
    "    min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "     class_weight=None)\n",
    "                  \n",
    "#svc\n",
    "from sklearn.svm import SVC\n",
    "SVC = SVC(C=1.0, kernel='rbf', gamma='auto')\n",
    "                  \n",
    "#kNN\n",
    "from sklearn import neighbors                 \n",
    "KNN = neighbors.KNeighborsClassifier(n_neighbors=5, n_jobs=1) # 分类\n",
    "                  \n",
    "#MLP               \n",
    "from sklearn.neural_network import MLPClassifier\n",
    "MLPClassifier = MLPClassifier(activation='relu', solver='adam', alpha=0.0001,max_iter=200)\n",
    "                  \n",
    "#rf\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "RandomForest = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f8594006-5eee-433e-8267-6c0cc540454e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-24T12:38:52.055907Z",
     "iopub.status.busy": "2021-10-24T12:38:52.055530Z",
     "iopub.status.idle": "2021-10-24T12:38:52.064056Z",
     "shell.execute_reply": "2021-10-24T12:38:52.062958Z",
     "shell.execute_reply.started": "2021-10-24T12:38:52.055840Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "#select target model\n",
    "cls_model=[]\n",
    "cls_model.append(('Logistic Regression:', LogisticRegression))\n",
    "cls_model.append(('GaussianNB :', GaussianNB))\n",
    "# cls_model.append(('MultinomialNB      :', MultinomialNB))\n",
    "cls_model.append(('BernoulliNB     :', BernoulliNB))\n",
    "cls_model.append(('DecisionTree :', DecisionTree))\n",
    "cls_model.append(('SVC :', SVC))   \n",
    "cls_model.append(('KNN :', KNN)) \n",
    "cls_model.append(('MLPClassifier :', MLPClassifier))\n",
    "cls_model.append(('RandomForest :', RandomForest)) \n",
    "print(len(cls_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b5ea51-c701-43e7-88a3-e87b480656f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-24T12:38:54.557411Z",
     "iopub.status.busy": "2021-10-24T12:38:54.557016Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression: 0.5856666666666667\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.18      0.25       157\n",
      "           1       0.59      0.81      0.68      2191\n",
      "           2       0.65      0.30      0.41       121\n",
      "           3       0.61      0.82      0.70      1460\n",
      "           4       0.00      0.00      0.00        55\n",
      "           5       0.47      0.17      0.26       269\n",
      "           6       0.00      0.00      0.00       134\n",
      "           7       0.53      0.43      0.47       560\n",
      "           8       0.00      0.00      0.00        58\n",
      "           9       0.49      0.26      0.34       254\n",
      "          10       0.44      0.21      0.28        77\n",
      "          11       0.66      0.46      0.54       206\n",
      "          12       0.39      0.06      0.10       127\n",
      "          13       0.61      0.11      0.18       208\n",
      "          14       0.00      0.00      0.00        36\n",
      "          15       0.00      0.00      0.00        14\n",
      "          16       0.00      0.00      0.00        22\n",
      "          17       0.00      0.00      0.00        25\n",
      "          18       0.00      0.00      0.00        26\n",
      "\n",
      "    accuracy                           0.59      6000\n",
      "   macro avg       0.31      0.20      0.22      6000\n",
      "weighted avg       0.54      0.59      0.53      6000\n",
      "\n",
      "GaussianNB : 0.3615\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.19      0.46      0.27       157\n",
      "           1       0.82      0.34      0.48      2191\n",
      "           2       0.34      0.50      0.40       121\n",
      "           3       0.65      0.37      0.47      1460\n",
      "           4       0.05      0.05      0.05        55\n",
      "           5       0.25      0.22      0.23       269\n",
      "           6       0.08      0.10      0.09       134\n",
      "           7       0.47      0.36      0.41       560\n",
      "           8       0.07      0.43      0.13        58\n",
      "           9       0.28      0.31      0.30       254\n",
      "          10       0.19      0.38      0.25        77\n",
      "          11       0.45      0.46      0.45       206\n",
      "          12       0.17      0.56      0.26       127\n",
      "          13       0.29      0.64      0.40       208\n",
      "          14       0.03      0.25      0.06        36\n",
      "          15       0.04      0.57      0.08        14\n",
      "          16       0.03      0.09      0.04        22\n",
      "          17       0.05      0.40      0.09        25\n",
      "          18       0.06      0.38      0.10        26\n",
      "\n",
      "    accuracy                           0.36      6000\n",
      "   macro avg       0.24      0.36      0.24      6000\n",
      "weighted avg       0.57      0.36      0.41      6000\n",
      "\n",
      "BernoulliNB     : 0.40066666666666667\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.17      0.38      0.24       157\n",
      "           1       0.75      0.38      0.51      2191\n",
      "           2       0.23      0.60      0.33       121\n",
      "           3       0.60      0.48      0.53      1460\n",
      "           4       0.00      0.00      0.00        55\n",
      "           5       0.22      0.28      0.25       269\n",
      "           6       0.10      0.10      0.10       134\n",
      "           7       0.44      0.44      0.44       560\n",
      "           8       0.09      0.38      0.14        58\n",
      "           9       0.23      0.29      0.26       254\n",
      "          10       0.23      0.26      0.24        77\n",
      "          11       0.45      0.38      0.41       206\n",
      "          12       0.18      0.50      0.27       127\n",
      "          13       0.30      0.52      0.38       208\n",
      "          14       0.05      0.17      0.08        36\n",
      "          15       0.14      0.14      0.14        14\n",
      "          16       0.06      0.05      0.05        22\n",
      "          17       0.04      0.36      0.08        25\n",
      "          18       0.08      0.27      0.13        26\n",
      "\n",
      "    accuracy                           0.40      6000\n",
      "   macro avg       0.23      0.31      0.24      6000\n",
      "weighted avg       0.53      0.40      0.43      6000\n",
      "\n",
      "DecisionTree : 0.3953333333333333\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.12      0.11      0.12       157\n",
      "           1       0.56      0.55      0.56      2191\n",
      "           2       0.29      0.25      0.27       121\n",
      "           3       0.51      0.51      0.51      1460\n",
      "           4       0.01      0.02      0.02        55\n",
      "           5       0.16      0.16      0.16       269\n",
      "           6       0.02      0.02      0.02       134\n",
      "           7       0.31      0.32      0.31       560\n",
      "           8       0.05      0.05      0.05        58\n",
      "           9       0.14      0.15      0.14       254\n",
      "          10       0.08      0.09      0.08        77\n",
      "          11       0.24      0.22      0.23       206\n",
      "          12       0.10      0.10      0.10       127\n",
      "          13       0.19      0.20      0.20       208\n",
      "          14       0.00      0.00      0.00        36\n",
      "          15       0.00      0.00      0.00        14\n",
      "          16       0.03      0.05      0.04        22\n",
      "          17       0.00      0.00      0.00        25\n",
      "          18       0.00      0.00      0.00        26\n",
      "\n",
      "    accuracy                           0.40      6000\n",
      "   macro avg       0.15      0.15      0.15      6000\n",
      "weighted avg       0.40      0.40      0.40      6000\n",
      "\n",
      "SVC : 0.5595\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.01      0.01       157\n",
      "           1       0.54      0.84      0.66      2191\n",
      "           2       0.93      0.11      0.19       121\n",
      "           3       0.60      0.85      0.70      1460\n",
      "           4       0.00      0.00      0.00        55\n",
      "           5       0.00      0.00      0.00       269\n",
      "           6       0.00      0.00      0.00       134\n",
      "           7       0.54      0.39      0.45       560\n",
      "           8       0.00      0.00      0.00        58\n",
      "           9       0.00      0.00      0.00       254\n",
      "          10       0.00      0.00      0.00        77\n",
      "          11       0.66      0.19      0.30       206\n",
      "          12       0.00      0.00      0.00       127\n",
      "          13       0.00      0.00      0.00       208\n",
      "          14       0.00      0.00      0.00        36\n",
      "          15       0.00      0.00      0.00        14\n",
      "          16       0.00      0.00      0.00        22\n",
      "          17       0.00      0.00      0.00        25\n",
      "          18       0.00      0.00      0.00        26\n",
      "\n",
      "    accuracy                           0.56      6000\n",
      "   macro avg       0.22      0.13      0.12      6000\n",
      "weighted avg       0.46      0.56      0.47      6000\n",
      "\n",
      "KNN : 0.5306666666666666\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.20      0.26      0.23       157\n",
      "           1       0.59      0.74      0.66      2191\n",
      "           2       0.45      0.25      0.32       121\n",
      "           3       0.57      0.76      0.65      1460\n",
      "           4       0.22      0.07      0.11        55\n",
      "           5       0.25      0.13      0.17       269\n",
      "           6       0.00      0.00      0.00       134\n",
      "           7       0.41      0.34      0.37       560\n",
      "           8       0.08      0.02      0.03        58\n",
      "           9       0.29      0.13      0.18       254\n",
      "          10       0.39      0.21      0.27        77\n",
      "          11       0.52      0.36      0.43       206\n",
      "          12       0.32      0.07      0.12       127\n",
      "          13       0.41      0.14      0.21       208\n",
      "          14       0.00      0.00      0.00        36\n",
      "          15       0.00      0.00      0.00        14\n",
      "          16       0.00      0.00      0.00        22\n",
      "          17       0.50      0.04      0.07        25\n",
      "          18       0.00      0.00      0.00        26\n",
      "\n",
      "    accuracy                           0.53      6000\n",
      "   macro avg       0.27      0.18      0.20      6000\n",
      "weighted avg       0.48      0.53      0.49      6000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score, classification_report\n",
    "\n",
    "for name, model in cls_model:\n",
    "    model.fit(X1_train, y1_train)\n",
    "    pred = model.predict(X1_test).astype(int)\n",
    "    print(name,accuracy_score(y1_test, pred))\n",
    "    print(classification_report(y1_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e21bab94-b087-468e-97bc-94a4ffa7320a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-24T13:18:46.293235Z",
     "iopub.status.busy": "2021-10-24T13:18:46.292888Z",
     "iopub.status.idle": "2021-10-24T13:18:46.929673Z",
     "shell.execute_reply": "2021-10-24T13:18:46.928960Z",
     "shell.execute_reply.started": "2021-10-24T13:18:46.293182Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "女听过别人最多的议论就是干啥啥不行不长心眼没有脑子这样的议论是针对谁呢？我也是一个从小被这样训到大的女生哦，总会被指责缺心少肺、没心眼儿、没眼力见儿、看不出来眉眼高低等等。不过在我成长一段时间之后，发现这件事情其实很简单，也没有什么大的问题。如果你愿意的话，可以找我聊聊，倾诉一下你遇到的事情，希望能够帮到你。我是树洞小太阳，欢迎你来找我玩❤好惨原生家庭也这么对你吗\n",
      "0\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "#try DL\n",
    "idx=1\n",
    "lines = [[post[\"title\"]] + [chat[\"value\"] for chat in post[\"chats\"]] for post in data]\n",
    "\n",
    "x=[]\n",
    "for i in lines:\n",
    "    tmp=''\n",
    "    for j in i:\n",
    "        tmp+=j\n",
    "    x.append(tmp)\n",
    "\n",
    "print(x[0].replace(' ',''))\n",
    "print(y_s1[0])\n",
    "\n",
    "with open(\"trainmh.txt\",\"w\") as f:\n",
    "    for i in range(0,20000):\n",
    "        ans=str(idx)+'\\t'+x[i].replace(' ','').replace('\\t','').replace('\\n','').replace('\\r','')+'\\t'+str(y_s1[i])\n",
    "        idx+=1\n",
    "        f.write(ans+'\\n')\n",
    "    print('done')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "53b28301-99b2-4a27-8b0f-a341a5c2d077",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-24T13:18:54.195512Z",
     "iopub.status.busy": "2021-10-24T13:18:54.195157Z",
     "iopub.status.idle": "2021-10-24T13:18:54.270221Z",
     "shell.execute_reply": "2021-10-24T13:18:54.269452Z",
     "shell.execute_reply.started": "2021-10-24T13:18:54.195459Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "idx=1\n",
    "with open(\"trainmh.txt\", 'r', encoding='utf-8') as fp:\n",
    "    for line in fp.readlines():\n",
    "        try:\n",
    "            order, words, labels = line.strip('\\n').split('\\t')\n",
    "        except:\n",
    "            print(line)\n",
    "        idx+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "75003ff8-7c71-4866-9b2d-a93f35233463",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-24T13:19:00.772355Z",
     "iopub.status.busy": "2021-10-24T13:19:00.771993Z",
     "iopub.status.idle": "2021-10-24T13:19:00.777632Z",
     "shell.execute_reply": "2021-10-24T13:19:00.776828Z",
     "shell.execute_reply.started": "2021-10-24T13:19:00.772302Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'我外表上被看作是一个很乖很单纯的男孩，但其实我内心邪恶，只有我自己知道，感觉我就是披着羊皮的狼，?\\r\\n此话怎讲怎么了发生什么了内心怎么邪恶的啊'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[697]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5295c9aa-8f1d-4b5f-997d-41f41fdb2c77",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-24T13:29:01.986019Z",
     "iopub.status.busy": "2021-10-24T13:29:01.985647Z",
     "iopub.status.idle": "2021-10-24T13:56:24.357381Z",
     "shell.execute_reply": "2021-10-24T13:56:24.356537Z",
     "shell.execute_reply.started": "2021-10-24T13:29:01.985957Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-10-24 21:29:01,990] [    INFO] - Already cached /home/aistudio/.paddlenlp/models/ernie-1.0/vocab.txt\n",
      "[2021-10-24 21:29:02,107] [    INFO] - Already cached /home/aistudio/.paddlenlp/models/ernie-1.0/ernie_v1_chn_base.pdparams\n",
      "[2021-10-24 21:29:02,110] [   DEBUG] - init ErnieModel with config: {'attention_probs_dropout_prob': 0.1, 'hidden_act': 'relu', 'hidden_dropout_prob': 0.1, 'hidden_size': 768, 'initializer_range': 0.02, 'max_position_embeddings': 513, 'num_attention_heads': 12, 'num_hidden_layers': 12, 'type_vocab_size': 2, 'vocab_size': 18000, 'pad_token_id': 0}\n",
      "[2021-10-24 21:29:02,205] [    INFO] - loading pretrained model from /home/aistudio/.paddlenlp/models/ernie-1.0/ernie_v1_chn_base.pdparams\n",
      "[2021-10-24 21:29:02,898] [    INFO] - param:mlm_bias not set in pretrained model, skip\n",
      "[2021-10-24 21:29:02,900] [    INFO] - param:mlm.weight not set in pretrained model, skip\n",
      "[2021-10-24 21:29:02,904] [    INFO] - param:mlm.bias not set in pretrained model, skip\n",
      "[2021-10-24 21:29:02,907] [    INFO] - param:mlm_ln.weight not set in pretrained model, skip\n",
      "[2021-10-24 21:29:02,909] [    INFO] - param:mlm_ln.bias not set in pretrained model, skip\n",
      "[2021-10-24 21:29:23,905] [    INFO] - [step 50 / 4000]train loss 20.21061, ppl 598902592.00000, elr 6.250e-06\n",
      "[2021-10-24 21:29:44,348] [    INFO] - [step 100 / 4000]train loss 0.91833, ppl 2.50510, elr 1.250e-05\n",
      "[2021-10-24 21:30:04,750] [    INFO] - [step 150 / 4000]train loss 0.59245, ppl 1.80842, elr 1.875e-05\n",
      "[2021-10-24 21:30:25,132] [    INFO] - [step 200 / 4000]train loss 0.49644, ppl 1.64285, elr 2.500e-05\n",
      "[2021-10-24 21:30:45,551] [    INFO] - [step 250 / 4000]train loss 0.27872, ppl 1.32143, elr 3.125e-05\n",
      "[2021-10-24 21:31:05,852] [    INFO] - [step 300 / 4000]train loss 0.35007, ppl 1.41917, elr 3.750e-05\n",
      "[2021-10-24 21:31:26,335] [    INFO] - [step 350 / 4000]train loss 0.40634, ppl 1.50132, elr 4.375e-05\n",
      "[2021-10-24 21:31:47,871] [    INFO] - [step 400 / 4000]train loss 0.50669, ppl 1.65979, elr 5.000e-05\n",
      "[2021-10-24 21:32:09,881] [    INFO] - [step 450 / 4000]train loss 0.44141, ppl 1.55490, elr 4.931e-05\n",
      "[2021-10-24 21:32:31,865] [    INFO] - [step 500 / 4000]train loss 0.51157, ppl 1.66791, elr 4.861e-05\n",
      "[2021-10-24 21:32:52,278] [    INFO] - [step 550 / 4000]train loss 0.45549, ppl 1.57694, elr 4.792e-05\n",
      "[2021-10-24 21:33:12,912] [    INFO] - [step 600 / 4000]train loss 0.26291, ppl 1.30071, elr 4.722e-05\n",
      "[2021-10-24 21:33:33,259] [    INFO] - [step 650 / 4000]train loss 0.46815, ppl 1.59703, elr 4.653e-05\n",
      "[2021-10-24 21:33:53,553] [    INFO] - [step 700 / 4000]train loss 0.30563, ppl 1.35748, elr 4.583e-05\n",
      "[2021-10-24 21:34:13,850] [    INFO] - [step 750 / 4000]train loss 0.34127, ppl 1.40673, elr 4.514e-05\n",
      "[2021-10-24 21:34:34,092] [    INFO] - [step 800 / 4000]train loss 0.36922, ppl 1.44661, elr 4.444e-05\n",
      "[2021-10-24 21:34:54,498] [    INFO] - [step 850 / 4000]train loss 0.20767, ppl 1.23081, elr 4.375e-05\n",
      "[2021-10-24 21:35:15,253] [    INFO] - [step 900 / 4000]train loss 0.42080, ppl 1.52318, elr 4.306e-05\n",
      "[2021-10-24 21:35:35,697] [    INFO] - [step 950 / 4000]train loss 0.34103, ppl 1.40640, elr 4.236e-05\n",
      "[2021-10-24 21:35:56,171] [    INFO] - [step 1000 / 4000]train loss 0.73207, ppl 2.07937, elr 4.167e-05\n",
      "[2021-10-24 21:36:16,963] [    INFO] - [step 1050 / 4000]train loss 0.43473, ppl 1.54455, elr 4.097e-05\n",
      "[2021-10-24 21:36:37,344] [    INFO] - [step 1100 / 4000]train loss 0.34964, ppl 1.41855, elr 4.028e-05\n",
      "[2021-10-24 21:36:57,652] [    INFO] - [step 1150 / 4000]train loss 0.33167, ppl 1.39330, elr 3.958e-05\n",
      "[2021-10-24 21:37:18,146] [    INFO] - [step 1200 / 4000]train loss 0.46976, ppl 1.59961, elr 3.889e-05\n",
      "[2021-10-24 21:37:38,494] [    INFO] - [step 1250 / 4000]train loss 0.24945, ppl 1.28332, elr 3.819e-05\n",
      "[2021-10-24 21:37:59,022] [    INFO] - [step 1300 / 4000]train loss 0.29240, ppl 1.33964, elr 3.750e-05\n",
      "[2021-10-24 21:38:19,497] [    INFO] - [step 1350 / 4000]train loss 0.65990, ppl 1.93460, elr 3.681e-05\n",
      "[2021-10-24 21:38:40,284] [    INFO] - [step 1400 / 4000]train loss 0.15726, ppl 1.17030, elr 3.611e-05\n",
      "[2021-10-24 21:39:00,473] [    INFO] - [step 1450 / 4000]train loss 0.44750, ppl 1.56440, elr 3.542e-05\n",
      "[2021-10-24 21:39:20,871] [    INFO] - [step 1500 / 4000]train loss 0.44525, ppl 1.56088, elr 3.472e-05\n",
      "[2021-10-24 21:39:41,194] [    INFO] - [step 1550 / 4000]train loss 0.49379, ppl 1.63851, elr 3.403e-05\n",
      "[2021-10-24 21:40:01,371] [    INFO] - [step 1600 / 4000]train loss 0.17700, ppl 1.19363, elr 3.333e-05\n",
      "[2021-10-24 21:40:21,526] [    INFO] - [step 1650 / 4000]train loss 0.46637, ppl 1.59419, elr 3.264e-05\n",
      "[2021-10-24 21:40:42,779] [    INFO] - [step 1700 / 4000]train loss 0.30571, ppl 1.35759, elr 3.194e-05\n",
      "[2021-10-24 21:41:03,008] [    INFO] - [step 1750 / 4000]train loss 0.16509, ppl 1.17950, elr 3.125e-05\n",
      "[2021-10-24 21:41:23,345] [    INFO] - [step 1800 / 4000]train loss 0.36090, ppl 1.43463, elr 3.056e-05\n",
      "[2021-10-24 21:41:43,805] [    INFO] - [step 1850 / 4000]train loss 0.50174, ppl 1.65160, elr 2.986e-05\n",
      "[2021-10-24 21:42:04,190] [    INFO] - [step 1900 / 4000]train loss 0.19899, ppl 1.22017, elr 2.917e-05\n",
      "[2021-10-24 21:42:24,490] [    INFO] - [step 1950 / 4000]train loss 0.32578, ppl 1.38512, elr 2.847e-05\n",
      "[2021-10-24 21:42:44,796] [    INFO] - [step 2000 / 4000]train loss 0.27971, ppl 1.32275, elr 2.778e-05\n",
      "[2021-10-24 21:42:44,799] [    INFO] - save the model in ernie_gen_ch_med/step_2000_ppl_1.32275.params\n",
      "[2021-10-24 21:43:06,339] [    INFO] - [step 2050 / 4000]train loss 0.39580, ppl 1.48557, elr 2.708e-05\n",
      "[2021-10-24 21:43:26,585] [    INFO] - [step 2100 / 4000]train loss 0.40306, ppl 1.49639, elr 2.639e-05\n",
      "[2021-10-24 21:43:46,991] [    INFO] - [step 2150 / 4000]train loss 0.29331, ppl 1.34085, elr 2.569e-05\n",
      "[2021-10-24 21:44:07,382] [    INFO] - [step 2200 / 4000]train loss 0.38176, ppl 1.46487, elr 2.500e-05\n",
      "[2021-10-24 21:44:28,181] [    INFO] - [step 2250 / 4000]train loss 0.28401, ppl 1.32845, elr 2.431e-05\n",
      "[2021-10-24 21:44:48,680] [    INFO] - [step 2300 / 4000]train loss 0.26028, ppl 1.29730, elr 2.361e-05\n",
      "[2021-10-24 21:45:09,159] [    INFO] - [step 2350 / 4000]train loss 0.28334, ppl 1.32756, elr 2.292e-05\n",
      "[2021-10-24 21:45:29,534] [    INFO] - [step 2400 / 4000]train loss 0.28717, ppl 1.33266, elr 2.222e-05\n",
      "[2021-10-24 21:45:50,025] [    INFO] - [step 2450 / 4000]train loss 0.25855, ppl 1.29505, elr 2.153e-05\n",
      "[2021-10-24 21:46:10,749] [    INFO] - [step 2500 / 4000]train loss 0.24674, ppl 1.27984, elr 2.083e-05\n",
      "[2021-10-24 21:46:31,441] [    INFO] - [step 2550 / 4000]train loss 0.20862, ppl 1.23198, elr 2.014e-05\n",
      "[2021-10-24 21:46:51,844] [    INFO] - [step 2600 / 4000]train loss 0.14085, ppl 1.15126, elr 1.944e-05\n",
      "[2021-10-24 21:47:12,309] [    INFO] - [step 2650 / 4000]train loss 0.18558, ppl 1.20392, elr 1.875e-05\n",
      "[2021-10-24 21:47:32,521] [    INFO] - [step 2700 / 4000]train loss 0.23041, ppl 1.25912, elr 1.806e-05\n",
      "[2021-10-24 21:47:52,745] [    INFO] - [step 2750 / 4000]train loss 0.12192, ppl 1.12966, elr 1.736e-05\n",
      "[2021-10-24 21:48:13,148] [    INFO] - [step 2800 / 4000]train loss 0.19550, ppl 1.21592, elr 1.667e-05\n",
      "[2021-10-24 21:48:33,542] [    INFO] - [step 2850 / 4000]train loss 0.27351, ppl 1.31457, elr 1.597e-05\n",
      "[2021-10-24 21:48:53,877] [    INFO] - [step 2900 / 4000]train loss 0.16181, ppl 1.17564, elr 1.528e-05\n",
      "[2021-10-24 21:49:14,428] [    INFO] - [step 2950 / 4000]train loss 0.31718, ppl 1.37326, elr 1.458e-05\n",
      "[2021-10-24 21:49:35,025] [    INFO] - [step 3000 / 4000]train loss 0.14146, ppl 1.15196, elr 1.389e-05\n",
      "[2021-10-24 21:49:55,556] [    INFO] - [step 3050 / 4000]train loss 0.12842, ppl 1.13703, elr 1.319e-05\n",
      "[2021-10-24 21:50:15,855] [    INFO] - [step 3100 / 4000]train loss 0.15979, ppl 1.17326, elr 1.250e-05\n",
      "[2021-10-24 21:50:36,296] [    INFO] - [step 3150 / 4000]train loss 0.16280, ppl 1.17680, elr 1.181e-05\n",
      "[2021-10-24 21:50:56,680] [    INFO] - [step 3200 / 4000]train loss 0.20680, ppl 1.22974, elr 1.111e-05\n",
      "[2021-10-24 21:51:16,943] [    INFO] - [step 3250 / 4000]train loss 0.23190, ppl 1.26099, elr 1.042e-05\n",
      "[2021-10-24 21:51:37,251] [    INFO] - [step 3300 / 4000]train loss 0.22426, ppl 1.25140, elr 9.722e-06\n",
      "[2021-10-24 21:51:57,669] [    INFO] - [step 3350 / 4000]train loss 0.10697, ppl 1.11290, elr 9.028e-06\n",
      "[2021-10-24 21:52:17,986] [    INFO] - [step 3400 / 4000]train loss 0.25462, ppl 1.28997, elr 8.333e-06\n",
      "[2021-10-24 21:52:38,704] [    INFO] - [step 3450 / 4000]train loss 0.19155, ppl 1.21112, elr 7.639e-06\n",
      "[2021-10-24 21:52:59,144] [    INFO] - [step 3500 / 4000]train loss 0.15898, ppl 1.17232, elr 6.944e-06\n",
      "[2021-10-24 21:53:19,517] [    INFO] - [step 3550 / 4000]train loss 0.15324, ppl 1.16560, elr 6.250e-06\n",
      "[2021-10-24 21:53:39,974] [    INFO] - [step 3600 / 4000]train loss 0.14726, ppl 1.15866, elr 5.556e-06\n",
      "[2021-10-24 21:54:00,285] [    INFO] - [step 3650 / 4000]train loss 0.10431, ppl 1.10994, elr 4.861e-06\n",
      "[2021-10-24 21:54:20,569] [    INFO] - [step 3700 / 4000]train loss 0.25284, ppl 1.28768, elr 4.167e-06\n",
      "[2021-10-24 21:54:40,775] [    INFO] - [step 3750 / 4000]train loss 0.26392, ppl 1.30202, elr 3.472e-06\n",
      "[2021-10-24 21:55:01,254] [    INFO] - [step 3800 / 4000]train loss 0.08337, ppl 1.08695, elr 2.778e-06\n",
      "[2021-10-24 21:55:21,615] [    INFO] - [step 3850 / 4000]train loss 0.08411, ppl 1.08775, elr 2.083e-06\n",
      "[2021-10-24 21:55:42,440] [    INFO] - [step 3900 / 4000]train loss 0.11322, ppl 1.11987, elr 1.389e-06\n",
      "[2021-10-24 21:56:02,908] [    INFO] - [step 3950 / 4000]train loss 0.08227, ppl 1.08575, elr 6.944e-07\n",
      "[2021-10-24 21:56:23,150] [    INFO] - [step 4000 / 4000]train loss 0.22031, ppl 1.24646, elr 0.000e+00\n",
      "[2021-10-24 21:56:23,153] [    INFO] - save the model in ernie_gen_ch_med/step_4000_ppl_1.24646.params\n"
     ]
    }
   ],
   "source": [
    "import paddlehub as hub\n",
    "\n",
    "module = hub.Module(name=\"ernie_gen\")\n",
    "\n",
    "result = module.finetune(\n",
    "    train_path='trainmh.txt',\n",
    "    save_dir=\"ernie_gen_ch_med\",\n",
    "    max_steps=4000,\n",
    "    max_encode_len=256,\n",
    "    max_decode_len=256,\n",
    "    noise_prob=0.1,\n",
    "    batch_size=16,\n",
    "    log_interval=50,\n",
    "    save_interval=2000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bda4507e-4ab2-4fef-a51c-bf8a24efc5c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-24T13:56:41.274860Z",
     "iopub.status.busy": "2021-10-24T13:56:41.274515Z",
     "iopub.status.idle": "2021-10-24T13:56:41.792632Z",
     "shell.execute_reply": "2021-10-24T13:56:41.791842Z",
     "shell.execute_reply.started": "2021-10-24T13:56:41.274808Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-10-24 21:56:41,278] [    INFO] - Already cached /home/aistudio/.paddlenlp/models/ernie-1.0/vocab.txt\n",
      "[2021-10-24 21:56:41,296] [    INFO] - Begin export the model save in ernie_gen_ch_med/step_4000_ppl_1.24646.params ...\n",
      "[2021-10-24 21:56:41,787] [    INFO] - The module has exported to /home/aistudio/ernie_gen_efaqa\n"
     ]
    }
   ],
   "source": [
    "import paddlehub as hub\n",
    "\n",
    "module = hub.Module(name=\"ernie_gen\")\n",
    "module.export(params_path='ernie_gen_ch_med/step_4000_ppl_1.24646.params', module_name=\"ernie_gen_efaqa\", author=\"vemodalen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "de961733-b118-44a0-a8ec-560c3f0549bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-24T13:57:12.293305Z",
     "iopub.status.busy": "2021-10-24T13:57:12.292933Z",
     "iopub.status.idle": "2021-10-24T13:57:18.090871Z",
     "shell.execute_reply": "2021-10-24T13:57:18.089636Z",
     "shell.execute_reply.started": "2021-10-24T13:57:12.293241Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/__init__.py:107: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import MutableMapping\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/rcsetup.py:20: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Iterable, Mapping\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/colors.py:53: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Sized\n",
      "[2021-10-24 21:57:17,512] [    INFO] - Successfully installed ernie_gen_efaqa-1.0.0\n"
     ]
    }
   ],
   "source": [
    "!hub install ernie_gen_efaqa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ec70d8c-0ca7-4605-85ed-bc334c8fb0b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-24T14:15:29.666034Z",
     "iopub.status.busy": "2021-10-24T14:15:29.665673Z",
     "iopub.status.idle": "2021-10-24T14:15:31.591051Z",
     "shell.execute_reply": "2021-10-24T14:15:31.587878Z",
     "shell.execute_reply.started": "2021-10-24T14:15:29.665980Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-10-24 22:15:29,803] [    INFO] - Already cached /home/aistudio/.paddlenlp/models/ernie-1.0/ernie_v1_chn_base.pdparams\n",
      "[2021-10-24 22:15:29,805] [   DEBUG] - init ErnieModel with config: {'attention_probs_dropout_prob': 0.1, 'hidden_act': 'relu', 'hidden_dropout_prob': 0.1, 'hidden_size': 768, 'initializer_range': 0.02, 'max_position_embeddings': 513, 'num_attention_heads': 12, 'num_hidden_layers': 12, 'type_vocab_size': 2, 'vocab_size': 18000, 'pad_token_id': 0}\n",
      "[2021-10-24 22:15:29,895] [    INFO] - loading pretrained model from /home/aistudio/.paddlenlp/models/ernie-1.0/ernie_v1_chn_base.pdparams\n",
      "[2021-10-24 22:15:30,394] [    INFO] - param:mlm_bias not set in pretrained model, skip\n",
      "[2021-10-24 22:15:30,397] [    INFO] - param:mlm.weight not set in pretrained model, skip\n",
      "[2021-10-24 22:15:30,399] [    INFO] - param:mlm.bias not set in pretrained model, skip\n",
      "[2021-10-24 22:15:30,401] [    INFO] - param:mlm_ln.weight not set in pretrained model, skip\n",
      "[2021-10-24 22:15:30,403] [    INFO] - param:mlm_ln.bias not set in pretrained model, skip\n",
      "[2021-10-24 22:15:31,396] [    INFO] - Already cached /home/aistudio/.paddlenlp/models/ernie-1.0/vocab.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['11', '1', '5', '6', '4']\n"
     ]
    }
   ],
   "source": [
    "import paddlehub as hub\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] =\"0\"\n",
    "\n",
    "module = hub.Module(name=\"ernie_gen_efaqa\")\n",
    "\n",
    "test_texts = [\"哇啊啊啊啊啊啊我学的好累了学不动了感觉要抑郁了\"]\n",
    "results = module.generate(texts=test_texts, use_gpu=True, beam_width=5)\n",
    "for result in results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48da180-a95b-42e9-99c5-e58e05c88a04",
   "metadata": {},
   "source": [
    "# FINETUNE WITH SKEP ERUINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "70caa927-e71a-4f27-b230-64141f6410e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-25T04:37:35.315855Z",
     "iopub.status.busy": "2021-10-25T04:37:35.315103Z",
     "iopub.status.idle": "2021-10-25T04:37:38.282872Z",
     "shell.execute_reply": "2021-10-25T04:37:38.282000Z",
     "shell.execute_reply.started": "2021-10-25T04:37:35.315525Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [00:00<00:00, 258723.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1.13': 0, '1.16': 1, '1.6': 2, '1.9': 3, '1.14': 4, '1.7': 5, '1.12': 6, '1.3': 7, '1.15': 8, '1.8': 9, '1.2': 10, '1.1': 11, '1.10': 12, '1.11': 13, '1.4': 14, '1.5': 15, '1.18': 16, '1.17': 17, '1.19': 18, '2.7': 19, '2.1': 20, '2.2': 21, '2.8': 22, '2.3': 23, '2.4': 24, '2.5': 25, '2.6': 26, '3.4': 27, '3.2': 28, '3.3': 29, '3.6': 30, '3.5': 31}\n",
      "20000 20000 20000\n",
      "[0, 1, 1, 1, 1]\n",
      "Counter({1: 7156, 3: 4968, 7: 1979, 5: 879, 9: 817, 13: 728, 11: 659, 0: 511, 6: 444, 2: 406, 12: 403, 10: 294, 8: 202, 4: 195, 14: 104, 18: 87, 16: 71, 17: 53, 15: 44})\n",
      "Counter({19: 17799, 20: 907, 21: 716, 24: 236, 22: 146, 23: 94, 25: 69, 26: 33})\n",
      "Counter({27: 19612, 28: 247, 29: 88, 30: 49, 31: 4})\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import efaqa_corpus_zh\n",
    "data = list(efaqa_corpus_zh.load('dataset/efaqa-corpus-zh.utf8.gz'))\n",
    "\n",
    "lines = [[post[\"title\"]] + [chat[\"value\"] for chat in post[\"chats\"]] for post in data]\n",
    "\n",
    "x=[]\n",
    "for i in lines:\n",
    "    tmp=''\n",
    "    for j in i:\n",
    "        tmp+=j\n",
    "    x.append(tmp)\n",
    "\n",
    "#get label for 3 category of 20000 post\n",
    "from tqdm import tqdm\n",
    "\n",
    "# predict label\n",
    "y_s1_raw = []\n",
    "y_s2_raw = []\n",
    "y_s3_raw = []\n",
    "\n",
    "for post in tqdm(data):\n",
    "    cluster = {item[0]: item[1] for item in post[\"label\"].items()} \n",
    "    y_s1_raw.append(cluster[\"s1\"])\n",
    "    y_s2_raw.append(cluster[\"s2\"])\n",
    "    y_s3_raw.append(cluster[\"s3\"])\n",
    "\n",
    "#mapping\n",
    "y_map = {}\n",
    "for label in y_s1_raw + y_s2_raw + y_s3_raw:\n",
    "    if label not in y_map:\n",
    "        y_map[label] = len(y_map)\n",
    "\n",
    "print(y_map)\n",
    "\n",
    "y_s1 = [y_map[label] for label in y_s1_raw]\n",
    "y_s2 = [y_map[label] for label in y_s2_raw]\n",
    "y_s3 = [y_map[label] for label in y_s3_raw]\n",
    "print(len(y_s1),len(y_s2),len(y_s3))\n",
    "print(y_s1[0:5])\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "#imbalance\n",
    "print( Counter(y_s1)   )\n",
    "print( Counter(y_s2)   )\n",
    "print( Counter(y_s3)   )\n",
    "\n",
    "x1_train, x1_test, y1_train, y1_test \\\n",
    "        = train_test_split(x, y_s1, test_size=0.3)\n",
    "x2_train, x2_test, y2_train, y2_test \\\n",
    "        = train_test_split(x, y_s2, test_size=0.3)\n",
    "x3_train, x3_test, y3_train, y3_test \\\n",
    "        = train_test_split(x, y_s3, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "29e61b20-1c3b-48a5-b63b-8079a0bec482",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-25T04:37:44.080784Z",
     "iopub.status.busy": "2021-10-25T04:37:44.080058Z",
     "iopub.status.idle": "2021-10-25T04:37:44.116255Z",
     "shell.execute_reply": "2021-10-25T04:37:44.114832Z",
     "shell.execute_reply.started": "2021-10-25T04:37:44.080466Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '女 这样是否有抑郁症？它这个我看不懂，麻烦了。我真的感觉生活没有希望。刚刚看完您所有的描述，scl～90这个测试有两个因子分，一个是阴性因子，另一个是阳性因子，只要关注下有症状的阳性因子分就好。\\n\\n那么所有＞＝2分的都属于阳性因子分。（上限分值是5，也就是最高5分是说明症状最严重。）\\n\\n不过，通常测试无法做诊断，仅供参考，更多的需要根据您生活中近期的状态相结合去做评估。s\\tc\\tl-90是从10个方面了解自己心理健康程度的1种自评量表。\\n1、反应的10个方面分别为:\\n（1）躯体化（2）强迫症状\\n（3）人际关系敏感\\n（4）抑郁（5）焦虑（6）敌对\\n（7）恐怖（8）偏执（9）精神病性\\n（10）其他，如睡眠饮食状况等\\n2、其中包括10个因子，每1个因子反映出个体某1方面的症状情况，通过因子分可了解症状分布特点。\\n3、如何评估:\\n（1）\\t总分:\\t超过160分\\n（2）阳性项目数:\\t超过43项\\n（3）单个因子分:超过2分\\n出现以上任意情况，就可考虑筛选阳性，需要进1步检查。测试条目2分以上显示有些困扰喔，有一定的抑郁情绪呢这个测试表面你有较重的抑郁情绪，不能说是否有抑郁症测试有很好的参考价值，但不做确诊依据。你生活中发生了什么？我家庭不是很完整和妈妈生活但是过的很累一周有六天都在和妈妈吵架\\n妈妈经常的一些举措让我感到害怕和妈妈没有好好说过话她也从来没有理解过我我基本有两天或者三天在深夜里或者白天都会哭我发现我现在真的改变了好多有时候没有原因但就是会难过对生活也没希望总是很烦躁。有时候睡不着觉我也时常有那些想法', 'label': 7, 'qid': ''}\n",
      "{'text': '我请问有什么可以帮你？欢迎你点我头像加关注有什么事情可以随时私聊我你可以点击关注我这里很安全我从七岁就得了癫痫病，一直吃药控制，因而导致阴茎短小，觉得自己低人一等，在性方面有欲望，但是在性生活时就勃而不坚，因此与女朋友分手，现在我总觉得自己活下去就是别人的笑柄可以点击咨询师头像，加关注，私聊，具体帮你分析确诊，咨询治疗。最好通过平台电话联系我咨询。', 'label': 1, 'qid': ''}\n"
     ]
    }
   ],
   "source": [
    "train_ds=[]\n",
    "dev_ds=[]\n",
    "\n",
    "for i in range(0,len(y1_train)):\n",
    "    tmp={'text':x1_train[i],'label':int(y1_train[i]),'qid':''}\n",
    "    train_ds.append(tmp)\n",
    "\n",
    "for i in range(0,len(y1_test)):\n",
    "    tmp={'text':x1_test[i],'label':int(y1_test[i]),'qid':''}\n",
    "    dev_ds.append(tmp)\n",
    "\n",
    "print(train_ds[0])\n",
    "print(dev_ds[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f8536f84-8ffc-4add-be76-123567194801",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-25T04:37:46.900762Z",
     "iopub.status.busy": "2021-10-25T04:37:46.899816Z",
     "iopub.status.idle": "2021-10-25T04:37:46.911841Z",
     "shell.execute_reply": "2021-10-25T04:37:46.910548Z",
     "shell.execute_reply.started": "2021-10-25T04:37:46.900348Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '有没有美女老师，想咨询下！心理变态。喜欢黑色丝袜的女人你多大了这种情况多长时间了24岁有2年了吧哦 这个问题让你有精神上的痛苦吗没有人性都是追求快乐，逃离痛苦，变态心理的起因是因为任自己欲望随心所遇，最后收不住脚。', 'label': 6, 'qid': ''}\n",
      "{'text': '女 小时候被暴露狂吓到过导致长大后对于人体X器官极为敏感一跟人交流就会手心冒汗焦虑满满的嫌弃感注意力老是觉得人体X性器官脏请问这种心理怎样克服至于吗一言难尽可是你以后得交男朋友，那避免不了有性生活的不要觉得脏不过确实也脏这个无法自我克服，需要通过咨询来处理当时给您留下的心结。心结处理好了，以后就不会影响您了。当时的场景带来的心灵创伤，需要重新认识。你这种事情有解决办法，我点我私聊吧过去引发的恐怖情绪还在今天困扰您。私聊', 'label': 9, 'qid': ''}\n"
     ]
    }
   ],
   "source": [
    "from paddlenlp.datasets import MapDataset\n",
    "train_ds=MapDataset(train_ds)\n",
    "dev_ds=MapDataset(dev_ds)\n",
    "\n",
    "print(train_ds[5])\n",
    "print(dev_ds[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddb7a765-8be9-4e24-bef1-1eea76ccd1bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-24T15:12:16.268280Z",
     "iopub.status.busy": "2021-10-24T15:12:16.267915Z",
     "iopub.status.idle": "2021-10-24T15:12:24.779870Z",
     "shell.execute_reply": "2021-10-24T15:12:24.779048Z",
     "shell.execute_reply.started": "2021-10-24T15:12:16.268224Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-10-24 23:12:16,270] [    INFO] - Already cached /home/aistudio/.paddlenlp/models/skep_ernie_1.0_large_ch/skep_ernie_1.0_large_ch.pdparams\n",
      "[2021-10-24 23:12:24,765] [    INFO] - Already cached /home/aistudio/.paddlenlp/models/skep_ernie_1.0_large_ch/skep_ernie_1.0_large_ch.vocab.txt\n"
     ]
    }
   ],
   "source": [
    "from paddlenlp.transformers import SkepForSequenceClassification, SkepTokenizer\n",
    "\n",
    "# 指定模型名称，一键加载模型\n",
    "model = SkepForSequenceClassification.from_pretrained(pretrained_model_name_or_path=\"skep_ernie_1.0_large_ch\", num_classes=19)\n",
    "# 同样地，通过指定模型名称一键加载对应的Tokenizer，用于处理文本数据，如切分token，转token_id等。\n",
    "tokenizer = SkepTokenizer.from_pretrained(pretrained_model_name_or_path=\"skep_ernie_1.0_large_ch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2eebca48-d26d-4159-ad94-325ba6c9a488",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-25T04:16:03.103693Z",
     "iopub.status.busy": "2021-10-25T04:16:03.103314Z",
     "iopub.status.idle": "2021-10-25T04:16:03.112718Z",
     "shell.execute_reply": "2021-10-25T04:16:03.111986Z",
     "shell.execute_reply.started": "2021-10-25T04:16:03.103643Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import paddle\n",
    "import paddle.nn.functional as F\n",
    "from paddlenlp.data import Stack, Tuple, Pad\n",
    "\n",
    "def convert_example(example,\n",
    "                    tokenizer,\n",
    "                    max_seq_length=512,\n",
    "                    is_test=False):\n",
    "    encoded_inputs = tokenizer(\n",
    "        text=example[\"text\"], max_seq_len=max_seq_length)\n",
    "\n",
    "\n",
    "    input_ids = encoded_inputs[\"input_ids\"]\n",
    "    # token_type_ids\n",
    "    token_type_ids = encoded_inputs[\"token_type_ids\"]\n",
    "\n",
    "    if not is_test:\n",
    "        # label\n",
    "        label = np.array([example[\"label\"]], dtype=\"int64\")\n",
    "        return input_ids, token_type_ids, label\n",
    "    else:\n",
    "        # qid = np.array([example[\"qid\"]], dtype=\"int64\")\n",
    "        return input_ids, token_type_ids\n",
    "\n",
    "def create_dataloader(dataset,\n",
    "                      trans_fn=None,\n",
    "                      mode='train',\n",
    "                      batch_size=1,\n",
    "                      batchify_fn=None):\n",
    "    if trans_fn:\n",
    "        dataset = dataset.map(trans_fn)\n",
    "\n",
    "    shuffle = True if mode == 'train' else False\n",
    "    if mode == \"train\":\n",
    "        sampler = paddle.io.DistributedBatchSampler(\n",
    "            dataset=dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    else:\n",
    "        sampler = paddle.io.BatchSampler(\n",
    "            dataset=dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    dataloader = paddle.io.DataLoader(\n",
    "        dataset, batch_sampler=sampler, collate_fn=batchify_fn)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b44968c7-49cd-425a-a076-6c614a1777ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-25T04:16:07.241336Z",
     "iopub.status.busy": "2021-10-25T04:16:07.240706Z",
     "iopub.status.idle": "2021-10-25T04:16:07.247826Z",
     "shell.execute_reply": "2021-10-25T04:16:07.247075Z",
     "shell.execute_reply.started": "2021-10-25T04:16:07.241060Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "# max_seq_length\n",
    "max_seq_length = 256\n",
    "\n",
    "trans_func = partial(\n",
    "    convert_example,\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_length=max_seq_length)\n",
    "\n",
    "batchify_fn = lambda samples, fn=Tuple(\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_id),  # input_ids\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_type_id),  # token_type_ids\n",
    "    Stack()  # labels\n",
    "): [data for data in fn(samples)]\n",
    "train_data_loader = create_dataloader(\n",
    "    train_ds,\n",
    "    mode='train',\n",
    "    batch_size=batch_size,\n",
    "    batchify_fn=batchify_fn,\n",
    "    trans_fn=trans_func)\n",
    "dev_data_loader = create_dataloader(\n",
    "    dev_ds,\n",
    "    mode='dev',\n",
    "    batch_size=batch_size,\n",
    "    batchify_fn=batchify_fn,\n",
    "    trans_fn=trans_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e24e57f-a75e-4236-9082-df2cb4f52523",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "@paddle.no_grad()\n",
    "def evaluate(model, criterion, metric, data_loader):\n",
    "    \"\"\"\n",
    "    Given a dataset, it evals model and computes the metric.\n",
    "\n",
    "    Args:\n",
    "        model(obj:`paddle.nn.Layer`): A model to classify texts.\n",
    "        criterion(obj:`paddle.nn.Layer`): It can compute the loss.\n",
    "        metric(obj:`paddle.metric.Metric`): The evaluation metric.\n",
    "        data_loader(obj:`paddle.io.DataLoader`): The dataset loader which generates batches.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    metric.reset()\n",
    "    losses = []\n",
    "    for batch in data_loader:\n",
    "        input_ids, token_type_ids, labels = batch\n",
    "        logits = model(input_ids, token_type_ids)\n",
    "        loss = criterion(logits, labels)\n",
    "        losses.append(loss.numpy())\n",
    "        correct = metric.compute(logits, labels)\n",
    "        metric.update(correct)\n",
    "        accu = metric.accumulate()\n",
    "    print(\"eval loss: %.5f, accu: %.5f\" % (np.mean(losses), accu))\n",
    "    model.train()\n",
    "    metric.reset()\n",
    "\n",
    "# setup\n",
    "epochs = 20\n",
    "# save\n",
    "ckpt_dir = \"skep_ckpt\"\n",
    "num_training_steps = len(train_data_loader) * epochs\n",
    "\n",
    "# optimizer\n",
    "optimizer = paddle.optimizer.AdamW(\n",
    "    learning_rate=2e-5,\n",
    "    parameters=model.parameters())\n",
    "# loss\n",
    "criterion = paddle.nn.loss.CrossEntropyLoss()\n",
    "# accuracy\n",
    "metric = paddle.metric.Accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384bb9f0-b3ef-4308-9e64-917bce4bd677",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-24T15:12:36.388032Z",
     "iopub.status.busy": "2021-10-24T15:12:36.387673Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 10, epoch: 1, batch: 10, loss: 2.29342, accu: 0.26250, speed: 1.33 step/s\n",
      "global step 20, epoch: 1, batch: 20, loss: 1.83934, accu: 0.36250, speed: 1.32 step/s\n",
      "global step 30, epoch: 1, batch: 30, loss: 1.79091, accu: 0.36250, speed: 1.31 step/s\n",
      "global step 40, epoch: 1, batch: 40, loss: 1.95799, accu: 0.38281, speed: 1.30 step/s\n",
      "global step 50, epoch: 1, batch: 50, loss: 2.03728, accu: 0.38250, speed: 1.29 step/s\n",
      "global step 60, epoch: 1, batch: 60, loss: 1.98817, accu: 0.39687, speed: 1.30 step/s\n",
      "global step 70, epoch: 1, batch: 70, loss: 1.23931, accu: 0.41786, speed: 1.30 step/s\n",
      "global step 80, epoch: 1, batch: 80, loss: 1.09943, accu: 0.43828, speed: 1.29 step/s\n",
      "global step 90, epoch: 1, batch: 90, loss: 1.81883, accu: 0.45139, speed: 1.29 step/s\n",
      "global step 100, epoch: 1, batch: 100, loss: 1.14124, accu: 0.46500, speed: 1.29 step/s\n",
      "eval loss: 1.35201, accu: 0.57983\n",
      "global step 110, epoch: 1, batch: 110, loss: 1.59147, accu: 0.56250, speed: 0.08 step/s\n",
      "global step 120, epoch: 1, batch: 120, loss: 1.52086, accu: 0.57812, speed: 1.30 step/s\n",
      "global step 130, epoch: 1, batch: 130, loss: 1.41202, accu: 0.55625, speed: 1.29 step/s\n",
      "global step 140, epoch: 1, batch: 140, loss: 1.48582, accu: 0.57969, speed: 1.29 step/s\n",
      "global step 150, epoch: 1, batch: 150, loss: 1.27337, accu: 0.57875, speed: 1.29 step/s\n",
      "global step 160, epoch: 1, batch: 160, loss: 1.59929, accu: 0.60104, speed: 1.29 step/s\n",
      "global step 170, epoch: 1, batch: 170, loss: 1.08960, accu: 0.59554, speed: 1.29 step/s\n",
      "global step 180, epoch: 1, batch: 180, loss: 1.27292, accu: 0.60781, speed: 1.29 step/s\n",
      "global step 190, epoch: 1, batch: 190, loss: 1.13611, accu: 0.61250, speed: 1.29 step/s\n",
      "global step 200, epoch: 1, batch: 200, loss: 1.02484, accu: 0.61687, speed: 1.30 step/s\n",
      "eval loss: 1.10838, accu: 0.63350\n",
      "global step 210, epoch: 1, batch: 210, loss: 1.37522, accu: 0.56875, speed: 0.08 step/s\n",
      "global step 220, epoch: 1, batch: 220, loss: 1.22150, accu: 0.61250, speed: 1.29 step/s\n",
      "global step 230, epoch: 1, batch: 230, loss: 1.47771, accu: 0.63958, speed: 1.29 step/s\n",
      "global step 240, epoch: 1, batch: 240, loss: 1.07959, accu: 0.64531, speed: 1.30 step/s\n",
      "global step 250, epoch: 1, batch: 250, loss: 1.06309, accu: 0.64125, speed: 1.28 step/s\n",
      "global step 260, epoch: 1, batch: 260, loss: 1.07986, accu: 0.64792, speed: 1.29 step/s\n",
      "global step 270, epoch: 1, batch: 270, loss: 1.50637, accu: 0.65268, speed: 1.29 step/s\n",
      "global step 280, epoch: 1, batch: 280, loss: 1.25570, accu: 0.65234, speed: 1.28 step/s\n",
      "global step 290, epoch: 1, batch: 290, loss: 1.19248, accu: 0.65000, speed: 1.29 step/s\n",
      "global step 300, epoch: 1, batch: 300, loss: 1.15652, accu: 0.65375, speed: 1.30 step/s\n",
      "eval loss: 1.01403, accu: 0.66767\n",
      "global step 310, epoch: 1, batch: 310, loss: 0.81375, accu: 0.68125, speed: 0.08 step/s\n",
      "global step 320, epoch: 1, batch: 320, loss: 0.67072, accu: 0.62500, speed: 1.30 step/s\n",
      "global step 330, epoch: 1, batch: 330, loss: 0.77304, accu: 0.63958, speed: 1.30 step/s\n",
      "global step 340, epoch: 1, batch: 340, loss: 1.03207, accu: 0.64531, speed: 1.30 step/s\n",
      "global step 350, epoch: 1, batch: 350, loss: 0.96652, accu: 0.64125, speed: 1.30 step/s\n",
      "global step 360, epoch: 1, batch: 360, loss: 1.03672, accu: 0.64583, speed: 1.29 step/s\n",
      "global step 370, epoch: 1, batch: 370, loss: 1.01523, accu: 0.64464, speed: 1.28 step/s\n",
      "global step 380, epoch: 1, batch: 380, loss: 1.11756, accu: 0.64219, speed: 1.29 step/s\n",
      "global step 390, epoch: 1, batch: 390, loss: 0.77353, accu: 0.63750, speed: 1.29 step/s\n",
      "global step 400, epoch: 1, batch: 400, loss: 0.83320, accu: 0.64125, speed: 1.29 step/s\n",
      "eval loss: 1.00612, accu: 0.65583\n",
      "global step 410, epoch: 1, batch: 410, loss: 1.29924, accu: 0.63750, speed: 0.08 step/s\n",
      "global step 420, epoch: 1, batch: 420, loss: 1.15813, accu: 0.65625, speed: 1.29 step/s\n",
      "global step 430, epoch: 1, batch: 430, loss: 1.37830, accu: 0.66667, speed: 1.30 step/s\n",
      "global step 440, epoch: 1, batch: 440, loss: 0.47763, accu: 0.67344, speed: 1.30 step/s\n",
      "global step 450, epoch: 1, batch: 450, loss: 0.96674, accu: 0.65875, speed: 1.30 step/s\n",
      "global step 460, epoch: 1, batch: 460, loss: 0.92018, accu: 0.65833, speed: 1.30 step/s\n",
      "global step 470, epoch: 1, batch: 470, loss: 0.87559, accu: 0.66339, speed: 1.30 step/s\n",
      "global step 480, epoch: 1, batch: 480, loss: 1.30763, accu: 0.65781, speed: 1.30 step/s\n",
      "global step 490, epoch: 1, batch: 490, loss: 1.35944, accu: 0.66250, speed: 1.30 step/s\n",
      "global step 500, epoch: 1, batch: 500, loss: 0.93525, accu: 0.66250, speed: 1.30 step/s\n",
      "eval loss: 0.96472, accu: 0.67250\n",
      "global step 510, epoch: 1, batch: 510, loss: 0.59938, accu: 0.75625, speed: 0.09 step/s\n",
      "global step 520, epoch: 1, batch: 520, loss: 1.03392, accu: 0.72188, speed: 1.28 step/s\n",
      "global step 530, epoch: 1, batch: 530, loss: 0.77945, accu: 0.72708, speed: 1.30 step/s\n",
      "global step 540, epoch: 1, batch: 540, loss: 1.36068, accu: 0.70312, speed: 1.29 step/s\n",
      "global step 550, epoch: 1, batch: 550, loss: 1.02567, accu: 0.70375, speed: 1.28 step/s\n",
      "global step 560, epoch: 1, batch: 560, loss: 1.17536, accu: 0.70000, speed: 1.28 step/s\n",
      "global step 570, epoch: 1, batch: 570, loss: 1.25064, accu: 0.70089, speed: 1.29 step/s\n",
      "global step 580, epoch: 1, batch: 580, loss: 0.66876, accu: 0.69375, speed: 1.29 step/s\n",
      "global step 590, epoch: 1, batch: 590, loss: 0.59287, accu: 0.69375, speed: 1.27 step/s\n",
      "global step 600, epoch: 1, batch: 600, loss: 0.58481, accu: 0.69375, speed: 1.28 step/s\n",
      "eval loss: 0.98934, accu: 0.65350\n",
      "global step 610, epoch: 1, batch: 610, loss: 1.05709, accu: 0.63750, speed: 0.09 step/s\n",
      "global step 620, epoch: 1, batch: 620, loss: 0.86949, accu: 0.66875, speed: 1.30 step/s\n",
      "global step 630, epoch: 1, batch: 630, loss: 1.28698, accu: 0.67292, speed: 1.30 step/s\n",
      "global step 640, epoch: 1, batch: 640, loss: 1.13239, accu: 0.67031, speed: 1.30 step/s\n",
      "global step 650, epoch: 1, batch: 650, loss: 1.00106, accu: 0.67000, speed: 1.31 step/s\n",
      "global step 660, epoch: 1, batch: 660, loss: 0.63271, accu: 0.68333, speed: 1.30 step/s\n",
      "global step 670, epoch: 1, batch: 670, loss: 0.99252, accu: 0.68036, speed: 1.24 step/s\n",
      "global step 680, epoch: 1, batch: 680, loss: 1.30272, accu: 0.67812, speed: 1.30 step/s\n",
      "global step 690, epoch: 1, batch: 690, loss: 0.89219, accu: 0.67847, speed: 1.29 step/s\n",
      "global step 700, epoch: 1, batch: 700, loss: 1.19961, accu: 0.67125, speed: 1.30 step/s\n",
      "eval loss: 0.94653, accu: 0.67533\n",
      "global step 710, epoch: 1, batch: 710, loss: 1.15933, accu: 0.63750, speed: 0.09 step/s\n",
      "global step 720, epoch: 1, batch: 720, loss: 0.89032, accu: 0.66875, speed: 1.30 step/s\n",
      "global step 730, epoch: 1, batch: 730, loss: 0.91446, accu: 0.70000, speed: 1.30 step/s\n",
      "global step 740, epoch: 1, batch: 740, loss: 0.46273, accu: 0.70312, speed: 1.29 step/s\n",
      "global step 750, epoch: 1, batch: 750, loss: 1.12405, accu: 0.70750, speed: 1.30 step/s\n",
      "global step 760, epoch: 1, batch: 760, loss: 1.19980, accu: 0.70000, speed: 1.30 step/s\n",
      "global step 770, epoch: 1, batch: 770, loss: 0.92369, accu: 0.69554, speed: 1.30 step/s\n",
      "global step 780, epoch: 1, batch: 780, loss: 1.27962, accu: 0.69766, speed: 1.29 step/s\n",
      "global step 790, epoch: 1, batch: 790, loss: 0.81519, accu: 0.69236, speed: 1.30 step/s\n",
      "global step 800, epoch: 1, batch: 800, loss: 0.74942, accu: 0.69000, speed: 1.30 step/s\n",
      "eval loss: 0.91461, accu: 0.68567\n",
      "global step 810, epoch: 1, batch: 810, loss: 1.07518, accu: 0.65625, speed: 0.09 step/s\n",
      "global step 820, epoch: 1, batch: 820, loss: 0.88849, accu: 0.68750, speed: 1.30 step/s\n",
      "global step 830, epoch: 1, batch: 830, loss: 0.82858, accu: 0.66667, speed: 1.29 step/s\n",
      "global step 840, epoch: 1, batch: 840, loss: 0.87045, accu: 0.65781, speed: 1.30 step/s\n",
      "global step 850, epoch: 1, batch: 850, loss: 0.97873, accu: 0.65500, speed: 1.29 step/s\n",
      "global step 860, epoch: 1, batch: 860, loss: 0.98921, accu: 0.64792, speed: 1.29 step/s\n",
      "global step 870, epoch: 1, batch: 870, loss: 0.54651, accu: 0.65625, speed: 1.29 step/s\n",
      "global step 880, epoch: 2, batch: 5, loss: 0.77319, accu: 0.66406, speed: 1.29 step/s\n",
      "global step 890, epoch: 2, batch: 15, loss: 0.85861, accu: 0.66458, speed: 1.29 step/s\n",
      "global step 900, epoch: 2, batch: 25, loss: 0.90790, accu: 0.67250, speed: 1.29 step/s\n",
      "eval loss: 0.92605, accu: 0.67867\n",
      "global step 910, epoch: 2, batch: 35, loss: 1.01448, accu: 0.76875, speed: 0.09 step/s\n",
      "global step 920, epoch: 2, batch: 45, loss: 0.87667, accu: 0.74375, speed: 1.31 step/s\n",
      "global step 930, epoch: 2, batch: 55, loss: 0.80716, accu: 0.74792, speed: 1.30 step/s\n",
      "global step 940, epoch: 2, batch: 65, loss: 0.91934, accu: 0.74375, speed: 1.30 step/s\n",
      "global step 950, epoch: 2, batch: 75, loss: 1.16664, accu: 0.73625, speed: 1.30 step/s\n",
      "global step 960, epoch: 2, batch: 85, loss: 0.82505, accu: 0.73438, speed: 1.29 step/s\n",
      "global step 970, epoch: 2, batch: 95, loss: 0.62882, accu: 0.73304, speed: 1.30 step/s\n",
      "global step 980, epoch: 2, batch: 105, loss: 0.94055, accu: 0.74141, speed: 1.30 step/s\n",
      "global step 990, epoch: 2, batch: 115, loss: 0.45961, accu: 0.74097, speed: 1.30 step/s\n",
      "global step 1000, epoch: 2, batch: 125, loss: 0.55660, accu: 0.74187, speed: 1.30 step/s\n",
      "eval loss: 0.92083, accu: 0.68817\n",
      "global step 1010, epoch: 2, batch: 135, loss: 1.10055, accu: 0.78750, speed: 0.09 step/s\n",
      "global step 1020, epoch: 2, batch: 145, loss: 0.89310, accu: 0.76562, speed: 1.30 step/s\n",
      "global step 1030, epoch: 2, batch: 155, loss: 0.79834, accu: 0.75000, speed: 1.30 step/s\n",
      "global step 1040, epoch: 2, batch: 165, loss: 0.69719, accu: 0.74219, speed: 1.30 step/s\n",
      "global step 1050, epoch: 2, batch: 175, loss: 0.88932, accu: 0.74750, speed: 1.30 step/s\n",
      "global step 1060, epoch: 2, batch: 185, loss: 0.60978, accu: 0.73958, speed: 1.30 step/s\n",
      "global step 1070, epoch: 2, batch: 195, loss: 0.49968, accu: 0.74554, speed: 1.31 step/s\n",
      "global step 1080, epoch: 2, batch: 205, loss: 0.79848, accu: 0.73672, speed: 1.30 step/s\n",
      "global step 1090, epoch: 2, batch: 215, loss: 0.73142, accu: 0.73611, speed: 1.29 step/s\n",
      "global step 1100, epoch: 2, batch: 225, loss: 1.09938, accu: 0.73313, speed: 1.30 step/s\n",
      "eval loss: 0.94139, accu: 0.68433\n",
      "global step 1110, epoch: 2, batch: 235, loss: 0.85364, accu: 0.78125, speed: 0.09 step/s\n",
      "global step 1120, epoch: 2, batch: 245, loss: 0.66402, accu: 0.74687, speed: 1.30 step/s\n",
      "global step 1130, epoch: 2, batch: 255, loss: 0.65764, accu: 0.72708, speed: 1.31 step/s\n",
      "global step 1140, epoch: 2, batch: 265, loss: 0.67831, accu: 0.72188, speed: 1.29 step/s\n",
      "global step 1150, epoch: 2, batch: 275, loss: 0.39816, accu: 0.72250, speed: 1.29 step/s\n",
      "global step 1160, epoch: 2, batch: 285, loss: 0.72864, accu: 0.72917, speed: 1.28 step/s\n",
      "global step 1170, epoch: 2, batch: 295, loss: 0.81238, accu: 0.73036, speed: 1.29 step/s\n",
      "global step 1180, epoch: 2, batch: 305, loss: 0.96961, accu: 0.73125, speed: 1.29 step/s\n",
      "global step 1190, epoch: 2, batch: 315, loss: 0.82694, accu: 0.72917, speed: 1.29 step/s\n",
      "global step 1200, epoch: 2, batch: 325, loss: 0.51293, accu: 0.72750, speed: 1.29 step/s\n",
      "eval loss: 0.95202, accu: 0.66967\n",
      "global step 1210, epoch: 2, batch: 335, loss: 0.51218, accu: 0.66875, speed: 0.09 step/s\n",
      "global step 1220, epoch: 2, batch: 345, loss: 0.47252, accu: 0.73125, speed: 1.29 step/s\n",
      "global step 1230, epoch: 2, batch: 355, loss: 0.65541, accu: 0.71875, speed: 1.29 step/s\n",
      "global step 1240, epoch: 2, batch: 365, loss: 1.14517, accu: 0.71250, speed: 1.29 step/s\n",
      "global step 1250, epoch: 2, batch: 375, loss: 0.92383, accu: 0.71000, speed: 1.29 step/s\n",
      "global step 1260, epoch: 2, batch: 385, loss: 0.66496, accu: 0.71875, speed: 1.29 step/s\n",
      "global step 1270, epoch: 2, batch: 395, loss: 0.66149, accu: 0.72143, speed: 1.28 step/s\n",
      "global step 1280, epoch: 2, batch: 405, loss: 0.73724, accu: 0.72734, speed: 1.29 step/s\n",
      "global step 1290, epoch: 2, batch: 415, loss: 0.57483, accu: 0.73194, speed: 1.29 step/s\n",
      "global step 1300, epoch: 2, batch: 425, loss: 1.08531, accu: 0.72562, speed: 1.29 step/s\n",
      "eval loss: 0.92362, accu: 0.67850\n",
      "global step 1310, epoch: 2, batch: 435, loss: 0.66157, accu: 0.68750, speed: 0.09 step/s\n",
      "global step 1320, epoch: 2, batch: 445, loss: 0.76088, accu: 0.69375, speed: 1.29 step/s\n",
      "global step 1330, epoch: 2, batch: 455, loss: 0.47884, accu: 0.71667, speed: 1.30 step/s\n",
      "global step 1340, epoch: 2, batch: 465, loss: 0.98108, accu: 0.70312, speed: 1.28 step/s\n",
      "global step 1350, epoch: 2, batch: 475, loss: 0.46156, accu: 0.71375, speed: 1.28 step/s\n",
      "global step 1360, epoch: 2, batch: 485, loss: 0.92334, accu: 0.71146, speed: 1.29 step/s\n",
      "global step 1370, epoch: 2, batch: 495, loss: 0.44152, accu: 0.72143, speed: 1.30 step/s\n",
      "global step 1380, epoch: 2, batch: 505, loss: 0.81254, accu: 0.72109, speed: 1.29 step/s\n",
      "global step 1390, epoch: 2, batch: 515, loss: 0.68787, accu: 0.72153, speed: 1.30 step/s\n",
      "global step 1400, epoch: 2, batch: 525, loss: 0.78941, accu: 0.72375, speed: 1.29 step/s\n",
      "eval loss: 0.92162, accu: 0.68233\n",
      "global step 1410, epoch: 2, batch: 535, loss: 0.97605, accu: 0.73750, speed: 0.09 step/s\n",
      "global step 1420, epoch: 2, batch: 545, loss: 0.88491, accu: 0.72500, speed: 1.29 step/s\n",
      "global step 1430, epoch: 2, batch: 555, loss: 0.67053, accu: 0.72292, speed: 1.29 step/s\n",
      "global step 1440, epoch: 2, batch: 565, loss: 0.52352, accu: 0.72188, speed: 1.29 step/s\n",
      "global step 1450, epoch: 2, batch: 575, loss: 1.08066, accu: 0.71500, speed: 1.29 step/s\n",
      "global step 1460, epoch: 2, batch: 585, loss: 0.77742, accu: 0.70417, speed: 1.27 step/s\n",
      "global step 1470, epoch: 2, batch: 595, loss: 0.93870, accu: 0.71875, speed: 1.28 step/s\n",
      "global step 1480, epoch: 2, batch: 605, loss: 0.81387, accu: 0.71484, speed: 1.30 step/s\n",
      "global step 1490, epoch: 2, batch: 615, loss: 1.24107, accu: 0.71667, speed: 1.29 step/s\n",
      "global step 1500, epoch: 2, batch: 625, loss: 0.80642, accu: 0.71875, speed: 1.30 step/s\n",
      "eval loss: 0.90451, accu: 0.67883\n",
      "global step 1510, epoch: 2, batch: 635, loss: 0.40024, accu: 0.69375, speed: 0.09 step/s\n",
      "global step 1520, epoch: 2, batch: 645, loss: 0.78904, accu: 0.72813, speed: 1.29 step/s\n",
      "global step 1530, epoch: 2, batch: 655, loss: 0.82063, accu: 0.71667, speed: 1.29 step/s\n",
      "global step 1540, epoch: 2, batch: 665, loss: 0.64819, accu: 0.72031, speed: 1.29 step/s\n",
      "global step 1550, epoch: 2, batch: 675, loss: 0.61819, accu: 0.72125, speed: 1.29 step/s\n",
      "global step 1560, epoch: 2, batch: 685, loss: 0.64764, accu: 0.72813, speed: 1.29 step/s\n",
      "global step 1570, epoch: 2, batch: 695, loss: 1.32267, accu: 0.72321, speed: 1.29 step/s\n",
      "global step 1580, epoch: 2, batch: 705, loss: 0.61129, accu: 0.72109, speed: 1.29 step/s\n",
      "global step 1590, epoch: 2, batch: 715, loss: 0.70805, accu: 0.72361, speed: 1.29 step/s\n",
      "global step 1600, epoch: 2, batch: 725, loss: 1.00574, accu: 0.72062, speed: 1.29 step/s\n",
      "eval loss: 0.90668, accu: 0.68467\n",
      "global step 1610, epoch: 2, batch: 735, loss: 0.56845, accu: 0.79375, speed: 0.09 step/s\n",
      "global step 1620, epoch: 2, batch: 745, loss: 0.84949, accu: 0.75313, speed: 1.29 step/s\n",
      "global step 1630, epoch: 2, batch: 755, loss: 0.75910, accu: 0.73958, speed: 1.29 step/s\n",
      "global step 1640, epoch: 2, batch: 765, loss: 0.78793, accu: 0.74062, speed: 1.29 step/s\n",
      "global step 1650, epoch: 2, batch: 775, loss: 0.57785, accu: 0.72875, speed: 1.29 step/s\n",
      "global step 1660, epoch: 2, batch: 785, loss: 0.81796, accu: 0.72500, speed: 1.30 step/s\n",
      "global step 1670, epoch: 2, batch: 795, loss: 0.42647, accu: 0.72857, speed: 1.29 step/s\n",
      "global step 1680, epoch: 2, batch: 805, loss: 0.62717, accu: 0.72813, speed: 1.28 step/s\n",
      "global step 1690, epoch: 2, batch: 815, loss: 0.85410, accu: 0.72778, speed: 1.30 step/s\n",
      "global step 1700, epoch: 2, batch: 825, loss: 1.20352, accu: 0.72188, speed: 1.30 step/s\n",
      "eval loss: 0.92466, accu: 0.68150\n",
      "global step 1710, epoch: 2, batch: 835, loss: 0.45476, accu: 0.80000, speed: 0.09 step/s\n",
      "global step 1720, epoch: 2, batch: 845, loss: 1.01736, accu: 0.75938, speed: 1.30 step/s\n",
      "global step 1730, epoch: 2, batch: 855, loss: 0.85150, accu: 0.75833, speed: 1.30 step/s\n",
      "global step 1740, epoch: 2, batch: 865, loss: 0.74575, accu: 0.75938, speed: 1.30 step/s\n",
      "global step 1750, epoch: 2, batch: 875, loss: 0.55936, accu: 0.75750, speed: 1.32 step/s\n",
      "global step 1760, epoch: 3, batch: 10, loss: 0.38277, accu: 0.77708, speed: 1.27 step/s\n",
      "global step 1770, epoch: 3, batch: 20, loss: 0.49403, accu: 0.77500, speed: 1.29 step/s\n",
      "global step 1780, epoch: 3, batch: 30, loss: 0.80117, accu: 0.77578, speed: 1.29 step/s\n",
      "global step 1790, epoch: 3, batch: 40, loss: 0.57538, accu: 0.77569, speed: 1.29 step/s\n",
      "global step 1800, epoch: 3, batch: 50, loss: 0.55575, accu: 0.78312, speed: 1.30 step/s\n",
      "eval loss: 0.94454, accu: 0.67883\n",
      "global step 1810, epoch: 3, batch: 60, loss: 0.50516, accu: 0.78125, speed: 0.09 step/s\n",
      "global step 1820, epoch: 3, batch: 70, loss: 0.49527, accu: 0.79063, speed: 1.29 step/s\n",
      "global step 1830, epoch: 3, batch: 80, loss: 0.89538, accu: 0.78333, speed: 1.27 step/s\n",
      "global step 1840, epoch: 3, batch: 90, loss: 0.36955, accu: 0.78906, speed: 1.27 step/s\n",
      "global step 1850, epoch: 3, batch: 100, loss: 0.77073, accu: 0.79375, speed: 1.27 step/s\n",
      "global step 1860, epoch: 3, batch: 110, loss: 0.54835, accu: 0.79063, speed: 1.28 step/s\n",
      "global step 1870, epoch: 3, batch: 120, loss: 0.52825, accu: 0.79018, speed: 1.28 step/s\n",
      "global step 1880, epoch: 3, batch: 130, loss: 0.45508, accu: 0.79688, speed: 1.28 step/s\n",
      "global step 1890, epoch: 3, batch: 140, loss: 0.71195, accu: 0.79931, speed: 1.28 step/s\n",
      "global step 1900, epoch: 3, batch: 150, loss: 0.18589, accu: 0.80500, speed: 1.30 step/s\n",
      "eval loss: 0.97418, accu: 0.68233\n",
      "global step 1910, epoch: 3, batch: 160, loss: 0.13584, accu: 0.87500, speed: 0.09 step/s\n",
      "global step 1920, epoch: 3, batch: 170, loss: 0.26083, accu: 0.85000, speed: 1.30 step/s\n",
      "global step 1930, epoch: 3, batch: 180, loss: 0.40767, accu: 0.84792, speed: 1.30 step/s\n",
      "global step 1940, epoch: 3, batch: 190, loss: 0.57220, accu: 0.84688, speed: 1.30 step/s\n",
      "global step 1950, epoch: 3, batch: 200, loss: 0.81868, accu: 0.83000, speed: 1.30 step/s\n",
      "global step 1960, epoch: 3, batch: 210, loss: 0.56197, accu: 0.82604, speed: 1.30 step/s\n",
      "global step 1970, epoch: 3, batch: 220, loss: 0.28141, accu: 0.82411, speed: 1.30 step/s\n",
      "global step 1980, epoch: 3, batch: 230, loss: 0.45503, accu: 0.82266, speed: 1.30 step/s\n",
      "global step 1990, epoch: 3, batch: 240, loss: 0.55581, accu: 0.82014, speed: 1.30 step/s\n",
      "global step 2000, epoch: 3, batch: 250, loss: 0.71088, accu: 0.82125, speed: 1.30 step/s\n",
      "eval loss: 1.04703, accu: 0.66850\n",
      "global step 2010, epoch: 3, batch: 260, loss: 0.79277, accu: 0.75000, speed: 0.09 step/s\n",
      "global step 2020, epoch: 3, batch: 270, loss: 0.33680, accu: 0.76562, speed: 1.30 step/s\n",
      "global step 2030, epoch: 3, batch: 280, loss: 1.04158, accu: 0.79583, speed: 1.29 step/s\n",
      "global step 2040, epoch: 3, batch: 290, loss: 0.33408, accu: 0.79375, speed: 1.29 step/s\n",
      "global step 2050, epoch: 3, batch: 300, loss: 0.45771, accu: 0.79500, speed: 1.28 step/s\n",
      "global step 2060, epoch: 3, batch: 310, loss: 0.44695, accu: 0.80208, speed: 1.30 step/s\n",
      "global step 2070, epoch: 3, batch: 320, loss: 0.47751, accu: 0.80625, speed: 1.29 step/s\n",
      "global step 2080, epoch: 3, batch: 330, loss: 0.32712, accu: 0.80625, speed: 1.29 step/s\n",
      "global step 2090, epoch: 3, batch: 340, loss: 0.61323, accu: 0.80625, speed: 1.29 step/s\n",
      "global step 2100, epoch: 3, batch: 350, loss: 0.71394, accu: 0.80625, speed: 1.29 step/s\n",
      "eval loss: 0.98903, accu: 0.66783\n",
      "global step 2110, epoch: 3, batch: 360, loss: 0.91979, accu: 0.79375, speed: 0.09 step/s\n",
      "global step 2120, epoch: 3, batch: 370, loss: 0.64343, accu: 0.76250, speed: 1.29 step/s\n",
      "global step 2130, epoch: 3, batch: 380, loss: 0.52046, accu: 0.76875, speed: 1.29 step/s\n",
      "global step 2140, epoch: 3, batch: 390, loss: 0.60602, accu: 0.78125, speed: 1.29 step/s\n",
      "global step 2150, epoch: 3, batch: 400, loss: 0.53282, accu: 0.78250, speed: 1.29 step/s\n",
      "global step 2160, epoch: 3, batch: 410, loss: 0.32271, accu: 0.78750, speed: 1.28 step/s\n",
      "global step 2170, epoch: 3, batch: 420, loss: 0.45427, accu: 0.78214, speed: 1.28 step/s\n",
      "global step 2180, epoch: 3, batch: 430, loss: 0.36072, accu: 0.78438, speed: 1.29 step/s\n",
      "global step 2190, epoch: 3, batch: 440, loss: 0.29197, accu: 0.78819, speed: 1.29 step/s\n",
      "global step 2200, epoch: 3, batch: 450, loss: 0.47181, accu: 0.78438, speed: 1.29 step/s\n",
      "eval loss: 1.00980, accu: 0.66633\n",
      "global step 2210, epoch: 3, batch: 460, loss: 1.13270, accu: 0.76250, speed: 0.09 step/s\n",
      "global step 2220, epoch: 3, batch: 470, loss: 0.24782, accu: 0.80312, speed: 1.28 step/s\n",
      "global step 2230, epoch: 3, batch: 480, loss: 0.80494, accu: 0.80833, speed: 1.28 step/s\n",
      "global step 2240, epoch: 3, batch: 490, loss: 0.49628, accu: 0.81563, speed: 1.28 step/s\n",
      "global step 2250, epoch: 3, batch: 500, loss: 0.52871, accu: 0.81750, speed: 1.28 step/s\n",
      "global step 2260, epoch: 3, batch: 510, loss: 0.30968, accu: 0.82083, speed: 1.28 step/s\n",
      "global step 2270, epoch: 3, batch: 520, loss: 1.06247, accu: 0.81161, speed: 1.28 step/s\n",
      "global step 2280, epoch: 3, batch: 530, loss: 0.44698, accu: 0.80625, speed: 1.28 step/s\n",
      "global step 2290, epoch: 3, batch: 540, loss: 0.40233, accu: 0.80625, speed: 1.29 step/s\n",
      "global step 2300, epoch: 3, batch: 550, loss: 0.67625, accu: 0.80812, speed: 1.30 step/s\n",
      "eval loss: 0.97977, accu: 0.66700\n",
      "global step 2310, epoch: 3, batch: 560, loss: 0.74814, accu: 0.81875, speed: 0.09 step/s\n",
      "global step 2320, epoch: 3, batch: 570, loss: 0.67765, accu: 0.80937, speed: 1.29 step/s\n",
      "global step 2330, epoch: 3, batch: 580, loss: 0.38344, accu: 0.81250, speed: 1.29 step/s\n",
      "global step 2340, epoch: 3, batch: 590, loss: 0.23968, accu: 0.81719, speed: 1.29 step/s\n",
      "global step 2350, epoch: 3, batch: 600, loss: 0.27919, accu: 0.81250, speed: 1.29 step/s\n",
      "global step 2360, epoch: 3, batch: 610, loss: 0.75088, accu: 0.80729, speed: 1.29 step/s\n",
      "global step 2370, epoch: 3, batch: 620, loss: 0.58964, accu: 0.81518, speed: 1.29 step/s\n",
      "global step 2380, epoch: 3, batch: 630, loss: 0.41749, accu: 0.81172, speed: 1.30 step/s\n",
      "global step 2390, epoch: 3, batch: 640, loss: 0.30126, accu: 0.80486, speed: 1.29 step/s\n",
      "global step 2400, epoch: 3, batch: 650, loss: 0.28385, accu: 0.80250, speed: 1.29 step/s\n",
      "eval loss: 0.98680, accu: 0.67567\n",
      "global step 2410, epoch: 3, batch: 660, loss: 1.03812, accu: 0.78125, speed: 0.09 step/s\n",
      "global step 2420, epoch: 3, batch: 670, loss: 0.65754, accu: 0.81250, speed: 1.29 step/s\n",
      "global step 2430, epoch: 3, batch: 680, loss: 0.38422, accu: 0.80000, speed: 1.28 step/s\n",
      "global step 2440, epoch: 3, batch: 690, loss: 0.38742, accu: 0.79688, speed: 1.29 step/s\n",
      "global step 2450, epoch: 3, batch: 700, loss: 0.65771, accu: 0.80000, speed: 1.28 step/s\n",
      "global step 2460, epoch: 3, batch: 710, loss: 0.78566, accu: 0.79063, speed: 1.29 step/s\n",
      "global step 2470, epoch: 3, batch: 720, loss: 0.68217, accu: 0.78036, speed: 1.29 step/s\n",
      "global step 2480, epoch: 3, batch: 730, loss: 0.20305, accu: 0.78203, speed: 1.29 step/s\n",
      "global step 2490, epoch: 3, batch: 740, loss: 0.91418, accu: 0.77708, speed: 1.29 step/s\n",
      "global step 2500, epoch: 3, batch: 750, loss: 0.52030, accu: 0.77750, speed: 1.28 step/s\n",
      "eval loss: 0.97703, accu: 0.66583\n",
      "global step 2510, epoch: 3, batch: 760, loss: 0.65909, accu: 0.80625, speed: 0.09 step/s\n",
      "global step 2520, epoch: 3, batch: 770, loss: 0.98001, accu: 0.80625, speed: 1.30 step/s\n",
      "global step 2530, epoch: 3, batch: 780, loss: 0.66860, accu: 0.79583, speed: 1.29 step/s\n",
      "global step 2540, epoch: 3, batch: 790, loss: 0.69867, accu: 0.77969, speed: 1.29 step/s\n",
      "global step 2550, epoch: 3, batch: 800, loss: 1.26655, accu: 0.77750, speed: 1.29 step/s\n",
      "global step 2560, epoch: 3, batch: 810, loss: 0.79536, accu: 0.77604, speed: 1.29 step/s\n",
      "global step 2570, epoch: 3, batch: 820, loss: 1.20102, accu: 0.77411, speed: 1.29 step/s\n",
      "global step 2580, epoch: 3, batch: 830, loss: 0.71108, accu: 0.77812, speed: 1.28 step/s\n",
      "global step 2590, epoch: 3, batch: 840, loss: 0.58701, accu: 0.77500, speed: 1.28 step/s\n",
      "global step 2600, epoch: 3, batch: 850, loss: 0.63418, accu: 0.77625, speed: 1.29 step/s\n",
      "eval loss: 0.99122, accu: 0.66517\n",
      "global step 2610, epoch: 3, batch: 860, loss: 0.73911, accu: 0.75000, speed: 0.09 step/s\n",
      "global step 2620, epoch: 3, batch: 870, loss: 0.59595, accu: 0.77500, speed: 1.30 step/s\n",
      "global step 2630, epoch: 4, batch: 5, loss: 0.81843, accu: 0.78125, speed: 1.29 step/s\n",
      "global step 2640, epoch: 4, batch: 15, loss: 0.31397, accu: 0.81250, speed: 1.29 step/s\n",
      "global step 2650, epoch: 4, batch: 25, loss: 0.49570, accu: 0.82625, speed: 1.28 step/s\n",
      "global step 2660, epoch: 4, batch: 35, loss: 0.38636, accu: 0.83125, speed: 1.30 step/s\n",
      "global step 2670, epoch: 4, batch: 45, loss: 0.74027, accu: 0.83839, speed: 1.30 step/s\n",
      "global step 2680, epoch: 4, batch: 55, loss: 0.53087, accu: 0.83750, speed: 1.28 step/s\n",
      "global step 2690, epoch: 4, batch: 65, loss: 0.23575, accu: 0.84861, speed: 1.29 step/s\n",
      "global step 2700, epoch: 4, batch: 75, loss: 0.29259, accu: 0.85625, speed: 1.30 step/s\n",
      "eval loss: 1.07643, accu: 0.67283\n",
      "global step 2710, epoch: 4, batch: 85, loss: 0.61579, accu: 0.86875, speed: 0.09 step/s\n",
      "global step 2720, epoch: 4, batch: 95, loss: 0.32495, accu: 0.87813, speed: 1.29 step/s\n",
      "global step 2730, epoch: 4, batch: 105, loss: 0.41853, accu: 0.88542, speed: 1.30 step/s\n",
      "global step 2740, epoch: 4, batch: 115, loss: 0.26165, accu: 0.88125, speed: 1.29 step/s\n",
      "global step 2750, epoch: 4, batch: 125, loss: 0.16554, accu: 0.88375, speed: 1.29 step/s\n",
      "global step 2760, epoch: 4, batch: 135, loss: 0.60701, accu: 0.88750, speed: 1.28 step/s\n",
      "global step 2770, epoch: 4, batch: 145, loss: 0.25099, accu: 0.88750, speed: 1.28 step/s\n",
      "global step 2780, epoch: 4, batch: 155, loss: 0.23737, accu: 0.88984, speed: 1.29 step/s\n",
      "global step 2790, epoch: 4, batch: 165, loss: 0.27169, accu: 0.88889, speed: 1.28 step/s\n",
      "global step 2800, epoch: 4, batch: 175, loss: 0.40923, accu: 0.88938, speed: 1.28 step/s\n",
      "eval loss: 1.06548, accu: 0.67067\n",
      "global step 2810, epoch: 4, batch: 185, loss: 0.86371, accu: 0.88125, speed: 0.09 step/s\n",
      "global step 2820, epoch: 4, batch: 195, loss: 0.16963, accu: 0.86875, speed: 1.31 step/s\n",
      "global step 2830, epoch: 4, batch: 205, loss: 0.16986, accu: 0.87917, speed: 1.29 step/s\n",
      "global step 2840, epoch: 4, batch: 215, loss: 0.24428, accu: 0.87813, speed: 1.30 step/s\n",
      "global step 2850, epoch: 4, batch: 225, loss: 0.25052, accu: 0.88125, speed: 1.31 step/s\n",
      "global step 2860, epoch: 4, batch: 235, loss: 0.19388, accu: 0.89375, speed: 1.30 step/s\n",
      "global step 2870, epoch: 4, batch: 245, loss: 0.34513, accu: 0.89196, speed: 1.30 step/s\n",
      "global step 2880, epoch: 4, batch: 255, loss: 0.20154, accu: 0.88750, speed: 1.29 step/s\n",
      "global step 2890, epoch: 4, batch: 265, loss: 0.40693, accu: 0.88611, speed: 1.30 step/s\n",
      "global step 2900, epoch: 4, batch: 275, loss: 0.21736, accu: 0.88813, speed: 1.30 step/s\n",
      "eval loss: 1.10581, accu: 0.66150\n",
      "global step 2910, epoch: 4, batch: 285, loss: 0.65574, accu: 0.88750, speed: 0.09 step/s\n",
      "global step 2920, epoch: 4, batch: 295, loss: 0.25801, accu: 0.90312, speed: 1.29 step/s\n",
      "global step 2930, epoch: 4, batch: 305, loss: 1.05177, accu: 0.87083, speed: 1.29 step/s\n",
      "global step 2940, epoch: 4, batch: 315, loss: 0.38135, accu: 0.87344, speed: 1.30 step/s\n",
      "global step 2950, epoch: 4, batch: 325, loss: 0.07566, accu: 0.87250, speed: 1.29 step/s\n",
      "global step 2960, epoch: 4, batch: 335, loss: 0.59996, accu: 0.87396, speed: 1.29 step/s\n",
      "global step 2970, epoch: 4, batch: 345, loss: 0.32090, accu: 0.87679, speed: 1.30 step/s\n",
      "global step 2980, epoch: 4, batch: 355, loss: 0.23376, accu: 0.88203, speed: 1.30 step/s\n",
      "global step 2990, epoch: 4, batch: 365, loss: 0.09805, accu: 0.88333, speed: 1.29 step/s\n",
      "global step 3000, epoch: 4, batch: 375, loss: 0.39188, accu: 0.88250, speed: 1.30 step/s\n",
      "eval loss: 1.11797, accu: 0.66967\n",
      "global step 3010, epoch: 4, batch: 385, loss: 0.21079, accu: 0.87500, speed: 0.09 step/s\n",
      "global step 3020, epoch: 4, batch: 395, loss: 0.18955, accu: 0.87500, speed: 1.28 step/s\n",
      "global step 3030, epoch: 4, batch: 405, loss: 0.36101, accu: 0.86042, speed: 1.29 step/s\n",
      "global step 3040, epoch: 4, batch: 415, loss: 0.36223, accu: 0.86094, speed: 1.29 step/s\n",
      "global step 3050, epoch: 4, batch: 425, loss: 0.25920, accu: 0.86375, speed: 1.28 step/s\n",
      "global step 3060, epoch: 4, batch: 435, loss: 0.23567, accu: 0.85938, speed: 1.29 step/s\n",
      "global step 3070, epoch: 4, batch: 445, loss: 0.28458, accu: 0.86607, speed: 1.29 step/s\n",
      "global step 3080, epoch: 4, batch: 455, loss: 0.30800, accu: 0.86719, speed: 1.29 step/s\n",
      "global step 3090, epoch: 4, batch: 465, loss: 0.88456, accu: 0.86319, speed: 1.29 step/s\n",
      "global step 3100, epoch: 4, batch: 475, loss: 0.31616, accu: 0.85875, speed: 1.28 step/s\n",
      "eval loss: 1.11022, accu: 0.66583\n",
      "global step 3110, epoch: 4, batch: 485, loss: 0.40938, accu: 0.86875, speed: 0.09 step/s\n",
      "global step 3120, epoch: 4, batch: 495, loss: 0.25769, accu: 0.88125, speed: 1.27 step/s\n",
      "global step 3130, epoch: 4, batch: 505, loss: 0.34364, accu: 0.88542, speed: 1.30 step/s\n",
      "global step 3140, epoch: 4, batch: 515, loss: 0.20021, accu: 0.88750, speed: 1.30 step/s\n",
      "global step 3150, epoch: 4, batch: 525, loss: 0.13381, accu: 0.89000, speed: 1.30 step/s\n",
      "global step 3160, epoch: 4, batch: 535, loss: 0.32018, accu: 0.88750, speed: 1.29 step/s\n",
      "global step 3170, epoch: 4, batch: 545, loss: 0.74752, accu: 0.88125, speed: 1.30 step/s\n",
      "global step 3180, epoch: 4, batch: 555, loss: 0.73112, accu: 0.87578, speed: 1.30 step/s\n",
      "global step 3190, epoch: 4, batch: 565, loss: 0.68867, accu: 0.87431, speed: 1.30 step/s\n",
      "global step 3200, epoch: 4, batch: 575, loss: 0.62385, accu: 0.87313, speed: 1.30 step/s\n",
      "eval loss: 1.12869, accu: 0.65650\n",
      "global step 3210, epoch: 4, batch: 585, loss: 1.03041, accu: 0.87500, speed: 0.09 step/s\n",
      "global step 3220, epoch: 4, batch: 595, loss: 0.29906, accu: 0.86250, speed: 1.29 step/s\n",
      "global step 3230, epoch: 4, batch: 605, loss: 0.50069, accu: 0.87292, speed: 1.29 step/s\n",
      "global step 3240, epoch: 4, batch: 615, loss: 0.27048, accu: 0.87500, speed: 1.28 step/s\n",
      "global step 3250, epoch: 4, batch: 625, loss: 0.54478, accu: 0.87750, speed: 1.28 step/s\n",
      "global step 3260, epoch: 4, batch: 635, loss: 0.35874, accu: 0.86979, speed: 1.29 step/s\n",
      "global step 3270, epoch: 4, batch: 645, loss: 0.11164, accu: 0.87321, speed: 1.30 step/s\n",
      "global step 3280, epoch: 4, batch: 655, loss: 0.31162, accu: 0.87109, speed: 1.29 step/s\n",
      "global step 3290, epoch: 4, batch: 665, loss: 0.24018, accu: 0.87292, speed: 1.29 step/s\n",
      "global step 3300, epoch: 4, batch: 675, loss: 0.30074, accu: 0.87125, speed: 1.29 step/s\n",
      "eval loss: 1.14416, accu: 0.66033\n",
      "global step 3310, epoch: 4, batch: 685, loss: 0.19966, accu: 0.86250, speed: 0.09 step/s\n",
      "global step 3320, epoch: 4, batch: 695, loss: 0.32886, accu: 0.88750, speed: 1.30 step/s\n",
      "global step 3330, epoch: 4, batch: 705, loss: 0.37215, accu: 0.84375, speed: 1.29 step/s\n",
      "global step 3340, epoch: 4, batch: 715, loss: 0.45578, accu: 0.85156, speed: 1.29 step/s\n",
      "global step 3350, epoch: 4, batch: 725, loss: 0.72357, accu: 0.83875, speed: 1.29 step/s\n",
      "global step 3360, epoch: 4, batch: 735, loss: 0.42602, accu: 0.83958, speed: 1.29 step/s\n",
      "global step 3370, epoch: 4, batch: 745, loss: 0.21067, accu: 0.84554, speed: 1.30 step/s\n",
      "global step 3380, epoch: 4, batch: 755, loss: 0.45745, accu: 0.85000, speed: 1.27 step/s\n",
      "global step 3390, epoch: 4, batch: 765, loss: 0.12653, accu: 0.85556, speed: 1.30 step/s\n",
      "global step 3400, epoch: 4, batch: 775, loss: 0.51005, accu: 0.85813, speed: 1.29 step/s\n",
      "eval loss: 1.08069, accu: 0.67383\n",
      "global step 3410, epoch: 4, batch: 785, loss: 0.55305, accu: 0.86250, speed: 0.09 step/s\n",
      "global step 3420, epoch: 4, batch: 795, loss: 0.33821, accu: 0.88125, speed: 1.31 step/s\n",
      "global step 3430, epoch: 4, batch: 805, loss: 0.12713, accu: 0.88333, speed: 1.30 step/s\n",
      "global step 3440, epoch: 4, batch: 815, loss: 0.54537, accu: 0.87187, speed: 1.28 step/s\n",
      "global step 3450, epoch: 4, batch: 825, loss: 0.09291, accu: 0.87000, speed: 1.29 step/s\n",
      "global step 3460, epoch: 4, batch: 835, loss: 0.28448, accu: 0.86562, speed: 1.29 step/s\n",
      "global step 3470, epoch: 4, batch: 845, loss: 1.07302, accu: 0.86518, speed: 1.29 step/s\n",
      "global step 3480, epoch: 4, batch: 855, loss: 0.17716, accu: 0.86484, speed: 1.28 step/s\n",
      "global step 3490, epoch: 4, batch: 865, loss: 0.35799, accu: 0.86181, speed: 1.30 step/s\n",
      "global step 3500, epoch: 4, batch: 875, loss: 0.12432, accu: 0.86562, speed: 1.32 step/s\n",
      "eval loss: 1.11989, accu: 0.67100\n",
      "global step 3510, epoch: 5, batch: 10, loss: 0.22749, accu: 0.91875, speed: 0.09 step/s\n",
      "global step 3520, epoch: 5, batch: 20, loss: 0.22573, accu: 0.92812, speed: 1.30 step/s\n",
      "global step 3530, epoch: 5, batch: 30, loss: 0.26561, accu: 0.94167, speed: 1.29 step/s\n",
      "global step 3540, epoch: 5, batch: 40, loss: 0.05103, accu: 0.93750, speed: 1.29 step/s\n",
      "global step 3550, epoch: 5, batch: 50, loss: 0.06983, accu: 0.94375, speed: 1.29 step/s\n",
      "global step 3560, epoch: 5, batch: 60, loss: 0.23708, accu: 0.94271, speed: 1.29 step/s\n",
      "global step 3570, epoch: 5, batch: 70, loss: 0.04203, accu: 0.93929, speed: 1.29 step/s\n",
      "global step 3580, epoch: 5, batch: 80, loss: 0.31693, accu: 0.93984, speed: 1.28 step/s\n",
      "global step 3590, epoch: 5, batch: 90, loss: 0.39649, accu: 0.94097, speed: 1.29 step/s\n",
      "global step 3600, epoch: 5, batch: 100, loss: 0.34936, accu: 0.94125, speed: 1.29 step/s\n",
      "eval loss: 1.16461, accu: 0.66933\n",
      "global step 3610, epoch: 5, batch: 110, loss: 0.06598, accu: 0.93125, speed: 0.09 step/s\n",
      "global step 3620, epoch: 5, batch: 120, loss: 0.19311, accu: 0.92812, speed: 1.28 step/s\n",
      "global step 3630, epoch: 5, batch: 130, loss: 0.39735, accu: 0.93542, speed: 1.30 step/s\n",
      "global step 3640, epoch: 5, batch: 140, loss: 0.36609, accu: 0.93125, speed: 1.29 step/s\n",
      "global step 3650, epoch: 5, batch: 150, loss: 0.16363, accu: 0.93750, speed: 1.30 step/s\n",
      "global step 3660, epoch: 5, batch: 160, loss: 0.12780, accu: 0.94063, speed: 1.29 step/s\n",
      "global step 3670, epoch: 5, batch: 170, loss: 0.14913, accu: 0.93750, speed: 1.30 step/s\n",
      "global step 3680, epoch: 5, batch: 180, loss: 0.06628, accu: 0.94297, speed: 1.30 step/s\n",
      "global step 3690, epoch: 5, batch: 190, loss: 0.39622, accu: 0.94236, speed: 1.30 step/s\n",
      "global step 3700, epoch: 5, batch: 200, loss: 0.14381, accu: 0.94563, speed: 1.29 step/s\n",
      "eval loss: 1.23004, accu: 0.66650\n",
      "global step 3710, epoch: 5, batch: 210, loss: 0.22416, accu: 0.91875, speed: 0.09 step/s\n",
      "global step 3720, epoch: 5, batch: 220, loss: 0.05586, accu: 0.92500, speed: 1.30 step/s\n",
      "global step 3730, epoch: 5, batch: 230, loss: 0.32296, accu: 0.92708, speed: 1.29 step/s\n",
      "global step 3740, epoch: 5, batch: 240, loss: 0.12101, accu: 0.92969, speed: 1.29 step/s\n",
      "global step 3750, epoch: 5, batch: 250, loss: 0.25043, accu: 0.92625, speed: 1.29 step/s\n",
      "global step 3760, epoch: 5, batch: 260, loss: 0.33536, accu: 0.92604, speed: 1.29 step/s\n",
      "global step 3770, epoch: 5, batch: 270, loss: 0.92728, accu: 0.92857, speed: 1.28 step/s\n",
      "global step 3780, epoch: 5, batch: 280, loss: 0.49971, accu: 0.93047, speed: 1.29 step/s\n",
      "global step 3790, epoch: 5, batch: 290, loss: 0.15376, accu: 0.92986, speed: 1.28 step/s\n",
      "global step 3800, epoch: 5, batch: 300, loss: 0.26126, accu: 0.93188, speed: 1.29 step/s\n",
      "eval loss: 1.25691, accu: 0.66300\n",
      "global step 3810, epoch: 5, batch: 310, loss: 0.22085, accu: 0.92500, speed: 0.09 step/s\n",
      "global step 3820, epoch: 5, batch: 320, loss: 0.30702, accu: 0.93750, speed: 1.30 step/s\n",
      "global step 3830, epoch: 5, batch: 330, loss: 0.05102, accu: 0.93750, speed: 1.30 step/s\n",
      "global step 3840, epoch: 5, batch: 340, loss: 0.22237, accu: 0.92656, speed: 1.29 step/s\n",
      "global step 3850, epoch: 5, batch: 350, loss: 0.23255, accu: 0.92375, speed: 1.29 step/s\n",
      "global step 3860, epoch: 5, batch: 360, loss: 0.05623, accu: 0.92188, speed: 1.30 step/s\n",
      "global step 3870, epoch: 5, batch: 370, loss: 0.26853, accu: 0.92321, speed: 1.31 step/s\n",
      "global step 3880, epoch: 5, batch: 380, loss: 0.34422, accu: 0.92656, speed: 1.31 step/s\n",
      "global step 3890, epoch: 5, batch: 390, loss: 0.06648, accu: 0.92917, speed: 1.31 step/s\n",
      "global step 3900, epoch: 5, batch: 400, loss: 0.31466, accu: 0.93063, speed: 1.30 step/s\n",
      "eval loss: 1.31468, accu: 0.66767\n",
      "global step 3910, epoch: 5, batch: 410, loss: 0.25475, accu: 0.92500, speed: 0.09 step/s\n",
      "global step 3920, epoch: 5, batch: 420, loss: 0.08357, accu: 0.93750, speed: 1.29 step/s\n",
      "global step 3930, epoch: 5, batch: 430, loss: 0.05477, accu: 0.94167, speed: 1.29 step/s\n",
      "global step 3940, epoch: 5, batch: 440, loss: 0.21663, accu: 0.93281, speed: 1.29 step/s\n",
      "global step 3950, epoch: 5, batch: 450, loss: 0.14715, accu: 0.94000, speed: 1.29 step/s\n",
      "global step 3960, epoch: 5, batch: 460, loss: 0.64389, accu: 0.93021, speed: 1.29 step/s\n",
      "global step 3970, epoch: 5, batch: 470, loss: 0.30633, accu: 0.92321, speed: 1.29 step/s\n",
      "global step 3980, epoch: 5, batch: 480, loss: 0.35457, accu: 0.92344, speed: 1.29 step/s\n",
      "global step 3990, epoch: 5, batch: 490, loss: 0.06659, accu: 0.92847, speed: 1.29 step/s\n",
      "global step 4000, epoch: 5, batch: 500, loss: 0.24031, accu: 0.92563, speed: 1.29 step/s\n",
      "eval loss: 1.28306, accu: 0.66517\n",
      "global step 4010, epoch: 5, batch: 510, loss: 0.13923, accu: 0.96250, speed: 0.09 step/s\n",
      "global step 4020, epoch: 5, batch: 520, loss: 0.04529, accu: 0.94688, speed: 1.29 step/s\n",
      "global step 4030, epoch: 5, batch: 530, loss: 0.04204, accu: 0.94375, speed: 1.30 step/s\n",
      "global step 4040, epoch: 5, batch: 540, loss: 0.31101, accu: 0.94375, speed: 1.30 step/s\n",
      "global step 4050, epoch: 5, batch: 550, loss: 0.17448, accu: 0.94375, speed: 1.30 step/s\n",
      "global step 4060, epoch: 5, batch: 560, loss: 0.02027, accu: 0.94167, speed: 1.30 step/s\n",
      "global step 4070, epoch: 5, batch: 570, loss: 0.35544, accu: 0.94107, speed: 1.29 step/s\n",
      "global step 4080, epoch: 5, batch: 580, loss: 0.25592, accu: 0.94375, speed: 1.29 step/s\n",
      "global step 4090, epoch: 5, batch: 590, loss: 0.05904, accu: 0.94097, speed: 1.29 step/s\n",
      "global step 4100, epoch: 5, batch: 600, loss: 0.45806, accu: 0.94125, speed: 1.29 step/s\n",
      "eval loss: 1.30811, accu: 0.65867\n",
      "global step 4110, epoch: 5, batch: 610, loss: 0.22050, accu: 0.91875, speed: 0.09 step/s\n",
      "global step 4120, epoch: 5, batch: 620, loss: 0.06217, accu: 0.91563, speed: 1.28 step/s\n",
      "global step 4130, epoch: 5, batch: 630, loss: 0.57850, accu: 0.91250, speed: 1.28 step/s\n",
      "global step 4140, epoch: 5, batch: 640, loss: 0.13743, accu: 0.90938, speed: 1.29 step/s\n",
      "global step 4150, epoch: 5, batch: 650, loss: 0.44834, accu: 0.90875, speed: 1.29 step/s\n",
      "global step 4160, epoch: 5, batch: 660, loss: 0.38938, accu: 0.91146, speed: 1.29 step/s\n",
      "global step 4170, epoch: 5, batch: 670, loss: 0.19499, accu: 0.91607, speed: 1.29 step/s\n",
      "global step 4180, epoch: 5, batch: 680, loss: 0.16166, accu: 0.91641, speed: 1.29 step/s\n",
      "global step 4190, epoch: 5, batch: 690, loss: 0.19725, accu: 0.91736, speed: 1.28 step/s\n",
      "global step 4200, epoch: 5, batch: 700, loss: 0.19985, accu: 0.92000, speed: 1.29 step/s\n",
      "eval loss: 1.30119, accu: 0.65317\n",
      "global step 4210, epoch: 5, batch: 710, loss: 0.08525, accu: 0.89375, speed: 0.09 step/s\n",
      "global step 4220, epoch: 5, batch: 720, loss: 0.37346, accu: 0.89687, speed: 1.29 step/s\n",
      "global step 4230, epoch: 5, batch: 730, loss: 0.06628, accu: 0.89792, speed: 1.29 step/s\n",
      "global step 4240, epoch: 5, batch: 740, loss: 0.20393, accu: 0.89687, speed: 1.29 step/s\n",
      "global step 4250, epoch: 5, batch: 750, loss: 0.16789, accu: 0.89750, speed: 1.29 step/s\n",
      "global step 4260, epoch: 5, batch: 760, loss: 0.09958, accu: 0.90521, speed: 1.29 step/s\n",
      "global step 4270, epoch: 5, batch: 770, loss: 0.42701, accu: 0.90804, speed: 1.30 step/s\n",
      "global step 4280, epoch: 5, batch: 780, loss: 0.05834, accu: 0.91094, speed: 1.29 step/s\n",
      "global step 4290, epoch: 5, batch: 790, loss: 0.04953, accu: 0.91319, speed: 1.29 step/s\n",
      "global step 4300, epoch: 5, batch: 800, loss: 0.19228, accu: 0.91187, speed: 1.29 step/s\n",
      "eval loss: 1.30015, accu: 0.65267\n",
      "global step 4310, epoch: 5, batch: 810, loss: 0.78462, accu: 0.85625, speed: 0.09 step/s\n",
      "global step 4320, epoch: 5, batch: 820, loss: 0.34855, accu: 0.88125, speed: 1.30 step/s\n",
      "global step 4330, epoch: 5, batch: 830, loss: 0.87901, accu: 0.86875, speed: 1.29 step/s\n",
      "global step 4340, epoch: 5, batch: 840, loss: 0.41986, accu: 0.87500, speed: 1.29 step/s\n",
      "global step 4350, epoch: 5, batch: 850, loss: 0.40146, accu: 0.86875, speed: 1.30 step/s\n",
      "global step 4360, epoch: 5, batch: 860, loss: 0.11572, accu: 0.87500, speed: 1.30 step/s\n",
      "global step 4370, epoch: 5, batch: 870, loss: 0.14389, accu: 0.87857, speed: 1.30 step/s\n",
      "global step 4380, epoch: 6, batch: 5, loss: 0.20475, accu: 0.88672, speed: 1.30 step/s\n",
      "global step 4390, epoch: 6, batch: 15, loss: 0.29637, accu: 0.89444, speed: 1.29 step/s\n",
      "global step 4400, epoch: 6, batch: 25, loss: 0.08192, accu: 0.90125, speed: 1.29 step/s\n",
      "eval loss: 1.30241, accu: 0.65450\n",
      "global step 4410, epoch: 6, batch: 35, loss: 0.19497, accu: 0.94375, speed: 0.09 step/s\n",
      "global step 4420, epoch: 6, batch: 45, loss: 0.02123, accu: 0.95625, speed: 1.30 step/s\n",
      "global step 4430, epoch: 6, batch: 55, loss: 0.18812, accu: 0.96250, speed: 1.30 step/s\n",
      "global step 4440, epoch: 6, batch: 65, loss: 0.33213, accu: 0.96094, speed: 1.30 step/s\n",
      "global step 4450, epoch: 6, batch: 75, loss: 0.02875, accu: 0.96250, speed: 1.29 step/s\n",
      "global step 4460, epoch: 6, batch: 85, loss: 0.10802, accu: 0.96458, speed: 1.29 step/s\n",
      "global step 4470, epoch: 6, batch: 95, loss: 0.11726, accu: 0.96429, speed: 1.29 step/s\n",
      "global step 4480, epoch: 6, batch: 105, loss: 0.03113, accu: 0.96250, speed: 1.30 step/s\n",
      "global step 4490, epoch: 6, batch: 115, loss: 0.09556, accu: 0.96319, speed: 1.30 step/s\n",
      "global step 4500, epoch: 6, batch: 125, loss: 0.10420, accu: 0.96375, speed: 1.30 step/s\n",
      "eval loss: 1.35796, accu: 0.66367\n",
      "global step 4510, epoch: 6, batch: 135, loss: 0.09041, accu: 0.97500, speed: 0.09 step/s\n",
      "global step 4520, epoch: 6, batch: 145, loss: 0.17960, accu: 0.97500, speed: 1.30 step/s\n",
      "global step 4530, epoch: 6, batch: 155, loss: 0.38322, accu: 0.96875, speed: 1.30 step/s\n",
      "global step 4540, epoch: 6, batch: 165, loss: 0.25978, accu: 0.97031, speed: 1.30 step/s\n",
      "global step 4550, epoch: 6, batch: 175, loss: 0.22577, accu: 0.95875, speed: 1.29 step/s\n",
      "global step 4560, epoch: 6, batch: 185, loss: 0.21510, accu: 0.95625, speed: 1.29 step/s\n",
      "global step 4570, epoch: 6, batch: 195, loss: 0.09597, accu: 0.95268, speed: 1.30 step/s\n",
      "global step 4580, epoch: 6, batch: 205, loss: 0.01744, accu: 0.95547, speed: 1.25 step/s\n",
      "global step 4590, epoch: 6, batch: 215, loss: 0.37808, accu: 0.95347, speed: 1.29 step/s\n",
      "global step 4600, epoch: 6, batch: 225, loss: 0.04713, accu: 0.95625, speed: 1.29 step/s\n",
      "eval loss: 1.39774, accu: 0.65933\n",
      "global step 4610, epoch: 6, batch: 235, loss: 0.03384, accu: 0.95625, speed: 0.09 step/s\n",
      "global step 4620, epoch: 6, batch: 245, loss: 0.36106, accu: 0.95312, speed: 1.29 step/s\n",
      "global step 4630, epoch: 6, batch: 255, loss: 0.01931, accu: 0.96875, speed: 1.29 step/s\n",
      "global step 4640, epoch: 6, batch: 265, loss: 0.01752, accu: 0.96406, speed: 1.29 step/s\n",
      "global step 4650, epoch: 6, batch: 275, loss: 0.05165, accu: 0.96750, speed: 1.29 step/s\n",
      "global step 4660, epoch: 6, batch: 285, loss: 0.24048, accu: 0.96354, speed: 1.29 step/s\n",
      "global step 4670, epoch: 6, batch: 295, loss: 0.03204, accu: 0.96518, speed: 1.29 step/s\n",
      "global step 4680, epoch: 6, batch: 305, loss: 0.68957, accu: 0.95937, speed: 1.28 step/s\n",
      "global step 4690, epoch: 6, batch: 315, loss: 0.34432, accu: 0.95833, speed: 1.29 step/s\n",
      "global step 4700, epoch: 6, batch: 325, loss: 0.03090, accu: 0.95750, speed: 1.28 step/s\n",
      "eval loss: 1.43443, accu: 0.65317\n",
      "global step 4710, epoch: 6, batch: 335, loss: 0.08595, accu: 0.96875, speed: 0.09 step/s\n",
      "global step 4720, epoch: 6, batch: 345, loss: 0.05550, accu: 0.95625, speed: 1.29 step/s\n",
      "global step 4730, epoch: 6, batch: 355, loss: 0.14335, accu: 0.95208, speed: 1.29 step/s\n",
      "global step 4740, epoch: 6, batch: 365, loss: 0.26190, accu: 0.95469, speed: 1.28 step/s\n",
      "global step 4750, epoch: 6, batch: 375, loss: 0.07603, accu: 0.95125, speed: 1.28 step/s\n",
      "global step 4760, epoch: 6, batch: 385, loss: 0.07386, accu: 0.95104, speed: 1.28 step/s\n",
      "global step 4770, epoch: 6, batch: 395, loss: 0.05964, accu: 0.94821, speed: 1.29 step/s\n",
      "global step 4780, epoch: 6, batch: 405, loss: 0.13349, accu: 0.94844, speed: 1.29 step/s\n",
      "global step 4790, epoch: 6, batch: 415, loss: 0.05372, accu: 0.95000, speed: 1.28 step/s\n",
      "global step 4800, epoch: 6, batch: 425, loss: 0.50492, accu: 0.95000, speed: 1.28 step/s\n",
      "eval loss: 1.45279, accu: 0.65650\n",
      "global step 4810, epoch: 6, batch: 435, loss: 0.29179, accu: 0.97500, speed: 0.09 step/s\n",
      "global step 4820, epoch: 6, batch: 445, loss: 0.11605, accu: 0.95937, speed: 1.30 step/s\n",
      "global step 4830, epoch: 6, batch: 455, loss: 0.08469, accu: 0.96042, speed: 1.30 step/s\n",
      "global step 4840, epoch: 6, batch: 465, loss: 0.25496, accu: 0.96094, speed: 1.31 step/s\n",
      "global step 4850, epoch: 6, batch: 475, loss: 0.06225, accu: 0.96375, speed: 1.31 step/s\n",
      "global step 4860, epoch: 6, batch: 485, loss: 0.06493, accu: 0.95937, speed: 1.31 step/s\n",
      "global step 4870, epoch: 6, batch: 495, loss: 0.07924, accu: 0.96071, speed: 1.32 step/s\n",
      "global step 4880, epoch: 6, batch: 505, loss: 0.44694, accu: 0.95937, speed: 1.30 step/s\n",
      "global step 4890, epoch: 6, batch: 515, loss: 0.02316, accu: 0.96042, speed: 1.31 step/s\n",
      "global step 4900, epoch: 6, batch: 525, loss: 0.10371, accu: 0.96000, speed: 1.29 step/s\n",
      "eval loss: 1.42488, accu: 0.65967\n",
      "global step 4910, epoch: 6, batch: 535, loss: 0.13951, accu: 0.98125, speed: 0.09 step/s\n",
      "global step 4920, epoch: 6, batch: 545, loss: 0.06688, accu: 0.96562, speed: 1.30 step/s\n",
      "global step 4930, epoch: 6, batch: 555, loss: 0.15109, accu: 0.95833, speed: 1.29 step/s\n",
      "global step 4940, epoch: 6, batch: 565, loss: 0.22106, accu: 0.96094, speed: 1.30 step/s\n",
      "global step 4950, epoch: 6, batch: 575, loss: 0.08563, accu: 0.96000, speed: 1.29 step/s\n",
      "global step 4960, epoch: 6, batch: 585, loss: 0.29037, accu: 0.96146, speed: 1.30 step/s\n",
      "global step 4970, epoch: 6, batch: 595, loss: 0.05624, accu: 0.96250, speed: 1.29 step/s\n",
      "global step 4980, epoch: 6, batch: 605, loss: 0.30966, accu: 0.96094, speed: 1.29 step/s\n",
      "global step 4990, epoch: 6, batch: 615, loss: 0.13196, accu: 0.96042, speed: 1.29 step/s\n",
      "global step 5000, epoch: 6, batch: 625, loss: 0.21235, accu: 0.95688, speed: 1.30 step/s\n",
      "eval loss: 1.40374, accu: 0.66200\n",
      "global step 5010, epoch: 6, batch: 635, loss: 0.23641, accu: 0.95625, speed: 0.09 step/s\n",
      "global step 5020, epoch: 6, batch: 645, loss: 0.21905, accu: 0.95000, speed: 1.29 step/s\n",
      "global step 5030, epoch: 6, batch: 655, loss: 0.03122, accu: 0.94583, speed: 1.30 step/s\n",
      "global step 5040, epoch: 6, batch: 665, loss: 0.24848, accu: 0.94219, speed: 1.29 step/s\n",
      "global step 5050, epoch: 6, batch: 675, loss: 0.03981, accu: 0.94750, speed: 1.28 step/s\n",
      "global step 5060, epoch: 6, batch: 685, loss: 0.32913, accu: 0.94792, speed: 1.29 step/s\n",
      "global step 5070, epoch: 6, batch: 695, loss: 0.35566, accu: 0.94911, speed: 1.29 step/s\n",
      "global step 5080, epoch: 6, batch: 705, loss: 0.10372, accu: 0.94922, speed: 1.29 step/s\n",
      "global step 5090, epoch: 6, batch: 715, loss: 0.37000, accu: 0.94792, speed: 1.30 step/s\n",
      "global step 5100, epoch: 6, batch: 725, loss: 0.04812, accu: 0.94750, speed: 1.30 step/s\n",
      "eval loss: 1.39116, accu: 0.66800\n",
      "global step 5110, epoch: 6, batch: 735, loss: 0.29141, accu: 0.93125, speed: 0.09 step/s\n",
      "global step 5120, epoch: 6, batch: 745, loss: 0.20167, accu: 0.94375, speed: 1.29 step/s\n",
      "global step 5130, epoch: 6, batch: 755, loss: 0.35255, accu: 0.94167, speed: 1.29 step/s\n",
      "global step 5140, epoch: 6, batch: 765, loss: 0.07781, accu: 0.94375, speed: 1.29 step/s\n",
      "global step 5150, epoch: 6, batch: 775, loss: 0.20819, accu: 0.94125, speed: 1.29 step/s\n",
      "global step 5160, epoch: 6, batch: 785, loss: 0.20687, accu: 0.93958, speed: 1.28 step/s\n",
      "global step 5170, epoch: 6, batch: 795, loss: 0.14002, accu: 0.94018, speed: 1.28 step/s\n",
      "global step 5180, epoch: 6, batch: 805, loss: 0.41789, accu: 0.93906, speed: 1.29 step/s\n",
      "global step 5190, epoch: 6, batch: 815, loss: 0.15732, accu: 0.93958, speed: 1.29 step/s\n",
      "global step 5200, epoch: 6, batch: 825, loss: 0.03401, accu: 0.94063, speed: 1.29 step/s\n",
      "eval loss: 1.36311, accu: 0.66533\n",
      "global step 5210, epoch: 6, batch: 835, loss: 0.07738, accu: 0.94375, speed: 0.09 step/s\n",
      "global step 5220, epoch: 6, batch: 845, loss: 0.09544, accu: 0.93437, speed: 1.29 step/s\n",
      "global step 5230, epoch: 6, batch: 855, loss: 0.03004, accu: 0.93958, speed: 1.29 step/s\n",
      "global step 5240, epoch: 6, batch: 865, loss: 0.46687, accu: 0.94063, speed: 1.30 step/s\n",
      "global step 5250, epoch: 6, batch: 875, loss: 0.36950, accu: 0.94500, speed: 1.32 step/s\n",
      "global step 5260, epoch: 7, batch: 10, loss: 0.03960, accu: 0.94896, speed: 1.28 step/s\n",
      "global step 5270, epoch: 7, batch: 20, loss: 0.06699, accu: 0.95446, speed: 1.30 step/s\n",
      "global step 5280, epoch: 7, batch: 30, loss: 0.05161, accu: 0.95859, speed: 1.30 step/s\n",
      "global step 5290, epoch: 7, batch: 40, loss: 0.01220, accu: 0.96111, speed: 1.29 step/s\n",
      "global step 5300, epoch: 7, batch: 50, loss: 0.02611, accu: 0.96188, speed: 1.29 step/s\n",
      "eval loss: 1.47975, accu: 0.66367\n",
      "global step 5310, epoch: 7, batch: 60, loss: 0.04751, accu: 0.98125, speed: 0.09 step/s\n",
      "global step 5320, epoch: 7, batch: 70, loss: 0.04585, accu: 0.98750, speed: 1.29 step/s\n",
      "global step 5330, epoch: 7, batch: 80, loss: 0.00512, accu: 0.98542, speed: 1.30 step/s\n",
      "global step 5340, epoch: 7, batch: 90, loss: 0.08038, accu: 0.98281, speed: 1.29 step/s\n",
      "global step 5350, epoch: 7, batch: 100, loss: 0.01567, accu: 0.98250, speed: 1.29 step/s\n",
      "global step 5360, epoch: 7, batch: 110, loss: 0.41721, accu: 0.97813, speed: 1.29 step/s\n",
      "global step 5370, epoch: 7, batch: 120, loss: 0.01343, accu: 0.98036, speed: 1.30 step/s\n",
      "global step 5380, epoch: 7, batch: 130, loss: 0.03416, accu: 0.98125, speed: 1.29 step/s\n",
      "global step 5390, epoch: 7, batch: 140, loss: 0.06156, accu: 0.98194, speed: 1.28 step/s\n",
      "global step 5400, epoch: 7, batch: 150, loss: 0.01859, accu: 0.98125, speed: 1.28 step/s\n",
      "eval loss: 1.52358, accu: 0.66717\n",
      "global step 5410, epoch: 7, batch: 160, loss: 0.14602, accu: 0.98125, speed: 0.09 step/s\n",
      "global step 5420, epoch: 7, batch: 170, loss: 0.04559, accu: 0.98125, speed: 1.28 step/s\n",
      "global step 5430, epoch: 7, batch: 180, loss: 0.01138, accu: 0.97708, speed: 1.30 step/s\n",
      "global step 5440, epoch: 7, batch: 190, loss: 0.01891, accu: 0.98125, speed: 1.29 step/s\n",
      "global step 5450, epoch: 7, batch: 200, loss: 0.04623, accu: 0.98250, speed: 1.28 step/s\n",
      "global step 5460, epoch: 7, batch: 210, loss: 0.01194, accu: 0.98438, speed: 1.29 step/s\n",
      "global step 5470, epoch: 7, batch: 220, loss: 0.02371, accu: 0.98571, speed: 1.30 step/s\n",
      "global step 5480, epoch: 7, batch: 230, loss: 0.33080, accu: 0.98438, speed: 1.28 step/s\n",
      "global step 5490, epoch: 7, batch: 240, loss: 0.28207, accu: 0.98542, speed: 1.30 step/s\n",
      "global step 5500, epoch: 7, batch: 250, loss: 0.05233, accu: 0.98375, speed: 1.29 step/s\n",
      "eval loss: 1.60075, accu: 0.65217\n",
      "global step 5510, epoch: 7, batch: 260, loss: 0.05709, accu: 0.96875, speed: 0.09 step/s\n",
      "global step 5520, epoch: 7, batch: 270, loss: 0.05067, accu: 0.97813, speed: 1.29 step/s\n",
      "global step 5530, epoch: 7, batch: 280, loss: 0.16187, accu: 0.97500, speed: 1.27 step/s\n",
      "global step 5540, epoch: 7, batch: 290, loss: 0.01431, accu: 0.97656, speed: 1.29 step/s\n",
      "global step 5550, epoch: 7, batch: 300, loss: 0.23642, accu: 0.97375, speed: 1.29 step/s\n",
      "global step 5560, epoch: 7, batch: 310, loss: 0.02158, accu: 0.97292, speed: 1.28 step/s\n",
      "global step 5570, epoch: 7, batch: 320, loss: 0.07255, accu: 0.97232, speed: 1.29 step/s\n",
      "global step 5580, epoch: 7, batch: 330, loss: 0.03914, accu: 0.97422, speed: 1.29 step/s\n",
      "global step 5590, epoch: 7, batch: 340, loss: 0.01198, accu: 0.97569, speed: 1.28 step/s\n",
      "global step 5600, epoch: 7, batch: 350, loss: 0.01308, accu: 0.97437, speed: 1.29 step/s\n",
      "eval loss: 1.66213, accu: 0.64200\n",
      "global step 5610, epoch: 7, batch: 360, loss: 0.11910, accu: 0.96875, speed: 0.09 step/s\n",
      "global step 5620, epoch: 7, batch: 370, loss: 0.35739, accu: 0.96875, speed: 1.29 step/s\n",
      "global step 5630, epoch: 7, batch: 380, loss: 0.05504, accu: 0.96875, speed: 1.29 step/s\n",
      "global step 5640, epoch: 7, batch: 390, loss: 0.05194, accu: 0.95937, speed: 1.28 step/s\n",
      "global step 5650, epoch: 7, batch: 400, loss: 0.12032, accu: 0.95750, speed: 1.29 step/s\n",
      "global step 5660, epoch: 7, batch: 410, loss: 0.13315, accu: 0.96146, speed: 1.30 step/s\n",
      "global step 5670, epoch: 7, batch: 420, loss: 0.11347, accu: 0.95982, speed: 1.29 step/s\n",
      "global step 5680, epoch: 7, batch: 430, loss: 0.04902, accu: 0.96016, speed: 1.29 step/s\n",
      "global step 5690, epoch: 7, batch: 440, loss: 0.10981, accu: 0.95903, speed: 1.29 step/s\n",
      "global step 5700, epoch: 7, batch: 450, loss: 0.07135, accu: 0.95937, speed: 1.29 step/s\n",
      "eval loss: 1.55893, accu: 0.65917\n",
      "global step 5710, epoch: 7, batch: 460, loss: 0.26036, accu: 0.93125, speed: 0.09 step/s\n",
      "global step 5720, epoch: 7, batch: 470, loss: 0.05307, accu: 0.95625, speed: 1.29 step/s\n",
      "global step 5730, epoch: 7, batch: 480, loss: 0.07667, accu: 0.96042, speed: 1.29 step/s\n",
      "global step 5740, epoch: 7, batch: 490, loss: 0.13595, accu: 0.96250, speed: 1.29 step/s\n",
      "global step 5750, epoch: 7, batch: 500, loss: 0.02190, accu: 0.96500, speed: 1.28 step/s\n",
      "global step 5760, epoch: 7, batch: 510, loss: 0.15910, accu: 0.96667, speed: 1.29 step/s\n",
      "global step 5770, epoch: 7, batch: 520, loss: 0.06297, accu: 0.97143, speed: 1.29 step/s\n",
      "global step 5780, epoch: 7, batch: 530, loss: 0.15740, accu: 0.96875, speed: 1.29 step/s\n",
      "global step 5790, epoch: 7, batch: 540, loss: 0.18900, accu: 0.97014, speed: 1.29 step/s\n",
      "global step 5800, epoch: 7, batch: 550, loss: 0.01550, accu: 0.96937, speed: 1.30 step/s\n",
      "eval loss: 1.56247, accu: 0.65317\n",
      "global step 5810, epoch: 7, batch: 560, loss: 0.16704, accu: 0.95000, speed: 0.09 step/s\n",
      "global step 5820, epoch: 7, batch: 570, loss: 0.10333, accu: 0.96250, speed: 1.29 step/s\n",
      "global step 5830, epoch: 7, batch: 580, loss: 0.35534, accu: 0.95833, speed: 1.30 step/s\n",
      "global step 5840, epoch: 7, batch: 590, loss: 0.15204, accu: 0.95781, speed: 1.29 step/s\n",
      "global step 5850, epoch: 7, batch: 600, loss: 0.10844, accu: 0.95875, speed: 1.30 step/s\n",
      "global step 5860, epoch: 7, batch: 610, loss: 0.02502, accu: 0.95937, speed: 1.30 step/s\n",
      "global step 5870, epoch: 7, batch: 620, loss: 0.09753, accu: 0.96161, speed: 1.29 step/s\n",
      "global step 5880, epoch: 7, batch: 630, loss: 0.01549, accu: 0.96250, speed: 1.30 step/s\n",
      "global step 5890, epoch: 7, batch: 640, loss: 0.09216, accu: 0.96250, speed: 1.30 step/s\n",
      "global step 5900, epoch: 7, batch: 650, loss: 0.07785, accu: 0.96062, speed: 1.30 step/s\n",
      "eval loss: 1.61510, accu: 0.64950\n",
      "global step 5910, epoch: 7, batch: 660, loss: 0.11420, accu: 0.95625, speed: 0.09 step/s\n",
      "global step 5920, epoch: 7, batch: 670, loss: 0.04592, accu: 0.94063, speed: 1.30 step/s\n",
      "global step 5930, epoch: 7, batch: 680, loss: 0.01348, accu: 0.94583, speed: 1.29 step/s\n",
      "global step 5940, epoch: 7, batch: 690, loss: 0.14649, accu: 0.95469, speed: 1.29 step/s\n",
      "global step 5950, epoch: 7, batch: 700, loss: 0.28947, accu: 0.94500, speed: 1.28 step/s\n",
      "global step 5960, epoch: 7, batch: 710, loss: 0.04020, accu: 0.95208, speed: 1.28 step/s\n",
      "global step 5970, epoch: 7, batch: 720, loss: 0.09828, accu: 0.95357, speed: 1.29 step/s\n",
      "global step 5980, epoch: 7, batch: 730, loss: 0.22264, accu: 0.95625, speed: 1.28 step/s\n",
      "global step 5990, epoch: 7, batch: 740, loss: 0.14999, accu: 0.96042, speed: 1.28 step/s\n",
      "global step 6000, epoch: 7, batch: 750, loss: 0.40663, accu: 0.95937, speed: 1.28 step/s\n",
      "eval loss: 1.59507, accu: 0.65300\n",
      "global step 6010, epoch: 7, batch: 760, loss: 0.15242, accu: 0.95000, speed: 0.09 step/s\n",
      "global step 6020, epoch: 7, batch: 770, loss: 0.12617, accu: 0.95625, speed: 1.24 step/s\n",
      "global step 6030, epoch: 7, batch: 780, loss: 0.02828, accu: 0.95625, speed: 1.29 step/s\n",
      "global step 6040, epoch: 7, batch: 790, loss: 0.04237, accu: 0.96250, speed: 1.30 step/s\n",
      "global step 6050, epoch: 7, batch: 800, loss: 0.18547, accu: 0.96375, speed: 1.28 step/s\n",
      "global step 6060, epoch: 7, batch: 810, loss: 0.04962, accu: 0.96146, speed: 1.29 step/s\n",
      "global step 6070, epoch: 7, batch: 820, loss: 0.26962, accu: 0.96071, speed: 1.30 step/s\n",
      "global step 6080, epoch: 7, batch: 830, loss: 0.17744, accu: 0.95937, speed: 1.30 step/s\n",
      "global step 6090, epoch: 7, batch: 840, loss: 0.07977, accu: 0.96319, speed: 1.30 step/s\n",
      "global step 6100, epoch: 7, batch: 850, loss: 0.01159, accu: 0.96375, speed: 1.29 step/s\n",
      "eval loss: 1.53694, accu: 0.66467\n",
      "global step 6110, epoch: 7, batch: 860, loss: 0.08060, accu: 0.96250, speed: 0.09 step/s\n",
      "global step 6120, epoch: 7, batch: 870, loss: 0.30551, accu: 0.95937, speed: 1.28 step/s\n",
      "global step 6130, epoch: 8, batch: 5, loss: 0.06633, accu: 0.97083, speed: 1.29 step/s\n",
      "global step 6140, epoch: 8, batch: 15, loss: 0.00548, accu: 0.97188, speed: 1.29 step/s\n",
      "global step 6150, epoch: 8, batch: 25, loss: 0.02426, accu: 0.96500, speed: 1.29 step/s\n",
      "global step 6160, epoch: 8, batch: 35, loss: 0.02185, accu: 0.96875, speed: 1.29 step/s\n",
      "global step 6170, epoch: 8, batch: 45, loss: 0.03376, accu: 0.97143, speed: 1.29 step/s\n",
      "global step 6180, epoch: 8, batch: 55, loss: 0.01200, accu: 0.97344, speed: 1.29 step/s\n",
      "global step 6190, epoch: 8, batch: 65, loss: 0.03650, accu: 0.97431, speed: 1.29 step/s\n",
      "global step 6200, epoch: 8, batch: 75, loss: 0.02893, accu: 0.97500, speed: 1.29 step/s\n",
      "eval loss: 1.57502, accu: 0.65967\n",
      "global step 6210, epoch: 8, batch: 85, loss: 0.05142, accu: 0.98125, speed: 0.09 step/s\n",
      "global step 6220, epoch: 8, batch: 95, loss: 0.01540, accu: 0.97500, speed: 1.29 step/s\n",
      "global step 6230, epoch: 8, batch: 105, loss: 0.08014, accu: 0.97708, speed: 1.30 step/s\n",
      "global step 6240, epoch: 8, batch: 115, loss: 0.06285, accu: 0.97500, speed: 1.29 step/s\n",
      "global step 6250, epoch: 8, batch: 125, loss: 0.07246, accu: 0.97750, speed: 1.29 step/s\n",
      "global step 6260, epoch: 8, batch: 135, loss: 0.05001, accu: 0.97813, speed: 1.30 step/s\n",
      "global step 6270, epoch: 8, batch: 145, loss: 0.01009, accu: 0.98036, speed: 1.30 step/s\n",
      "global step 6280, epoch: 8, batch: 155, loss: 0.00909, accu: 0.97891, speed: 1.30 step/s\n",
      "global step 6290, epoch: 8, batch: 165, loss: 0.00810, accu: 0.98056, speed: 1.29 step/s\n",
      "global step 6300, epoch: 8, batch: 175, loss: 0.00652, accu: 0.98062, speed: 1.30 step/s\n",
      "eval loss: 1.61728, accu: 0.66050\n",
      "global step 6310, epoch: 8, batch: 185, loss: 0.20999, accu: 0.98750, speed: 0.09 step/s\n",
      "global step 6320, epoch: 8, batch: 195, loss: 0.00502, accu: 0.99062, speed: 1.29 step/s\n",
      "global step 6330, epoch: 8, batch: 205, loss: 0.01117, accu: 0.98958, speed: 1.29 step/s\n",
      "global step 6340, epoch: 8, batch: 215, loss: 0.00898, accu: 0.99219, speed: 1.30 step/s\n",
      "global step 6350, epoch: 8, batch: 225, loss: 0.15310, accu: 0.99000, speed: 1.29 step/s\n",
      "global step 6360, epoch: 8, batch: 235, loss: 0.03749, accu: 0.99062, speed: 1.29 step/s\n",
      "global step 6370, epoch: 8, batch: 245, loss: 0.01537, accu: 0.99107, speed: 1.29 step/s\n",
      "global step 6380, epoch: 8, batch: 255, loss: 0.01275, accu: 0.98984, speed: 1.29 step/s\n",
      "global step 6390, epoch: 8, batch: 265, loss: 0.03982, accu: 0.98958, speed: 1.29 step/s\n",
      "global step 6400, epoch: 8, batch: 275, loss: 0.03735, accu: 0.98875, speed: 1.29 step/s\n",
      "eval loss: 1.69880, accu: 0.66233\n",
      "global step 6410, epoch: 8, batch: 285, loss: 0.03433, accu: 0.97500, speed: 0.09 step/s\n",
      "global step 6420, epoch: 8, batch: 295, loss: 0.00995, accu: 0.98125, speed: 1.30 step/s\n",
      "global step 6430, epoch: 8, batch: 305, loss: 0.18224, accu: 0.98333, speed: 1.30 step/s\n",
      "global step 6440, epoch: 8, batch: 315, loss: 0.09807, accu: 0.98281, speed: 1.29 step/s\n",
      "global step 6450, epoch: 8, batch: 325, loss: 0.00789, accu: 0.98250, speed: 1.29 step/s\n",
      "global step 6460, epoch: 8, batch: 335, loss: 0.08976, accu: 0.98333, speed: 1.31 step/s\n",
      "global step 6470, epoch: 8, batch: 345, loss: 0.00753, accu: 0.98393, speed: 1.30 step/s\n",
      "global step 6480, epoch: 8, batch: 355, loss: 0.17114, accu: 0.98203, speed: 1.30 step/s\n",
      "global step 6490, epoch: 8, batch: 365, loss: 0.03329, accu: 0.98264, speed: 1.29 step/s\n",
      "global step 6500, epoch: 8, batch: 375, loss: 0.03403, accu: 0.98313, speed: 1.29 step/s\n",
      "eval loss: 1.69419, accu: 0.66900\n",
      "global step 6510, epoch: 8, batch: 385, loss: 0.00243, accu: 0.96875, speed: 0.09 step/s\n",
      "global step 6520, epoch: 8, batch: 395, loss: 0.06205, accu: 0.96562, speed: 1.29 step/s\n",
      "global step 6530, epoch: 8, batch: 405, loss: 0.02774, accu: 0.96667, speed: 1.29 step/s\n",
      "global step 6540, epoch: 8, batch: 415, loss: 0.14969, accu: 0.97031, speed: 1.29 step/s\n",
      "global step 6550, epoch: 8, batch: 425, loss: 0.01728, accu: 0.97250, speed: 1.29 step/s\n",
      "global step 6560, epoch: 8, batch: 435, loss: 0.05296, accu: 0.97604, speed: 1.30 step/s\n",
      "global step 6570, epoch: 8, batch: 445, loss: 0.04596, accu: 0.97679, speed: 1.30 step/s\n",
      "global step 6580, epoch: 8, batch: 455, loss: 0.02311, accu: 0.97500, speed: 1.30 step/s\n",
      "global step 6590, epoch: 8, batch: 465, loss: 0.05355, accu: 0.97361, speed: 1.29 step/s\n",
      "global step 6600, epoch: 8, batch: 475, loss: 0.20999, accu: 0.97313, speed: 1.30 step/s\n",
      "eval loss: 1.69350, accu: 0.66800\n",
      "global step 6610, epoch: 8, batch: 485, loss: 0.21387, accu: 0.95625, speed: 0.09 step/s\n",
      "global step 6620, epoch: 8, batch: 495, loss: 0.11554, accu: 0.95625, speed: 1.29 step/s\n",
      "global step 6630, epoch: 8, batch: 505, loss: 0.05069, accu: 0.95833, speed: 1.29 step/s\n",
      "global step 6640, epoch: 8, batch: 515, loss: 0.04243, accu: 0.95937, speed: 1.29 step/s\n",
      "global step 6650, epoch: 8, batch: 525, loss: 0.04822, accu: 0.95875, speed: 1.29 step/s\n",
      "global step 6660, epoch: 8, batch: 535, loss: 0.23810, accu: 0.95729, speed: 1.29 step/s\n",
      "global step 6670, epoch: 8, batch: 545, loss: 0.02405, accu: 0.95893, speed: 1.29 step/s\n",
      "global step 6680, epoch: 8, batch: 555, loss: 0.04241, accu: 0.95937, speed: 1.29 step/s\n",
      "global step 6690, epoch: 8, batch: 565, loss: 0.01420, accu: 0.96181, speed: 1.28 step/s\n",
      "global step 6700, epoch: 8, batch: 575, loss: 0.24212, accu: 0.96250, speed: 1.29 step/s\n",
      "eval loss: 1.70994, accu: 0.66017\n",
      "global step 6710, epoch: 8, batch: 585, loss: 0.01725, accu: 0.94375, speed: 0.09 step/s\n",
      "global step 6720, epoch: 8, batch: 595, loss: 0.20705, accu: 0.95625, speed: 1.29 step/s\n",
      "global step 6730, epoch: 8, batch: 605, loss: 0.02924, accu: 0.95833, speed: 1.30 step/s\n",
      "global step 6740, epoch: 8, batch: 615, loss: 0.03626, accu: 0.96719, speed: 1.28 step/s\n",
      "global step 6750, epoch: 8, batch: 625, loss: 0.08239, accu: 0.96500, speed: 1.29 step/s\n",
      "global step 6760, epoch: 8, batch: 635, loss: 0.00489, accu: 0.96250, speed: 1.30 step/s\n",
      "global step 6770, epoch: 8, batch: 645, loss: 0.00721, accu: 0.96429, speed: 1.29 step/s\n",
      "global step 6780, epoch: 8, batch: 655, loss: 0.04343, accu: 0.96172, speed: 1.28 step/s\n",
      "global step 6790, epoch: 8, batch: 665, loss: 0.06070, accu: 0.96111, speed: 1.30 step/s\n",
      "global step 6800, epoch: 8, batch: 675, loss: 0.07269, accu: 0.95937, speed: 1.28 step/s\n",
      "eval loss: 1.65786, accu: 0.65733\n",
      "global step 6810, epoch: 8, batch: 685, loss: 0.06696, accu: 0.96875, speed: 0.09 step/s\n",
      "global step 6820, epoch: 8, batch: 695, loss: 0.03219, accu: 0.96250, speed: 1.29 step/s\n",
      "global step 6830, epoch: 8, batch: 705, loss: 0.21833, accu: 0.95833, speed: 1.28 step/s\n",
      "global step 6840, epoch: 8, batch: 715, loss: 0.04404, accu: 0.96094, speed: 1.28 step/s\n",
      "global step 6850, epoch: 8, batch: 725, loss: 0.03228, accu: 0.96500, speed: 1.27 step/s\n",
      "global step 6860, epoch: 8, batch: 735, loss: 0.00780, accu: 0.97083, speed: 1.30 step/s\n",
      "global step 6870, epoch: 8, batch: 745, loss: 0.02022, accu: 0.97232, speed: 1.27 step/s\n",
      "global step 6880, epoch: 8, batch: 755, loss: 0.16535, accu: 0.96875, speed: 1.28 step/s\n",
      "global step 6890, epoch: 8, batch: 765, loss: 0.47803, accu: 0.96389, speed: 1.30 step/s\n",
      "global step 6900, epoch: 8, batch: 775, loss: 0.27017, accu: 0.96188, speed: 1.28 step/s\n",
      "eval loss: 1.69532, accu: 0.66300\n",
      "global step 6910, epoch: 8, batch: 785, loss: 0.09631, accu: 0.98125, speed: 0.09 step/s\n",
      "global step 6920, epoch: 8, batch: 795, loss: 0.03576, accu: 0.97188, speed: 1.29 step/s\n",
      "global step 6930, epoch: 8, batch: 805, loss: 0.21242, accu: 0.97292, speed: 1.30 step/s\n",
      "global step 6940, epoch: 8, batch: 815, loss: 0.01465, accu: 0.97656, speed: 1.30 step/s\n",
      "global step 6950, epoch: 8, batch: 825, loss: 0.01670, accu: 0.97875, speed: 1.30 step/s\n",
      "global step 6960, epoch: 8, batch: 835, loss: 0.31279, accu: 0.97188, speed: 1.30 step/s\n",
      "global step 6970, epoch: 8, batch: 845, loss: 0.07898, accu: 0.97411, speed: 1.30 step/s\n",
      "global step 6980, epoch: 8, batch: 855, loss: 0.15235, accu: 0.97422, speed: 1.29 step/s\n",
      "global step 6990, epoch: 8, batch: 865, loss: 0.05064, accu: 0.97569, speed: 1.30 step/s\n",
      "global step 7000, epoch: 8, batch: 875, loss: 0.02745, accu: 0.97500, speed: 1.33 step/s\n",
      "eval loss: 1.69819, accu: 0.65717\n",
      "global step 7010, epoch: 9, batch: 10, loss: 0.00518, accu: 0.98750, speed: 0.09 step/s\n",
      "global step 7020, epoch: 9, batch: 20, loss: 0.02869, accu: 0.99375, speed: 1.30 step/s\n",
      "global step 7030, epoch: 9, batch: 30, loss: 0.01213, accu: 0.99375, speed: 1.28 step/s\n",
      "global step 7040, epoch: 9, batch: 40, loss: 0.04283, accu: 0.99219, speed: 1.29 step/s\n",
      "global step 7050, epoch: 9, batch: 50, loss: 0.06331, accu: 0.99000, speed: 1.30 step/s\n",
      "global step 7060, epoch: 9, batch: 60, loss: 0.02206, accu: 0.98958, speed: 1.28 step/s\n",
      "global step 7070, epoch: 9, batch: 70, loss: 0.01641, accu: 0.98839, speed: 1.29 step/s\n",
      "global step 7080, epoch: 9, batch: 80, loss: 0.07650, accu: 0.98672, speed: 1.28 step/s\n",
      "global step 7090, epoch: 9, batch: 90, loss: 0.00989, accu: 0.98681, speed: 1.29 step/s\n",
      "global step 7100, epoch: 9, batch: 100, loss: 0.00798, accu: 0.98687, speed: 1.29 step/s\n",
      "eval loss: 1.74125, accu: 0.64400\n",
      "global step 7110, epoch: 9, batch: 110, loss: 0.01728, accu: 0.98750, speed: 0.09 step/s\n",
      "global step 7120, epoch: 9, batch: 120, loss: 0.01355, accu: 0.98750, speed: 1.29 step/s\n",
      "global step 7130, epoch: 9, batch: 130, loss: 0.10035, accu: 0.97917, speed: 1.29 step/s\n",
      "global step 7140, epoch: 9, batch: 140, loss: 0.02742, accu: 0.97969, speed: 1.28 step/s\n",
      "global step 7150, epoch: 9, batch: 150, loss: 0.00165, accu: 0.98125, speed: 1.28 step/s\n",
      "global step 7160, epoch: 9, batch: 160, loss: 0.02378, accu: 0.97813, speed: 1.28 step/s\n",
      "global step 7170, epoch: 9, batch: 170, loss: 0.01902, accu: 0.97946, speed: 1.28 step/s\n",
      "global step 7180, epoch: 9, batch: 180, loss: 0.00261, accu: 0.97969, speed: 1.28 step/s\n",
      "global step 7190, epoch: 9, batch: 190, loss: 0.04025, accu: 0.97917, speed: 1.28 step/s\n",
      "global step 7200, epoch: 9, batch: 200, loss: 0.03587, accu: 0.97875, speed: 1.28 step/s\n",
      "eval loss: 1.75763, accu: 0.64350\n",
      "global step 7210, epoch: 9, batch: 210, loss: 0.03577, accu: 0.96875, speed: 0.09 step/s\n",
      "global step 7220, epoch: 9, batch: 220, loss: 0.24952, accu: 0.96250, speed: 1.29 step/s\n",
      "global step 7230, epoch: 9, batch: 230, loss: 0.16367, accu: 0.97083, speed: 1.29 step/s\n",
      "global step 7240, epoch: 9, batch: 240, loss: 0.03483, accu: 0.97656, speed: 1.29 step/s\n",
      "global step 7250, epoch: 9, batch: 250, loss: 0.00908, accu: 0.97250, speed: 1.30 step/s\n",
      "global step 7260, epoch: 9, batch: 260, loss: 0.00192, accu: 0.97292, speed: 1.29 step/s\n",
      "global step 7270, epoch: 9, batch: 270, loss: 0.03391, accu: 0.97500, speed: 1.28 step/s\n",
      "global step 7280, epoch: 9, batch: 280, loss: 0.14655, accu: 0.97578, speed: 1.29 step/s\n",
      "global step 7290, epoch: 9, batch: 290, loss: 0.00675, accu: 0.97778, speed: 1.28 step/s\n",
      "global step 7300, epoch: 9, batch: 300, loss: 0.16617, accu: 0.97813, speed: 1.28 step/s\n",
      "eval loss: 1.79593, accu: 0.65483\n",
      "global step 7310, epoch: 9, batch: 310, loss: 0.04563, accu: 0.97500, speed: 0.09 step/s\n",
      "global step 7320, epoch: 9, batch: 320, loss: 0.01013, accu: 0.97188, speed: 1.28 step/s\n",
      "global step 7330, epoch: 9, batch: 330, loss: 0.15365, accu: 0.97083, speed: 1.28 step/s\n",
      "global step 7340, epoch: 9, batch: 340, loss: 0.01270, accu: 0.97188, speed: 1.26 step/s\n",
      "global step 7350, epoch: 9, batch: 350, loss: 0.24584, accu: 0.97125, speed: 1.27 step/s\n",
      "global step 7360, epoch: 9, batch: 360, loss: 0.05215, accu: 0.97500, speed: 1.28 step/s\n",
      "global step 7370, epoch: 9, batch: 370, loss: 0.15856, accu: 0.97411, speed: 1.29 step/s\n",
      "global step 7380, epoch: 9, batch: 380, loss: 0.04230, accu: 0.97578, speed: 1.28 step/s\n",
      "global step 7390, epoch: 9, batch: 390, loss: 0.01544, accu: 0.97708, speed: 1.28 step/s\n",
      "global step 7400, epoch: 9, batch: 400, loss: 0.04561, accu: 0.97562, speed: 1.24 step/s\n",
      "eval loss: 1.79918, accu: 0.65917\n",
      "global step 7410, epoch: 9, batch: 410, loss: 0.08047, accu: 0.99375, speed: 0.09 step/s\n",
      "global step 7420, epoch: 9, batch: 420, loss: 0.03200, accu: 0.99062, speed: 1.29 step/s\n",
      "global step 7430, epoch: 9, batch: 430, loss: 0.11146, accu: 0.98542, speed: 1.29 step/s\n",
      "global step 7440, epoch: 9, batch: 440, loss: 0.02845, accu: 0.98750, speed: 1.29 step/s\n",
      "global step 7450, epoch: 9, batch: 450, loss: 0.00318, accu: 0.98000, speed: 1.29 step/s\n",
      "global step 7460, epoch: 9, batch: 460, loss: 0.13169, accu: 0.98021, speed: 1.29 step/s\n",
      "global step 7470, epoch: 9, batch: 470, loss: 0.10610, accu: 0.97679, speed: 1.28 step/s\n",
      "global step 7480, epoch: 9, batch: 480, loss: 0.02239, accu: 0.97891, speed: 1.30 step/s\n",
      "global step 7490, epoch: 9, batch: 490, loss: 0.02914, accu: 0.97986, speed: 1.29 step/s\n",
      "global step 7500, epoch: 9, batch: 500, loss: 0.00710, accu: 0.98062, speed: 1.29 step/s\n",
      "eval loss: 1.82845, accu: 0.65733\n",
      "global step 7510, epoch: 9, batch: 510, loss: 0.00988, accu: 0.98125, speed: 0.09 step/s\n",
      "global step 7520, epoch: 9, batch: 520, loss: 0.01035, accu: 0.98125, speed: 1.30 step/s\n",
      "global step 7530, epoch: 9, batch: 530, loss: 0.21538, accu: 0.97292, speed: 1.30 step/s\n",
      "global step 7540, epoch: 9, batch: 540, loss: 0.02518, accu: 0.97656, speed: 1.31 step/s\n",
      "global step 7550, epoch: 9, batch: 550, loss: 0.00304, accu: 0.97875, speed: 1.29 step/s\n",
      "global step 7560, epoch: 9, batch: 560, loss: 0.20669, accu: 0.97917, speed: 1.30 step/s\n",
      "global step 7570, epoch: 9, batch: 570, loss: 0.02503, accu: 0.98036, speed: 1.30 step/s\n",
      "global step 7580, epoch: 9, batch: 580, loss: 0.28135, accu: 0.97734, speed: 1.30 step/s\n",
      "global step 7590, epoch: 9, batch: 590, loss: 0.26881, accu: 0.97431, speed: 1.30 step/s\n",
      "global step 7600, epoch: 9, batch: 600, loss: 0.00977, accu: 0.97625, speed: 1.29 step/s\n",
      "eval loss: 1.84834, accu: 0.64667\n",
      "global step 7610, epoch: 9, batch: 610, loss: 0.00490, accu: 0.99375, speed: 0.09 step/s\n",
      "global step 7620, epoch: 9, batch: 620, loss: 0.01015, accu: 0.97813, speed: 1.29 step/s\n",
      "global step 7630, epoch: 9, batch: 630, loss: 0.16150, accu: 0.97500, speed: 1.29 step/s\n",
      "global step 7640, epoch: 9, batch: 640, loss: 0.08649, accu: 0.97344, speed: 1.29 step/s\n",
      "global step 7650, epoch: 9, batch: 650, loss: 0.00949, accu: 0.96875, speed: 1.29 step/s\n",
      "global step 7660, epoch: 9, batch: 660, loss: 0.01505, accu: 0.96875, speed: 1.27 step/s\n",
      "global step 7670, epoch: 9, batch: 670, loss: 0.00477, accu: 0.97232, speed: 1.28 step/s\n",
      "global step 7680, epoch: 9, batch: 680, loss: 0.05912, accu: 0.97422, speed: 1.28 step/s\n",
      "global step 7690, epoch: 9, batch: 690, loss: 0.05412, accu: 0.97431, speed: 1.28 step/s\n",
      "global step 7700, epoch: 9, batch: 700, loss: 0.01436, accu: 0.97500, speed: 1.28 step/s\n",
      "eval loss: 1.84545, accu: 0.65200\n",
      "global step 7710, epoch: 9, batch: 710, loss: 0.01456, accu: 0.97500, speed: 0.09 step/s\n",
      "global step 7720, epoch: 9, batch: 720, loss: 0.00260, accu: 0.98438, speed: 1.28 step/s\n",
      "global step 7730, epoch: 9, batch: 730, loss: 0.03683, accu: 0.97708, speed: 1.29 step/s\n",
      "global step 7740, epoch: 9, batch: 740, loss: 0.06378, accu: 0.97813, speed: 1.30 step/s\n",
      "global step 7750, epoch: 9, batch: 750, loss: 0.43794, accu: 0.97500, speed: 1.29 step/s\n",
      "global step 7760, epoch: 9, batch: 760, loss: 0.02367, accu: 0.97500, speed: 1.29 step/s\n",
      "global step 7770, epoch: 9, batch: 770, loss: 0.04856, accu: 0.97500, speed: 1.30 step/s\n",
      "global step 7780, epoch: 9, batch: 780, loss: 0.02247, accu: 0.97500, speed: 1.31 step/s\n",
      "global step 7790, epoch: 9, batch: 790, loss: 0.37895, accu: 0.97500, speed: 1.31 step/s\n",
      "global step 7800, epoch: 9, batch: 800, loss: 0.06492, accu: 0.97250, speed: 1.31 step/s\n",
      "eval loss: 1.84387, accu: 0.63350\n",
      "global step 7810, epoch: 9, batch: 810, loss: 0.32815, accu: 0.96875, speed: 0.09 step/s\n",
      "global step 7820, epoch: 9, batch: 820, loss: 0.07149, accu: 0.97500, speed: 1.30 step/s\n",
      "global step 7830, epoch: 9, batch: 830, loss: 0.01577, accu: 0.97083, speed: 1.29 step/s\n",
      "global step 7840, epoch: 9, batch: 840, loss: 0.01210, accu: 0.97031, speed: 1.29 step/s\n",
      "global step 7850, epoch: 9, batch: 850, loss: 0.11241, accu: 0.97000, speed: 1.28 step/s\n",
      "global step 7860, epoch: 9, batch: 860, loss: 0.22202, accu: 0.97083, speed: 1.29 step/s\n",
      "global step 7870, epoch: 9, batch: 870, loss: 0.11938, accu: 0.96786, speed: 1.28 step/s\n",
      "global step 7880, epoch: 10, batch: 5, loss: 0.04917, accu: 0.97031, speed: 1.28 step/s\n",
      "global step 7890, epoch: 10, batch: 15, loss: 0.00559, accu: 0.97222, speed: 1.28 step/s\n",
      "global step 7900, epoch: 10, batch: 25, loss: 0.01736, accu: 0.97437, speed: 1.30 step/s\n",
      "eval loss: 1.75432, accu: 0.66067\n",
      "global step 7910, epoch: 10, batch: 35, loss: 0.01644, accu: 1.00000, speed: 0.09 step/s\n",
      "global step 7920, epoch: 10, batch: 45, loss: 0.00550, accu: 0.99687, speed: 1.30 step/s\n",
      "global step 7930, epoch: 10, batch: 55, loss: 0.00711, accu: 0.99167, speed: 1.30 step/s\n",
      "global step 7940, epoch: 10, batch: 65, loss: 0.02467, accu: 0.98750, speed: 1.31 step/s\n",
      "global step 7950, epoch: 10, batch: 75, loss: 0.01208, accu: 0.99000, speed: 1.31 step/s\n",
      "global step 7960, epoch: 10, batch: 85, loss: 0.08523, accu: 0.98958, speed: 1.31 step/s\n",
      "global step 7970, epoch: 10, batch: 95, loss: 0.00681, accu: 0.99107, speed: 1.30 step/s\n",
      "global step 7980, epoch: 10, batch: 105, loss: 0.03417, accu: 0.99141, speed: 1.30 step/s\n",
      "global step 7990, epoch: 10, batch: 115, loss: 0.00329, accu: 0.99097, speed: 1.29 step/s\n",
      "global step 8000, epoch: 10, batch: 125, loss: 0.03943, accu: 0.98750, speed: 1.29 step/s\n",
      "eval loss: 1.89369, accu: 0.65567\n",
      "global step 8010, epoch: 10, batch: 135, loss: 0.01599, accu: 0.96875, speed: 0.09 step/s\n",
      "global step 8020, epoch: 10, batch: 145, loss: 0.05708, accu: 0.97188, speed: 1.30 step/s\n",
      "global step 8030, epoch: 10, batch: 155, loss: 0.00464, accu: 0.97917, speed: 1.28 step/s\n",
      "global step 8040, epoch: 10, batch: 165, loss: 0.00766, accu: 0.98281, speed: 1.29 step/s\n",
      "global step 8050, epoch: 10, batch: 175, loss: 0.09218, accu: 0.97875, speed: 1.28 step/s\n",
      "global step 8060, epoch: 10, batch: 185, loss: 0.01830, accu: 0.97917, speed: 1.29 step/s\n",
      "global step 8070, epoch: 10, batch: 195, loss: 0.07124, accu: 0.97946, speed: 1.28 step/s\n",
      "global step 8080, epoch: 10, batch: 205, loss: 0.01375, accu: 0.98203, speed: 1.28 step/s\n",
      "global step 8090, epoch: 10, batch: 215, loss: 0.01920, accu: 0.98264, speed: 1.29 step/s\n",
      "global step 8100, epoch: 10, batch: 225, loss: 0.02656, accu: 0.98250, speed: 1.28 step/s\n",
      "eval loss: 1.78675, accu: 0.64850\n",
      "global step 8110, epoch: 10, batch: 235, loss: 0.01131, accu: 0.98125, speed: 0.09 step/s\n",
      "global step 8120, epoch: 10, batch: 245, loss: 0.04680, accu: 0.98438, speed: 1.29 step/s\n",
      "global step 8130, epoch: 10, batch: 255, loss: 0.12508, accu: 0.98333, speed: 1.29 step/s\n",
      "global step 8140, epoch: 10, batch: 265, loss: 0.00737, accu: 0.98594, speed: 1.29 step/s\n",
      "global step 8150, epoch: 10, batch: 275, loss: 0.02079, accu: 0.98000, speed: 1.29 step/s\n",
      "global step 8160, epoch: 10, batch: 285, loss: 0.02343, accu: 0.98021, speed: 1.29 step/s\n",
      "global step 8170, epoch: 10, batch: 295, loss: 0.15481, accu: 0.97857, speed: 1.29 step/s\n",
      "global step 8180, epoch: 10, batch: 305, loss: 0.00128, accu: 0.98047, speed: 1.29 step/s\n",
      "global step 8190, epoch: 10, batch: 315, loss: 0.01570, accu: 0.98194, speed: 1.29 step/s\n",
      "global step 8200, epoch: 10, batch: 325, loss: 0.07851, accu: 0.98000, speed: 1.28 step/s\n",
      "eval loss: 1.87192, accu: 0.66317\n",
      "global step 8210, epoch: 10, batch: 335, loss: 0.23662, accu: 0.98125, speed: 0.09 step/s\n",
      "global step 8220, epoch: 10, batch: 345, loss: 0.10090, accu: 0.97813, speed: 1.26 step/s\n",
      "global step 8230, epoch: 10, batch: 355, loss: 0.01709, accu: 0.97917, speed: 1.27 step/s\n",
      "global step 8240, epoch: 10, batch: 365, loss: 0.01831, accu: 0.97656, speed: 1.27 step/s\n",
      "global step 8250, epoch: 10, batch: 375, loss: 0.09443, accu: 0.97750, speed: 1.29 step/s\n",
      "global step 8260, epoch: 10, batch: 385, loss: 0.04547, accu: 0.97917, speed: 1.29 step/s\n",
      "global step 8270, epoch: 10, batch: 395, loss: 0.00519, accu: 0.98036, speed: 1.30 step/s\n",
      "global step 8280, epoch: 10, batch: 405, loss: 0.05148, accu: 0.98203, speed: 1.29 step/s\n",
      "global step 8290, epoch: 10, batch: 415, loss: 0.12591, accu: 0.98194, speed: 1.29 step/s\n",
      "global step 8300, epoch: 10, batch: 425, loss: 0.08193, accu: 0.98250, speed: 1.29 step/s\n",
      "eval loss: 1.89626, accu: 0.65417\n",
      "global step 8310, epoch: 10, batch: 435, loss: 0.08881, accu: 0.97500, speed: 0.09 step/s\n",
      "global step 8320, epoch: 10, batch: 445, loss: 0.00844, accu: 0.98438, speed: 1.29 step/s\n",
      "global step 8330, epoch: 10, batch: 455, loss: 0.00310, accu: 0.98542, speed: 1.29 step/s\n",
      "global step 8340, epoch: 10, batch: 465, loss: 0.00501, accu: 0.98750, speed: 1.30 step/s\n",
      "global step 8350, epoch: 10, batch: 475, loss: 0.20144, accu: 0.98375, speed: 1.30 step/s\n",
      "global step 8360, epoch: 10, batch: 485, loss: 0.11952, accu: 0.97813, speed: 1.30 step/s\n",
      "global step 8370, epoch: 10, batch: 495, loss: 0.00991, accu: 0.97857, speed: 1.30 step/s\n",
      "global step 8380, epoch: 10, batch: 505, loss: 0.01308, accu: 0.97266, speed: 1.30 step/s\n",
      "global step 8390, epoch: 10, batch: 515, loss: 0.05595, accu: 0.97431, speed: 1.30 step/s\n",
      "global step 8400, epoch: 10, batch: 525, loss: 0.03343, accu: 0.97562, speed: 1.30 step/s\n",
      "eval loss: 1.90813, accu: 0.65733\n",
      "global step 8410, epoch: 10, batch: 535, loss: 0.45435, accu: 0.96875, speed: 0.09 step/s\n",
      "global step 8420, epoch: 10, batch: 545, loss: 0.19070, accu: 0.97188, speed: 1.29 step/s\n",
      "global step 8430, epoch: 10, batch: 555, loss: 0.11706, accu: 0.97083, speed: 1.29 step/s\n",
      "global step 8440, epoch: 10, batch: 565, loss: 0.14818, accu: 0.97344, speed: 1.30 step/s\n",
      "global step 8450, epoch: 10, batch: 575, loss: 0.02418, accu: 0.97625, speed: 1.24 step/s\n",
      "global step 8460, epoch: 10, batch: 585, loss: 0.00953, accu: 0.97708, speed: 1.29 step/s\n",
      "global step 8470, epoch: 10, batch: 595, loss: 0.02965, accu: 0.97946, speed: 1.29 step/s\n",
      "global step 8480, epoch: 10, batch: 605, loss: 0.01288, accu: 0.98125, speed: 1.28 step/s\n",
      "global step 8490, epoch: 10, batch: 615, loss: 0.05282, accu: 0.98125, speed: 1.29 step/s\n",
      "global step 8500, epoch: 10, batch: 625, loss: 0.07078, accu: 0.98000, speed: 1.29 step/s\n",
      "eval loss: 1.89357, accu: 0.64950\n",
      "global step 8510, epoch: 10, batch: 635, loss: 0.00296, accu: 1.00000, speed: 0.09 step/s\n",
      "global step 8520, epoch: 10, batch: 645, loss: 0.02634, accu: 0.98438, speed: 1.30 step/s\n",
      "global step 8530, epoch: 10, batch: 655, loss: 0.12704, accu: 0.98333, speed: 1.29 step/s\n",
      "global step 8540, epoch: 10, batch: 665, loss: 0.02148, accu: 0.98594, speed: 1.28 step/s\n",
      "global step 8550, epoch: 10, batch: 675, loss: 0.00260, accu: 0.98625, speed: 1.29 step/s\n",
      "global step 8560, epoch: 10, batch: 685, loss: 0.02552, accu: 0.98542, speed: 1.29 step/s\n",
      "global step 8570, epoch: 10, batch: 695, loss: 0.01247, accu: 0.98304, speed: 1.28 step/s\n",
      "global step 8580, epoch: 10, batch: 705, loss: 0.28895, accu: 0.98281, speed: 1.30 step/s\n",
      "global step 8590, epoch: 10, batch: 715, loss: 0.00391, accu: 0.98125, speed: 1.28 step/s\n",
      "global step 8600, epoch: 10, batch: 725, loss: 0.03118, accu: 0.98000, speed: 1.29 step/s\n",
      "eval loss: 1.86728, accu: 0.66217\n",
      "global step 8610, epoch: 10, batch: 735, loss: 0.01210, accu: 0.99375, speed: 0.09 step/s\n",
      "global step 8620, epoch: 10, batch: 745, loss: 0.06437, accu: 0.98750, speed: 1.29 step/s\n",
      "global step 8630, epoch: 10, batch: 755, loss: 0.02145, accu: 0.98750, speed: 1.30 step/s\n",
      "global step 8640, epoch: 10, batch: 765, loss: 0.18009, accu: 0.98438, speed: 1.29 step/s\n",
      "global step 8650, epoch: 10, batch: 775, loss: 0.02038, accu: 0.98500, speed: 1.28 step/s\n",
      "global step 8660, epoch: 10, batch: 785, loss: 0.00307, accu: 0.98125, speed: 1.29 step/s\n",
      "global step 8670, epoch: 10, batch: 795, loss: 0.15453, accu: 0.97946, speed: 1.29 step/s\n",
      "global step 8680, epoch: 10, batch: 805, loss: 0.39395, accu: 0.97656, speed: 1.29 step/s\n",
      "global step 8690, epoch: 10, batch: 815, loss: 0.03066, accu: 0.97639, speed: 1.30 step/s\n",
      "global step 8700, epoch: 10, batch: 825, loss: 0.02878, accu: 0.97562, speed: 1.30 step/s\n",
      "eval loss: 1.92913, accu: 0.65967\n",
      "global step 8710, epoch: 10, batch: 835, loss: 0.17985, accu: 0.96875, speed: 0.09 step/s\n",
      "global step 8720, epoch: 10, batch: 845, loss: 0.08630, accu: 0.95937, speed: 1.30 step/s\n",
      "global step 8730, epoch: 10, batch: 855, loss: 0.01361, accu: 0.95833, speed: 1.30 step/s\n",
      "global step 8740, epoch: 10, batch: 865, loss: 0.20880, accu: 0.95781, speed: 1.29 step/s\n",
      "global step 8750, epoch: 10, batch: 875, loss: 0.05388, accu: 0.95875, speed: 1.32 step/s\n",
      "global step 8760, epoch: 11, batch: 10, loss: 0.04970, accu: 0.96250, speed: 1.26 step/s\n",
      "global step 8770, epoch: 11, batch: 20, loss: 0.02059, accu: 0.96429, speed: 1.30 step/s\n",
      "global step 8780, epoch: 11, batch: 30, loss: 0.00528, accu: 0.96719, speed: 1.29 step/s\n",
      "global step 8790, epoch: 11, batch: 40, loss: 0.00547, accu: 0.96875, speed: 1.28 step/s\n",
      "global step 8800, epoch: 11, batch: 50, loss: 0.05910, accu: 0.96875, speed: 1.29 step/s\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "global_step = 0\n",
    "tic_train = time.time()\n",
    "for epoch in range(1, epochs + 1):\n",
    "    for step, batch in enumerate(train_data_loader, start=1):\n",
    "        input_ids, token_type_ids, labels = batch\n",
    "        logits = model(input_ids, token_type_ids)\n",
    "        # loss\n",
    "        loss = criterion(logits, labels)\n",
    "        probs = F.softmax(logits, axis=1)\n",
    "        # acc\n",
    "        correct = metric.compute(probs, labels)\n",
    "        metric.update(correct)\n",
    "        acc = metric.accumulate()\n",
    "\n",
    "        global_step += 1\n",
    "        if global_step % 10 == 0:\n",
    "            print(\n",
    "                \"global step %d, epoch: %d, batch: %d, loss: %.5f, accu: %.5f, speed: %.2f step/s\"\n",
    "                % (global_step, epoch, step, loss, acc,\n",
    "                    10 / (time.time() - tic_train)))\n",
    "            tic_train = time.time()\n",
    "        \n",
    "        # bp\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.clear_grad()\n",
    "\n",
    "        if global_step % 100 == 0:\n",
    "            save_dir = os.path.join(ckpt_dir, \"model_%d\" % global_step)\n",
    "            if not os.path.exists(save_dir):\n",
    "                os.makedirs(save_dir)\n",
    "            # evaluate model\n",
    "            evaluate(model, criterion, metric, dev_data_loader)\n",
    "            # save\n",
    "            model.save_pretrained(save_dir)\n",
    "            # save\n",
    "            tokenizer.save_pretrained(save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43cfd886-78e1-4d14-ba1c-c4a4f334f639",
   "metadata": {},
   "source": [
    "# ERNIE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77efe87-3e82-4dfb-8b25-3895691abc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install paddle-ernie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ef52dbb9-6449-49b0-98cf-ff014857bd6a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-25T04:38:02.263314Z",
     "iopub.status.busy": "2021-10-25T04:38:02.262751Z",
     "iopub.status.idle": "2021-10-25T04:38:05.187876Z",
     "shell.execute_reply": "2021-10-25T04:38:05.186551Z",
     "shell.execute_reply.started": "2021-10-25T04:38:02.263236Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-10-25 12:38:02,271] [    INFO] - Found /home/aistudio/.paddlenlp/models/ernie-1.0/vocab.txt\n",
      "[2021-10-25 12:38:02,295] [    INFO] - Already cached /home/aistudio/.paddlenlp/models/ernie-1.0/ernie_v1_chn_base.pdparams\n",
      "[2021-10-25 12:38:03,283] [    INFO] - Weights from pretrained model not used in ErnieModel: ['cls.predictions.layer_norm.weight', 'cls.predictions.decoder_bias', 'cls.predictions.transform.bias', 'cls.predictions.transform.weight', 'cls.predictions.layer_norm.bias']\n",
      "[2021-10-25 12:38:03,588] [    INFO] - Already cached /home/aistudio/.paddlenlp/models/ernie-1.0/ernie_v1_chn_base.pdparams\n"
     ]
    }
   ],
   "source": [
    "#TRY ERNIE\n",
    "import paddlenlp as ppnlp\n",
    "MODEL_NAME = \"ernie-1.0\"\n",
    "\n",
    "tokenizer = ppnlp.transformers.ErnieTokenizer.from_pretrained(MODEL_NAME)\n",
    "ernie_model = ppnlp.transformers.ErnieModel.from_pretrained(MODEL_NAME)\n",
    "\n",
    "from functools import partial\n",
    "from paddlenlp.data import Stack, Tuple, Pad\n",
    "\n",
    "# 模型运行批处理大小\n",
    "batch_size = 32\n",
    "max_seq_length = 256\n",
    "\n",
    "trans_func = partial(\n",
    "    convert_example,\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_length=max_seq_length)\n",
    "batchify_fn = lambda samples, fn=Tuple(\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_id),  # input\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_type_id),  # segment\n",
    "    Stack(dtype=\"int64\")  # label\n",
    "): [data for data in fn(samples)]\n",
    "train_data_loader = create_dataloader(\n",
    "    train_ds,\n",
    "    mode='train',\n",
    "    batch_size=batch_size,\n",
    "    batchify_fn=batchify_fn,\n",
    "    trans_fn=trans_func)\n",
    "dev_data_loader = create_dataloader(\n",
    "    dev_ds,\n",
    "    mode='dev',\n",
    "    batch_size=batch_size,\n",
    "    batchify_fn=batchify_fn,\n",
    "    trans_fn=trans_func)\n",
    "\n",
    "model = ppnlp.transformers.ErnieForSequenceClassification.from_pretrained(MODEL_NAME, num_classes=19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "192e1c1d-58da-4041-81b6-3a8e5c73c755",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-25T04:38:06.048456Z",
     "iopub.status.busy": "2021-10-25T04:38:06.047625Z",
     "iopub.status.idle": "2021-10-25T04:38:06.060198Z",
     "shell.execute_reply": "2021-10-25T04:38:06.059304Z",
     "shell.execute_reply.started": "2021-10-25T04:38:06.048130Z"
    }
   },
   "outputs": [],
   "source": [
    "from paddlenlp.transformers import LinearDecayWithWarmup\n",
    "\n",
    "# lr\n",
    "learning_rate = 5e-5 \n",
    "# epoech\n",
    "epochs = 20 \n",
    "# warm_up\n",
    "warmup_proportion = 0.1\n",
    "# weight_decay\n",
    "weight_decay = 0.01\n",
    "\n",
    "num_training_steps = len(train_data_loader) * epochs\n",
    "lr_scheduler = LinearDecayWithWarmup(learning_rate, num_training_steps, warmup_proportion)\n",
    "optimizer = paddle.optimizer.AdamW(\n",
    "    learning_rate=lr_scheduler,\n",
    "    parameters=model.parameters(),\n",
    "    weight_decay=weight_decay,\n",
    "    apply_decay_param_fun=lambda x: x in [\n",
    "        p.name for n, p in model.named_parameters()\n",
    "        if not any(nd in n for nd in [\"bias\", \"norm\"])\n",
    "    ])\n",
    "\n",
    "criterion = paddle.nn.loss.CrossEntropyLoss()\n",
    "metric = paddle.metric.Accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "08c23a83-0ba2-4a7b-af4b-2a547c559cd0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-25T04:38:23.512272Z",
     "iopub.status.busy": "2021-10-25T04:38:23.511510Z",
     "iopub.status.idle": "2021-10-25T05:53:36.508933Z",
     "shell.execute_reply": "2021-10-25T05:53:36.507879Z",
     "shell.execute_reply.started": "2021-10-25T04:38:23.511962Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 10, epoch: 1, batch: 10, loss: 2.95865, acc: 0.09007\n",
      "global step 20, epoch: 1, batch: 20, loss: 2.91498, acc: 0.09730\n",
      "global step 30, epoch: 1, batch: 30, loss: 2.62086, acc: 0.11516\n",
      "global step 40, epoch: 1, batch: 40, loss: 2.81059, acc: 0.13477\n",
      "global step 50, epoch: 1, batch: 50, loss: 2.27408, acc: 0.15583\n",
      "global step 60, epoch: 1, batch: 60, loss: 2.55199, acc: 0.17671\n",
      "global step 70, epoch: 1, batch: 70, loss: 1.92069, acc: 0.20013\n",
      "global step 80, epoch: 1, batch: 80, loss: 2.01599, acc: 0.21935\n",
      "global step 90, epoch: 1, batch: 90, loss: 2.30413, acc: 0.23163\n",
      "global step 100, epoch: 1, batch: 100, loss: 2.09712, acc: 0.24597\n",
      "global step 110, epoch: 1, batch: 110, loss: 2.09347, acc: 0.26329\n",
      "global step 120, epoch: 1, batch: 120, loss: 2.12261, acc: 0.27951\n",
      "global step 130, epoch: 1, batch: 130, loss: 2.20095, acc: 0.29383\n",
      "global step 140, epoch: 1, batch: 140, loss: 2.22322, acc: 0.30316\n",
      "global step 150, epoch: 1, batch: 150, loss: 1.85222, acc: 0.31699\n",
      "global step 160, epoch: 1, batch: 160, loss: 1.68056, acc: 0.32609\n",
      "global step 170, epoch: 1, batch: 170, loss: 1.94701, acc: 0.33570\n",
      "global step 180, epoch: 1, batch: 180, loss: 1.53522, acc: 0.34635\n",
      "global step 190, epoch: 1, batch: 190, loss: 1.57986, acc: 0.35689\n",
      "global step 200, epoch: 1, batch: 200, loss: 1.74699, acc: 0.36537\n",
      "global step 210, epoch: 1, batch: 210, loss: 1.64514, acc: 0.37233\n",
      "global step 220, epoch: 1, batch: 220, loss: 1.39092, acc: 0.38025\n",
      "global step 230, epoch: 1, batch: 230, loss: 1.25644, acc: 0.38755\n",
      "global step 240, epoch: 1, batch: 240, loss: 1.72354, acc: 0.39500\n",
      "global step 250, epoch: 1, batch: 250, loss: 1.57231, acc: 0.40488\n",
      "global step 260, epoch: 1, batch: 260, loss: 1.46894, acc: 0.41230\n",
      "global step 270, epoch: 1, batch: 270, loss: 1.64808, acc: 0.41847\n",
      "global step 280, epoch: 1, batch: 280, loss: 1.55218, acc: 0.42527\n",
      "global step 290, epoch: 1, batch: 290, loss: 1.17049, acc: 0.43083\n",
      "global step 300, epoch: 1, batch: 300, loss: 0.93461, acc: 0.43663\n",
      "global step 310, epoch: 1, batch: 310, loss: 1.17600, acc: 0.44171\n",
      "global step 320, epoch: 1, batch: 320, loss: 1.28110, acc: 0.44631\n",
      "global step 330, epoch: 1, batch: 330, loss: 0.83004, acc: 0.45189\n",
      "global step 340, epoch: 1, batch: 340, loss: 1.15353, acc: 0.45682\n",
      "global step 350, epoch: 1, batch: 350, loss: 1.20413, acc: 0.46131\n",
      "global step 360, epoch: 1, batch: 360, loss: 1.16642, acc: 0.46558\n",
      "global step 370, epoch: 1, batch: 370, loss: 1.56812, acc: 0.47049\n",
      "global step 380, epoch: 1, batch: 380, loss: 1.16366, acc: 0.47440\n",
      "global step 390, epoch: 1, batch: 390, loss: 1.13125, acc: 0.47841\n",
      "global step 400, epoch: 1, batch: 400, loss: 1.36549, acc: 0.48231\n",
      "global step 410, epoch: 1, batch: 410, loss: 1.22698, acc: 0.48567\n",
      "global step 420, epoch: 1, batch: 420, loss: 0.97097, acc: 0.48965\n",
      "global step 430, epoch: 1, batch: 430, loss: 1.02627, acc: 0.49387\n",
      "eval loss: 1.03669, accu: 0.65700\n",
      "global step 440, epoch: 2, batch: 2, loss: 0.87373, acc: 0.64062\n",
      "global step 450, epoch: 2, batch: 12, loss: 1.10859, acc: 0.67708\n",
      "global step 460, epoch: 2, batch: 22, loss: 0.85660, acc: 0.66903\n",
      "global step 470, epoch: 2, batch: 32, loss: 1.23065, acc: 0.67773\n",
      "global step 480, epoch: 2, batch: 42, loss: 1.02847, acc: 0.67336\n",
      "global step 490, epoch: 2, batch: 52, loss: 1.15909, acc: 0.67188\n",
      "global step 500, epoch: 2, batch: 62, loss: 0.68050, acc: 0.67843\n",
      "global step 510, epoch: 2, batch: 72, loss: 1.26027, acc: 0.67491\n",
      "global step 520, epoch: 2, batch: 82, loss: 0.94539, acc: 0.67759\n",
      "global step 530, epoch: 2, batch: 92, loss: 1.12902, acc: 0.67493\n",
      "global step 540, epoch: 2, batch: 102, loss: 0.88937, acc: 0.67770\n",
      "global step 550, epoch: 2, batch: 112, loss: 1.01201, acc: 0.67243\n",
      "global step 560, epoch: 2, batch: 122, loss: 1.16389, acc: 0.67367\n",
      "global step 570, epoch: 2, batch: 132, loss: 0.99631, acc: 0.66974\n",
      "global step 580, epoch: 2, batch: 142, loss: 1.20623, acc: 0.66945\n",
      "global step 590, epoch: 2, batch: 152, loss: 1.01335, acc: 0.67188\n",
      "global step 600, epoch: 2, batch: 162, loss: 0.71676, acc: 0.67168\n",
      "global step 610, epoch: 2, batch: 172, loss: 0.87684, acc: 0.67333\n",
      "global step 620, epoch: 2, batch: 182, loss: 0.89077, acc: 0.67497\n",
      "global step 630, epoch: 2, batch: 192, loss: 1.03180, acc: 0.67546\n",
      "global step 640, epoch: 2, batch: 202, loss: 0.83618, acc: 0.67621\n",
      "global step 650, epoch: 2, batch: 212, loss: 1.31750, acc: 0.67541\n",
      "global step 660, epoch: 2, batch: 222, loss: 0.99376, acc: 0.67525\n",
      "global step 670, epoch: 2, batch: 232, loss: 1.37778, acc: 0.67376\n",
      "global step 680, epoch: 2, batch: 242, loss: 0.65341, acc: 0.67472\n",
      "global step 690, epoch: 2, batch: 252, loss: 0.81255, acc: 0.67634\n",
      "global step 700, epoch: 2, batch: 262, loss: 1.00766, acc: 0.67879\n",
      "global step 710, epoch: 2, batch: 272, loss: 0.86791, acc: 0.67992\n",
      "global step 720, epoch: 2, batch: 282, loss: 0.77524, acc: 0.68074\n",
      "global step 730, epoch: 2, batch: 292, loss: 1.25439, acc: 0.68001\n",
      "global step 740, epoch: 2, batch: 302, loss: 0.82042, acc: 0.68015\n",
      "global step 750, epoch: 2, batch: 312, loss: 0.75493, acc: 0.67969\n",
      "global step 760, epoch: 2, batch: 322, loss: 0.90060, acc: 0.67847\n",
      "global step 770, epoch: 2, batch: 332, loss: 0.89850, acc: 0.67799\n",
      "global step 780, epoch: 2, batch: 342, loss: 0.84102, acc: 0.67845\n",
      "global step 790, epoch: 2, batch: 352, loss: 0.68343, acc: 0.67924\n",
      "global step 800, epoch: 2, batch: 362, loss: 0.81383, acc: 0.67783\n",
      "global step 810, epoch: 2, batch: 372, loss: 0.71569, acc: 0.67792\n",
      "global step 820, epoch: 2, batch: 382, loss: 1.01287, acc: 0.67727\n",
      "global step 830, epoch: 2, batch: 392, loss: 0.94655, acc: 0.67714\n",
      "global step 840, epoch: 2, batch: 402, loss: 1.04275, acc: 0.67646\n",
      "global step 850, epoch: 2, batch: 412, loss: 1.09930, acc: 0.67643\n",
      "global step 860, epoch: 2, batch: 422, loss: 0.68742, acc: 0.67632\n",
      "global step 870, epoch: 2, batch: 432, loss: 0.90693, acc: 0.67614\n",
      "eval loss: 0.94282, accu: 0.66717\n",
      "global step 880, epoch: 3, batch: 4, loss: 0.62772, acc: 0.78125\n",
      "global step 890, epoch: 3, batch: 14, loss: 0.82257, acc: 0.73884\n",
      "global step 900, epoch: 3, batch: 24, loss: 0.75471, acc: 0.74349\n",
      "global step 910, epoch: 3, batch: 34, loss: 0.65978, acc: 0.74449\n",
      "global step 920, epoch: 3, batch: 44, loss: 0.86267, acc: 0.74077\n",
      "global step 930, epoch: 3, batch: 54, loss: 0.94322, acc: 0.73843\n",
      "global step 940, epoch: 3, batch: 64, loss: 0.94691, acc: 0.73486\n",
      "global step 950, epoch: 3, batch: 74, loss: 0.55765, acc: 0.73353\n",
      "global step 960, epoch: 3, batch: 84, loss: 0.90569, acc: 0.73214\n",
      "global step 970, epoch: 3, batch: 94, loss: 0.72077, acc: 0.72540\n",
      "global step 980, epoch: 3, batch: 104, loss: 0.81335, acc: 0.72506\n",
      "global step 990, epoch: 3, batch: 114, loss: 0.82616, acc: 0.72560\n",
      "global step 1000, epoch: 3, batch: 124, loss: 0.73900, acc: 0.72354\n",
      "global step 1010, epoch: 3, batch: 134, loss: 0.78730, acc: 0.72505\n",
      "global step 1020, epoch: 3, batch: 144, loss: 0.70800, acc: 0.72396\n",
      "global step 1030, epoch: 3, batch: 154, loss: 0.70484, acc: 0.72423\n",
      "global step 1040, epoch: 3, batch: 164, loss: 0.81572, acc: 0.72123\n",
      "global step 1050, epoch: 3, batch: 174, loss: 0.85377, acc: 0.72001\n",
      "global step 1060, epoch: 3, batch: 184, loss: 0.82617, acc: 0.72164\n",
      "global step 1070, epoch: 3, batch: 194, loss: 0.71154, acc: 0.72310\n",
      "global step 1080, epoch: 3, batch: 204, loss: 0.74955, acc: 0.72457\n",
      "global step 1090, epoch: 3, batch: 214, loss: 1.32378, acc: 0.72298\n",
      "global step 1100, epoch: 3, batch: 224, loss: 0.70402, acc: 0.72447\n",
      "global step 1110, epoch: 3, batch: 234, loss: 0.85809, acc: 0.72449\n",
      "global step 1120, epoch: 3, batch: 244, loss: 0.74507, acc: 0.72349\n",
      "global step 1130, epoch: 3, batch: 254, loss: 0.96967, acc: 0.72306\n",
      "global step 1140, epoch: 3, batch: 264, loss: 1.04780, acc: 0.72254\n",
      "global step 1150, epoch: 3, batch: 274, loss: 0.81681, acc: 0.72297\n",
      "global step 1160, epoch: 3, batch: 284, loss: 0.65499, acc: 0.72370\n",
      "global step 1170, epoch: 3, batch: 294, loss: 0.76894, acc: 0.72311\n",
      "global step 1180, epoch: 3, batch: 304, loss: 0.49556, acc: 0.72266\n",
      "global step 1190, epoch: 3, batch: 314, loss: 0.68237, acc: 0.72203\n",
      "global step 1200, epoch: 3, batch: 324, loss: 0.76177, acc: 0.72251\n",
      "global step 1210, epoch: 3, batch: 334, loss: 0.75758, acc: 0.72324\n",
      "global step 1220, epoch: 3, batch: 344, loss: 1.00790, acc: 0.72356\n",
      "global step 1230, epoch: 3, batch: 354, loss: 1.03085, acc: 0.72334\n",
      "global step 1240, epoch: 3, batch: 364, loss: 0.80137, acc: 0.72210\n",
      "global step 1250, epoch: 3, batch: 374, loss: 1.05978, acc: 0.72193\n",
      "global step 1260, epoch: 3, batch: 384, loss: 0.95018, acc: 0.72160\n",
      "global step 1270, epoch: 3, batch: 394, loss: 0.67569, acc: 0.72256\n",
      "global step 1280, epoch: 3, batch: 404, loss: 0.88397, acc: 0.72254\n",
      "global step 1290, epoch: 3, batch: 414, loss: 0.83535, acc: 0.72207\n",
      "global step 1300, epoch: 3, batch: 424, loss: 0.85538, acc: 0.72199\n",
      "global step 1310, epoch: 3, batch: 434, loss: 1.02005, acc: 0.72192\n",
      "eval loss: 0.91050, accu: 0.67650\n",
      "global step 1320, epoch: 4, batch: 6, loss: 0.75438, acc: 0.76042\n",
      "global step 1330, epoch: 4, batch: 16, loss: 0.47557, acc: 0.74023\n",
      "global step 1340, epoch: 4, batch: 26, loss: 0.52357, acc: 0.76803\n",
      "global step 1350, epoch: 4, batch: 36, loss: 0.52837, acc: 0.77865\n",
      "global step 1360, epoch: 4, batch: 46, loss: 0.65859, acc: 0.78533\n",
      "global step 1370, epoch: 4, batch: 56, loss: 0.62460, acc: 0.77846\n",
      "global step 1380, epoch: 4, batch: 66, loss: 0.45346, acc: 0.78220\n",
      "global step 1390, epoch: 4, batch: 76, loss: 0.51511, acc: 0.78536\n",
      "global step 1400, epoch: 4, batch: 86, loss: 0.86716, acc: 0.78452\n",
      "global step 1410, epoch: 4, batch: 96, loss: 0.47250, acc: 0.78581\n",
      "global step 1420, epoch: 4, batch: 106, loss: 0.54471, acc: 0.78361\n",
      "global step 1430, epoch: 4, batch: 116, loss: 0.37628, acc: 0.78421\n",
      "global step 1440, epoch: 4, batch: 126, loss: 0.89840, acc: 0.78373\n",
      "global step 1450, epoch: 4, batch: 136, loss: 0.72107, acc: 0.78309\n",
      "global step 1460, epoch: 4, batch: 146, loss: 0.61188, acc: 0.78403\n",
      "global step 1470, epoch: 4, batch: 156, loss: 0.70891, acc: 0.78486\n",
      "global step 1480, epoch: 4, batch: 166, loss: 0.63209, acc: 0.78313\n",
      "global step 1490, epoch: 4, batch: 176, loss: 0.41881, acc: 0.78427\n",
      "global step 1500, epoch: 4, batch: 186, loss: 1.19734, acc: 0.78226\n",
      "global step 1510, epoch: 4, batch: 196, loss: 0.62829, acc: 0.78029\n",
      "global step 1520, epoch: 4, batch: 206, loss: 0.54262, acc: 0.77988\n",
      "global step 1530, epoch: 4, batch: 216, loss: 0.74553, acc: 0.77922\n",
      "global step 1540, epoch: 4, batch: 226, loss: 0.52858, acc: 0.78014\n",
      "global step 1550, epoch: 4, batch: 236, loss: 0.62127, acc: 0.78019\n",
      "global step 1560, epoch: 4, batch: 246, loss: 0.72305, acc: 0.78011\n",
      "global step 1570, epoch: 4, batch: 256, loss: 0.58786, acc: 0.77954\n",
      "global step 1580, epoch: 4, batch: 266, loss: 0.71431, acc: 0.78008\n",
      "global step 1590, epoch: 4, batch: 276, loss: 0.26088, acc: 0.78080\n",
      "global step 1600, epoch: 4, batch: 286, loss: 0.45469, acc: 0.78191\n",
      "global step 1610, epoch: 4, batch: 296, loss: 0.46883, acc: 0.78262\n",
      "global step 1620, epoch: 4, batch: 306, loss: 0.76811, acc: 0.78227\n",
      "global step 1630, epoch: 4, batch: 316, loss: 0.87765, acc: 0.78165\n",
      "global step 1640, epoch: 4, batch: 326, loss: 0.52464, acc: 0.78144\n",
      "global step 1650, epoch: 4, batch: 336, loss: 0.93694, acc: 0.78041\n",
      "global step 1660, epoch: 4, batch: 346, loss: 0.88670, acc: 0.77953\n",
      "global step 1670, epoch: 4, batch: 356, loss: 0.48683, acc: 0.77870\n",
      "global step 1680, epoch: 4, batch: 366, loss: 0.91459, acc: 0.77869\n",
      "global step 1690, epoch: 4, batch: 376, loss: 1.12977, acc: 0.77801\n",
      "global step 1700, epoch: 4, batch: 386, loss: 0.92751, acc: 0.77785\n",
      "global step 1710, epoch: 4, batch: 396, loss: 0.58953, acc: 0.77723\n",
      "global step 1720, epoch: 4, batch: 406, loss: 0.74404, acc: 0.77655\n",
      "global step 1730, epoch: 4, batch: 416, loss: 0.61591, acc: 0.77652\n",
      "global step 1740, epoch: 4, batch: 426, loss: 0.92700, acc: 0.77692\n",
      "global step 1750, epoch: 4, batch: 436, loss: 0.58570, acc: 0.77652\n",
      "eval loss: 1.00839, accu: 0.65467\n",
      "global step 1760, epoch: 5, batch: 8, loss: 0.37872, acc: 0.83203\n",
      "global step 1770, epoch: 5, batch: 18, loss: 0.41255, acc: 0.84201\n",
      "global step 1780, epoch: 5, batch: 28, loss: 0.52297, acc: 0.84263\n",
      "global step 1790, epoch: 5, batch: 38, loss: 0.59524, acc: 0.84293\n",
      "global step 1800, epoch: 5, batch: 48, loss: 0.55008, acc: 0.84245\n",
      "global step 1810, epoch: 5, batch: 58, loss: 0.33128, acc: 0.84106\n",
      "global step 1820, epoch: 5, batch: 68, loss: 0.58698, acc: 0.84099\n",
      "global step 1830, epoch: 5, batch: 78, loss: 0.53790, acc: 0.84575\n",
      "global step 1840, epoch: 5, batch: 88, loss: 0.55918, acc: 0.84553\n",
      "global step 1850, epoch: 5, batch: 98, loss: 0.50172, acc: 0.84407\n",
      "global step 1860, epoch: 5, batch: 108, loss: 0.26929, acc: 0.84751\n",
      "global step 1870, epoch: 5, batch: 118, loss: 0.50345, acc: 0.84772\n",
      "global step 1880, epoch: 5, batch: 128, loss: 0.53224, acc: 0.84424\n",
      "global step 1890, epoch: 5, batch: 138, loss: 0.53097, acc: 0.84511\n",
      "global step 1900, epoch: 5, batch: 148, loss: 0.58196, acc: 0.84375\n",
      "global step 1910, epoch: 5, batch: 158, loss: 0.40944, acc: 0.84316\n",
      "global step 1920, epoch: 5, batch: 168, loss: 0.58519, acc: 0.84263\n",
      "global step 1930, epoch: 5, batch: 178, loss: 0.14251, acc: 0.84252\n",
      "global step 1940, epoch: 5, batch: 188, loss: 0.56295, acc: 0.84176\n",
      "global step 1950, epoch: 5, batch: 198, loss: 0.55912, acc: 0.84217\n",
      "global step 1960, epoch: 5, batch: 208, loss: 0.51962, acc: 0.84135\n",
      "global step 1970, epoch: 5, batch: 218, loss: 0.35968, acc: 0.84131\n",
      "global step 1980, epoch: 5, batch: 228, loss: 0.55474, acc: 0.84101\n",
      "global step 1990, epoch: 5, batch: 238, loss: 0.74351, acc: 0.84139\n",
      "global step 2000, epoch: 5, batch: 248, loss: 0.39209, acc: 0.84136\n",
      "global step 2010, epoch: 5, batch: 258, loss: 0.28049, acc: 0.84096\n",
      "global step 2020, epoch: 5, batch: 268, loss: 0.32941, acc: 0.84060\n",
      "global step 2030, epoch: 5, batch: 278, loss: 0.70272, acc: 0.84004\n",
      "global step 2040, epoch: 5, batch: 288, loss: 0.28759, acc: 0.84028\n",
      "global step 2050, epoch: 5, batch: 298, loss: 0.24638, acc: 0.84081\n",
      "global step 2060, epoch: 5, batch: 308, loss: 0.55384, acc: 0.83979\n",
      "global step 2070, epoch: 5, batch: 318, loss: 0.47256, acc: 0.83864\n",
      "global step 2080, epoch: 5, batch: 328, loss: 0.34525, acc: 0.83851\n",
      "global step 2090, epoch: 5, batch: 338, loss: 0.48612, acc: 0.83774\n",
      "global step 2100, epoch: 5, batch: 348, loss: 0.60800, acc: 0.83657\n",
      "global step 2110, epoch: 5, batch: 358, loss: 0.65270, acc: 0.83650\n",
      "global step 2120, epoch: 5, batch: 368, loss: 0.43869, acc: 0.83713\n",
      "global step 2130, epoch: 5, batch: 378, loss: 0.28629, acc: 0.83664\n",
      "global step 2140, epoch: 5, batch: 388, loss: 0.21505, acc: 0.83698\n",
      "global step 2150, epoch: 5, batch: 398, loss: 0.53219, acc: 0.83770\n",
      "global step 2160, epoch: 5, batch: 408, loss: 0.76206, acc: 0.83739\n",
      "global step 2170, epoch: 5, batch: 418, loss: 0.47022, acc: 0.83560\n",
      "global step 2180, epoch: 5, batch: 428, loss: 0.33646, acc: 0.83462\n",
      "global step 2190, epoch: 5, batch: 438, loss: 0.51240, acc: 0.83400\n",
      "eval loss: 1.06123, accu: 0.66217\n",
      "global step 2200, epoch: 6, batch: 10, loss: 0.13137, acc: 0.87500\n",
      "global step 2210, epoch: 6, batch: 20, loss: 0.25604, acc: 0.88906\n",
      "global step 2220, epoch: 6, batch: 30, loss: 0.51661, acc: 0.88229\n",
      "global step 2230, epoch: 6, batch: 40, loss: 0.32296, acc: 0.88828\n",
      "global step 2240, epoch: 6, batch: 50, loss: 0.31810, acc: 0.89000\n",
      "global step 2250, epoch: 6, batch: 60, loss: 0.30283, acc: 0.89271\n",
      "global step 2260, epoch: 6, batch: 70, loss: 0.25721, acc: 0.88884\n",
      "global step 2270, epoch: 6, batch: 80, loss: 0.17491, acc: 0.89141\n",
      "global step 2280, epoch: 6, batch: 90, loss: 0.26231, acc: 0.89653\n",
      "global step 2290, epoch: 6, batch: 100, loss: 0.28115, acc: 0.89594\n",
      "global step 2300, epoch: 6, batch: 110, loss: 0.16032, acc: 0.89403\n",
      "global step 2310, epoch: 6, batch: 120, loss: 0.15986, acc: 0.89583\n",
      "global step 2320, epoch: 6, batch: 130, loss: 0.30249, acc: 0.89591\n",
      "global step 2330, epoch: 6, batch: 140, loss: 0.19086, acc: 0.89598\n",
      "global step 2340, epoch: 6, batch: 150, loss: 0.20029, acc: 0.89667\n",
      "global step 2350, epoch: 6, batch: 160, loss: 0.51890, acc: 0.89707\n",
      "global step 2360, epoch: 6, batch: 170, loss: 0.12135, acc: 0.89743\n",
      "global step 2370, epoch: 6, batch: 180, loss: 0.15060, acc: 0.89792\n",
      "global step 2380, epoch: 6, batch: 190, loss: 0.15322, acc: 0.89803\n",
      "global step 2390, epoch: 6, batch: 200, loss: 0.21404, acc: 0.89828\n",
      "global step 2400, epoch: 6, batch: 210, loss: 0.42600, acc: 0.89821\n",
      "global step 2410, epoch: 6, batch: 220, loss: 0.46580, acc: 0.89716\n",
      "global step 2420, epoch: 6, batch: 230, loss: 0.12491, acc: 0.89701\n",
      "global step 2430, epoch: 6, batch: 240, loss: 0.28792, acc: 0.89609\n",
      "global step 2440, epoch: 6, batch: 250, loss: 0.36344, acc: 0.89675\n",
      "global step 2450, epoch: 6, batch: 260, loss: 0.43261, acc: 0.89483\n",
      "global step 2460, epoch: 6, batch: 270, loss: 0.49157, acc: 0.89502\n",
      "global step 2470, epoch: 6, batch: 280, loss: 0.29463, acc: 0.89442\n",
      "global step 2480, epoch: 6, batch: 290, loss: 0.28815, acc: 0.89332\n",
      "global step 2490, epoch: 6, batch: 300, loss: 0.23942, acc: 0.89260\n",
      "global step 2500, epoch: 6, batch: 310, loss: 0.44987, acc: 0.89224\n",
      "global step 2510, epoch: 6, batch: 320, loss: 0.51610, acc: 0.89189\n",
      "global step 2520, epoch: 6, batch: 330, loss: 0.16871, acc: 0.89176\n",
      "global step 2530, epoch: 6, batch: 340, loss: 0.53954, acc: 0.89099\n",
      "global step 2540, epoch: 6, batch: 350, loss: 0.19695, acc: 0.89062\n",
      "global step 2550, epoch: 6, batch: 360, loss: 0.24171, acc: 0.89036\n",
      "global step 2560, epoch: 6, batch: 370, loss: 0.41650, acc: 0.88978\n",
      "global step 2570, epoch: 6, batch: 380, loss: 0.37432, acc: 0.88873\n",
      "global step 2580, epoch: 6, batch: 390, loss: 0.27053, acc: 0.88846\n",
      "global step 2590, epoch: 6, batch: 400, loss: 0.50216, acc: 0.88852\n",
      "global step 2600, epoch: 6, batch: 410, loss: 0.41160, acc: 0.88849\n",
      "global step 2610, epoch: 6, batch: 420, loss: 0.26430, acc: 0.88854\n",
      "global step 2620, epoch: 6, batch: 430, loss: 0.22175, acc: 0.88808\n",
      "eval loss: 1.20032, accu: 0.66183\n",
      "global step 2630, epoch: 7, batch: 2, loss: 0.27945, acc: 0.93750\n",
      "global step 2640, epoch: 7, batch: 12, loss: 0.24883, acc: 0.92448\n",
      "global step 2650, epoch: 7, batch: 22, loss: 0.09888, acc: 0.93040\n",
      "global step 2660, epoch: 7, batch: 32, loss: 0.31683, acc: 0.93164\n",
      "global step 2670, epoch: 7, batch: 42, loss: 0.19626, acc: 0.93527\n",
      "global step 2680, epoch: 7, batch: 52, loss: 0.40268, acc: 0.93510\n",
      "global step 2690, epoch: 7, batch: 62, loss: 0.38917, acc: 0.93599\n",
      "global step 2700, epoch: 7, batch: 72, loss: 0.21819, acc: 0.94010\n",
      "global step 2710, epoch: 7, batch: 82, loss: 0.11917, acc: 0.94207\n",
      "global step 2720, epoch: 7, batch: 92, loss: 0.10058, acc: 0.93886\n",
      "global step 2730, epoch: 7, batch: 102, loss: 0.26779, acc: 0.94056\n",
      "global step 2740, epoch: 7, batch: 112, loss: 0.17390, acc: 0.94196\n",
      "global step 2750, epoch: 7, batch: 122, loss: 0.05861, acc: 0.94160\n",
      "global step 2760, epoch: 7, batch: 132, loss: 0.13800, acc: 0.94247\n",
      "global step 2770, epoch: 7, batch: 142, loss: 0.10981, acc: 0.94212\n",
      "global step 2780, epoch: 7, batch: 152, loss: 0.12468, acc: 0.94202\n",
      "global step 2790, epoch: 7, batch: 162, loss: 0.10423, acc: 0.94194\n",
      "global step 2800, epoch: 7, batch: 172, loss: 0.16154, acc: 0.94222\n",
      "global step 2810, epoch: 7, batch: 182, loss: 0.09035, acc: 0.94196\n",
      "global step 2820, epoch: 7, batch: 192, loss: 0.14215, acc: 0.94076\n",
      "global step 2830, epoch: 7, batch: 202, loss: 0.39130, acc: 0.94028\n",
      "global step 2840, epoch: 7, batch: 212, loss: 0.32342, acc: 0.94001\n",
      "global step 2850, epoch: 7, batch: 222, loss: 0.11473, acc: 0.94003\n",
      "global step 2860, epoch: 7, batch: 232, loss: 0.38287, acc: 0.94060\n",
      "global step 2870, epoch: 7, batch: 242, loss: 0.27888, acc: 0.93995\n",
      "global step 2880, epoch: 7, batch: 252, loss: 0.44242, acc: 0.93874\n",
      "global step 2890, epoch: 7, batch: 262, loss: 0.29468, acc: 0.93810\n",
      "global step 2900, epoch: 7, batch: 272, loss: 0.35384, acc: 0.93773\n",
      "global step 2910, epoch: 7, batch: 282, loss: 0.18310, acc: 0.93750\n",
      "global step 2920, epoch: 7, batch: 292, loss: 0.22617, acc: 0.93804\n",
      "global step 2930, epoch: 7, batch: 302, loss: 0.09282, acc: 0.93781\n",
      "global step 2940, epoch: 7, batch: 312, loss: 0.35242, acc: 0.93780\n",
      "global step 2950, epoch: 7, batch: 322, loss: 0.08949, acc: 0.93740\n",
      "global step 2960, epoch: 7, batch: 332, loss: 0.15226, acc: 0.93712\n",
      "global step 2970, epoch: 7, batch: 342, loss: 0.23471, acc: 0.93723\n",
      "global step 2980, epoch: 7, batch: 352, loss: 0.24770, acc: 0.93679\n",
      "global step 2990, epoch: 7, batch: 362, loss: 0.17038, acc: 0.93733\n",
      "global step 3000, epoch: 7, batch: 372, loss: 0.25891, acc: 0.93691\n",
      "global step 3010, epoch: 7, batch: 382, loss: 0.11054, acc: 0.93635\n",
      "global step 3020, epoch: 7, batch: 392, loss: 0.15982, acc: 0.93614\n",
      "global step 3030, epoch: 7, batch: 402, loss: 0.25468, acc: 0.93517\n",
      "global step 3040, epoch: 7, batch: 412, loss: 0.19463, acc: 0.93507\n",
      "global step 3050, epoch: 7, batch: 422, loss: 0.23152, acc: 0.93506\n",
      "global step 3060, epoch: 7, batch: 432, loss: 0.10167, acc: 0.93461\n",
      "eval loss: 1.41623, accu: 0.63567\n",
      "global step 3070, epoch: 8, batch: 4, loss: 0.10343, acc: 0.96875\n",
      "global step 3080, epoch: 8, batch: 14, loss: 0.13209, acc: 0.96652\n",
      "global step 3090, epoch: 8, batch: 24, loss: 0.05558, acc: 0.96484\n",
      "global step 3100, epoch: 8, batch: 34, loss: 0.25890, acc: 0.96140\n",
      "global step 3110, epoch: 8, batch: 44, loss: 0.05489, acc: 0.96449\n",
      "global step 3120, epoch: 8, batch: 54, loss: 0.11130, acc: 0.96296\n",
      "global step 3130, epoch: 8, batch: 64, loss: 0.12568, acc: 0.96533\n",
      "global step 3140, epoch: 8, batch: 74, loss: 0.09066, acc: 0.96622\n",
      "global step 3150, epoch: 8, batch: 84, loss: 0.03949, acc: 0.96763\n",
      "global step 3160, epoch: 8, batch: 94, loss: 0.10254, acc: 0.96709\n",
      "global step 3170, epoch: 8, batch: 104, loss: 0.02422, acc: 0.96815\n",
      "global step 3180, epoch: 8, batch: 114, loss: 0.17006, acc: 0.96902\n",
      "global step 3190, epoch: 8, batch: 124, loss: 0.03187, acc: 0.96951\n",
      "global step 3200, epoch: 8, batch: 134, loss: 0.20226, acc: 0.96875\n",
      "global step 3210, epoch: 8, batch: 144, loss: 0.07277, acc: 0.96788\n",
      "global step 3220, epoch: 8, batch: 154, loss: 0.13850, acc: 0.96774\n",
      "global step 3230, epoch: 8, batch: 164, loss: 0.27277, acc: 0.96704\n",
      "global step 3240, epoch: 8, batch: 174, loss: 0.10215, acc: 0.96659\n",
      "global step 3250, epoch: 8, batch: 184, loss: 0.01611, acc: 0.96637\n",
      "global step 3260, epoch: 8, batch: 194, loss: 0.14522, acc: 0.96553\n",
      "global step 3270, epoch: 8, batch: 204, loss: 0.15259, acc: 0.96584\n",
      "global step 3280, epoch: 8, batch: 214, loss: 0.05651, acc: 0.96612\n",
      "global step 3290, epoch: 8, batch: 224, loss: 0.07224, acc: 0.96554\n",
      "global step 3300, epoch: 8, batch: 234, loss: 0.24976, acc: 0.96528\n",
      "global step 3310, epoch: 8, batch: 244, loss: 0.13881, acc: 0.96440\n",
      "global step 3320, epoch: 8, batch: 254, loss: 0.03032, acc: 0.96518\n",
      "global step 3330, epoch: 8, batch: 264, loss: 0.22717, acc: 0.96390\n",
      "global step 3340, epoch: 8, batch: 274, loss: 0.17403, acc: 0.96339\n",
      "global step 3350, epoch: 8, batch: 284, loss: 0.09210, acc: 0.96314\n",
      "global step 3360, epoch: 8, batch: 294, loss: 0.03229, acc: 0.96259\n",
      "global step 3370, epoch: 8, batch: 304, loss: 0.09443, acc: 0.96166\n",
      "global step 3380, epoch: 8, batch: 314, loss: 0.21334, acc: 0.96139\n",
      "global step 3390, epoch: 8, batch: 324, loss: 0.21269, acc: 0.96123\n",
      "global step 3400, epoch: 8, batch: 334, loss: 0.04203, acc: 0.96098\n",
      "global step 3410, epoch: 8, batch: 344, loss: 0.09103, acc: 0.96103\n",
      "global step 3420, epoch: 8, batch: 354, loss: 0.10354, acc: 0.96054\n",
      "global step 3430, epoch: 8, batch: 364, loss: 0.43847, acc: 0.96025\n",
      "global step 3440, epoch: 8, batch: 374, loss: 0.06620, acc: 0.95981\n",
      "global step 3450, epoch: 8, batch: 384, loss: 0.16717, acc: 0.96004\n",
      "global step 3460, epoch: 8, batch: 394, loss: 0.04820, acc: 0.96003\n",
      "global step 3470, epoch: 8, batch: 404, loss: 0.26504, acc: 0.95993\n",
      "global step 3480, epoch: 8, batch: 414, loss: 0.12804, acc: 0.95947\n",
      "global step 3490, epoch: 8, batch: 424, loss: 0.12830, acc: 0.95932\n",
      "global step 3500, epoch: 8, batch: 434, loss: 0.05107, acc: 0.95910\n",
      "eval loss: 1.56207, accu: 0.64083\n",
      "global step 3510, epoch: 9, batch: 6, loss: 0.02300, acc: 0.97396\n",
      "global step 3520, epoch: 9, batch: 16, loss: 0.02764, acc: 0.97656\n",
      "global step 3530, epoch: 9, batch: 26, loss: 0.04639, acc: 0.97957\n",
      "global step 3540, epoch: 9, batch: 36, loss: 0.06240, acc: 0.97569\n",
      "global step 3550, epoch: 9, batch: 46, loss: 0.02928, acc: 0.97690\n",
      "global step 3560, epoch: 9, batch: 56, loss: 0.12454, acc: 0.97600\n",
      "global step 3570, epoch: 9, batch: 66, loss: 0.02482, acc: 0.97680\n",
      "global step 3580, epoch: 9, batch: 76, loss: 0.19892, acc: 0.97738\n",
      "global step 3590, epoch: 9, batch: 86, loss: 0.03078, acc: 0.97965\n",
      "global step 3600, epoch: 9, batch: 96, loss: 0.14297, acc: 0.97917\n",
      "global step 3610, epoch: 9, batch: 106, loss: 0.04257, acc: 0.97877\n",
      "global step 3620, epoch: 9, batch: 116, loss: 0.03472, acc: 0.97926\n",
      "global step 3630, epoch: 9, batch: 126, loss: 0.07462, acc: 0.98041\n",
      "global step 3640, epoch: 9, batch: 136, loss: 0.10109, acc: 0.97932\n",
      "global step 3650, epoch: 9, batch: 146, loss: 0.07837, acc: 0.97988\n",
      "global step 3660, epoch: 9, batch: 156, loss: 0.03181, acc: 0.98037\n",
      "global step 3670, epoch: 9, batch: 166, loss: 0.02439, acc: 0.98023\n",
      "global step 3680, epoch: 9, batch: 176, loss: 0.02161, acc: 0.98065\n",
      "global step 3690, epoch: 9, batch: 186, loss: 0.02712, acc: 0.98034\n",
      "global step 3700, epoch: 9, batch: 196, loss: 0.08949, acc: 0.97991\n",
      "global step 3710, epoch: 9, batch: 206, loss: 0.01280, acc: 0.97937\n",
      "global step 3720, epoch: 9, batch: 216, loss: 0.14708, acc: 0.97873\n",
      "global step 3730, epoch: 9, batch: 226, loss: 0.03520, acc: 0.97857\n",
      "global step 3740, epoch: 9, batch: 236, loss: 0.07981, acc: 0.97881\n",
      "global step 3750, epoch: 9, batch: 246, loss: 0.04632, acc: 0.97853\n",
      "global step 3760, epoch: 9, batch: 256, loss: 0.06860, acc: 0.97864\n",
      "global step 3770, epoch: 9, batch: 266, loss: 0.35114, acc: 0.97803\n",
      "global step 3780, epoch: 9, batch: 276, loss: 0.07547, acc: 0.97792\n",
      "global step 3790, epoch: 9, batch: 286, loss: 0.01797, acc: 0.97738\n",
      "global step 3800, epoch: 9, batch: 296, loss: 0.15885, acc: 0.97720\n",
      "global step 3810, epoch: 9, batch: 306, loss: 0.01564, acc: 0.97672\n",
      "global step 3820, epoch: 9, batch: 316, loss: 0.02324, acc: 0.97676\n",
      "global step 3830, epoch: 9, batch: 326, loss: 0.01509, acc: 0.97680\n",
      "global step 3840, epoch: 9, batch: 336, loss: 0.03949, acc: 0.97703\n",
      "global step 3850, epoch: 9, batch: 346, loss: 0.32695, acc: 0.97661\n",
      "global step 3860, epoch: 9, batch: 356, loss: 0.16858, acc: 0.97639\n",
      "global step 3870, epoch: 9, batch: 366, loss: 0.10232, acc: 0.97626\n",
      "global step 3880, epoch: 9, batch: 376, loss: 0.02348, acc: 0.97656\n",
      "global step 3890, epoch: 9, batch: 386, loss: 0.07112, acc: 0.97685\n",
      "global step 3900, epoch: 9, batch: 396, loss: 0.02658, acc: 0.97664\n",
      "global step 3910, epoch: 9, batch: 406, loss: 0.04195, acc: 0.97629\n",
      "global step 3920, epoch: 9, batch: 416, loss: 0.10577, acc: 0.97589\n",
      "global step 3930, epoch: 9, batch: 426, loss: 0.06741, acc: 0.97565\n",
      "global step 3940, epoch: 9, batch: 436, loss: 0.03110, acc: 0.97563\n",
      "eval loss: 1.70682, accu: 0.63933\n",
      "global step 3950, epoch: 10, batch: 8, loss: 0.01985, acc: 0.96875\n",
      "global step 3960, epoch: 10, batch: 18, loss: 0.00614, acc: 0.97396\n",
      "global step 3970, epoch: 10, batch: 28, loss: 0.02473, acc: 0.97656\n",
      "global step 3980, epoch: 10, batch: 38, loss: 0.13471, acc: 0.97697\n",
      "global step 3990, epoch: 10, batch: 48, loss: 0.00898, acc: 0.97917\n",
      "global step 4000, epoch: 10, batch: 58, loss: 0.02343, acc: 0.98060\n",
      "global step 4010, epoch: 10, batch: 68, loss: 0.04902, acc: 0.98162\n",
      "global step 4020, epoch: 10, batch: 78, loss: 0.07251, acc: 0.98277\n",
      "global step 4030, epoch: 10, batch: 88, loss: 0.05898, acc: 0.98366\n",
      "global step 4040, epoch: 10, batch: 98, loss: 0.23148, acc: 0.98310\n",
      "global step 4050, epoch: 10, batch: 108, loss: 0.01550, acc: 0.98380\n",
      "global step 4060, epoch: 10, batch: 118, loss: 0.13262, acc: 0.98385\n",
      "global step 4070, epoch: 10, batch: 128, loss: 0.04173, acc: 0.98413\n",
      "global step 4080, epoch: 10, batch: 138, loss: 0.05973, acc: 0.98438\n",
      "global step 4090, epoch: 10, batch: 148, loss: 0.02238, acc: 0.98311\n",
      "global step 4100, epoch: 10, batch: 158, loss: 0.00461, acc: 0.98339\n",
      "global step 4110, epoch: 10, batch: 168, loss: 0.15709, acc: 0.98307\n",
      "global step 4120, epoch: 10, batch: 178, loss: 0.08249, acc: 0.98297\n",
      "global step 4130, epoch: 10, batch: 188, loss: 0.01355, acc: 0.98305\n",
      "global step 4140, epoch: 10, batch: 198, loss: 0.01637, acc: 0.98264\n",
      "global step 4150, epoch: 10, batch: 208, loss: 0.04179, acc: 0.98197\n",
      "global step 4160, epoch: 10, batch: 218, loss: 0.02313, acc: 0.98237\n",
      "global step 4170, epoch: 10, batch: 228, loss: 0.01815, acc: 0.98273\n",
      "global step 4180, epoch: 10, batch: 238, loss: 0.07829, acc: 0.98241\n",
      "global step 4190, epoch: 10, batch: 248, loss: 0.01604, acc: 0.98223\n",
      "global step 4200, epoch: 10, batch: 258, loss: 0.05720, acc: 0.98256\n",
      "global step 4210, epoch: 10, batch: 268, loss: 0.00845, acc: 0.98274\n",
      "global step 4220, epoch: 10, batch: 278, loss: 0.03740, acc: 0.98258\n",
      "global step 4230, epoch: 10, batch: 288, loss: 0.02009, acc: 0.98242\n",
      "global step 4240, epoch: 10, batch: 298, loss: 0.03345, acc: 0.98186\n",
      "global step 4250, epoch: 10, batch: 308, loss: 0.07275, acc: 0.98164\n",
      "global step 4260, epoch: 10, batch: 318, loss: 0.01994, acc: 0.98172\n",
      "global step 4270, epoch: 10, batch: 328, loss: 0.01070, acc: 0.98218\n",
      "global step 4280, epoch: 10, batch: 338, loss: 0.01554, acc: 0.98206\n",
      "global step 4290, epoch: 10, batch: 348, loss: 0.09596, acc: 0.98177\n",
      "global step 4300, epoch: 10, batch: 358, loss: 0.02409, acc: 0.98149\n",
      "global step 4310, epoch: 10, batch: 368, loss: 0.18834, acc: 0.98166\n",
      "global step 4320, epoch: 10, batch: 378, loss: 0.16289, acc: 0.98198\n",
      "global step 4330, epoch: 10, batch: 388, loss: 0.01404, acc: 0.98212\n",
      "global step 4340, epoch: 10, batch: 398, loss: 0.04001, acc: 0.98210\n",
      "global step 4350, epoch: 10, batch: 408, loss: 0.05205, acc: 0.98208\n",
      "global step 4360, epoch: 10, batch: 418, loss: 0.02548, acc: 0.98228\n",
      "global step 4370, epoch: 10, batch: 428, loss: 0.01866, acc: 0.98233\n",
      "global step 4380, epoch: 10, batch: 438, loss: 0.00614, acc: 0.98264\n",
      "eval loss: 1.78486, accu: 0.65350\n",
      "global step 4390, epoch: 11, batch: 10, loss: 0.02335, acc: 0.98750\n",
      "global step 4400, epoch: 11, batch: 20, loss: 0.00718, acc: 0.98438\n",
      "global step 4410, epoch: 11, batch: 30, loss: 0.00529, acc: 0.98646\n",
      "global step 4420, epoch: 11, batch: 40, loss: 0.00509, acc: 0.98672\n",
      "global step 4430, epoch: 11, batch: 50, loss: 0.01362, acc: 0.98813\n",
      "global step 4440, epoch: 11, batch: 60, loss: 0.08586, acc: 0.98906\n",
      "global step 4450, epoch: 11, batch: 70, loss: 0.01541, acc: 0.99018\n",
      "global step 4460, epoch: 11, batch: 80, loss: 0.01082, acc: 0.99023\n",
      "global step 4470, epoch: 11, batch: 90, loss: 0.00869, acc: 0.99062\n",
      "global step 4480, epoch: 11, batch: 100, loss: 0.01606, acc: 0.99094\n",
      "global step 4490, epoch: 11, batch: 110, loss: 0.04603, acc: 0.99148\n",
      "global step 4500, epoch: 11, batch: 120, loss: 0.10246, acc: 0.99062\n",
      "global step 4510, epoch: 11, batch: 130, loss: 0.02295, acc: 0.99062\n",
      "global step 4520, epoch: 11, batch: 140, loss: 0.11906, acc: 0.99062\n",
      "global step 4530, epoch: 11, batch: 150, loss: 0.02423, acc: 0.99104\n",
      "global step 4540, epoch: 11, batch: 160, loss: 0.01355, acc: 0.99082\n",
      "global step 4550, epoch: 11, batch: 170, loss: 0.01070, acc: 0.99062\n",
      "global step 4560, epoch: 11, batch: 180, loss: 0.06668, acc: 0.99028\n",
      "global step 4570, epoch: 11, batch: 190, loss: 0.05400, acc: 0.98997\n",
      "global step 4580, epoch: 11, batch: 200, loss: 0.01314, acc: 0.99016\n",
      "global step 4590, epoch: 11, batch: 210, loss: 0.04566, acc: 0.99003\n",
      "global step 4600, epoch: 11, batch: 220, loss: 0.01312, acc: 0.98977\n",
      "global step 4610, epoch: 11, batch: 230, loss: 0.02733, acc: 0.98954\n",
      "global step 4620, epoch: 11, batch: 240, loss: 0.02522, acc: 0.98854\n",
      "global step 4630, epoch: 11, batch: 250, loss: 0.02325, acc: 0.98800\n",
      "global step 4640, epoch: 11, batch: 260, loss: 0.00584, acc: 0.98810\n",
      "global step 4650, epoch: 11, batch: 270, loss: 0.00525, acc: 0.98785\n",
      "global step 4660, epoch: 11, batch: 280, loss: 0.01001, acc: 0.98817\n",
      "global step 4670, epoch: 11, batch: 290, loss: 0.11927, acc: 0.98793\n",
      "global step 4680, epoch: 11, batch: 300, loss: 0.02528, acc: 0.98740\n",
      "global step 4690, epoch: 11, batch: 310, loss: 0.02431, acc: 0.98720\n",
      "global step 4700, epoch: 11, batch: 320, loss: 0.01249, acc: 0.98721\n",
      "global step 4710, epoch: 11, batch: 330, loss: 0.05050, acc: 0.98731\n",
      "global step 4720, epoch: 11, batch: 340, loss: 0.05470, acc: 0.98741\n",
      "global step 4730, epoch: 11, batch: 350, loss: 0.03353, acc: 0.98750\n",
      "global step 4740, epoch: 11, batch: 360, loss: 0.01783, acc: 0.98759\n",
      "global step 4750, epoch: 11, batch: 370, loss: 0.00462, acc: 0.98784\n",
      "global step 4760, epoch: 11, batch: 380, loss: 0.01186, acc: 0.98791\n",
      "global step 4770, epoch: 11, batch: 390, loss: 0.12434, acc: 0.98758\n",
      "global step 4780, epoch: 11, batch: 400, loss: 0.05882, acc: 0.98750\n",
      "global step 4790, epoch: 11, batch: 410, loss: 0.01117, acc: 0.98750\n",
      "global step 4800, epoch: 11, batch: 420, loss: 0.12590, acc: 0.98750\n",
      "global step 4810, epoch: 11, batch: 430, loss: 0.02167, acc: 0.98750\n",
      "eval loss: 1.92616, accu: 0.65433\n",
      "global step 4820, epoch: 12, batch: 2, loss: 0.00239, acc: 0.98438\n",
      "global step 4830, epoch: 12, batch: 12, loss: 0.05793, acc: 0.98438\n",
      "global step 4840, epoch: 12, batch: 22, loss: 0.00863, acc: 0.98438\n",
      "global step 4850, epoch: 12, batch: 32, loss: 0.00842, acc: 0.98340\n",
      "global step 4860, epoch: 12, batch: 42, loss: 0.03051, acc: 0.98661\n",
      "global step 4870, epoch: 12, batch: 52, loss: 0.01278, acc: 0.98858\n",
      "global step 4880, epoch: 12, batch: 62, loss: 0.00615, acc: 0.98992\n",
      "global step 4890, epoch: 12, batch: 72, loss: 0.00802, acc: 0.99045\n",
      "global step 4900, epoch: 12, batch: 82, loss: 0.01651, acc: 0.99009\n",
      "global step 4910, epoch: 12, batch: 92, loss: 0.01050, acc: 0.98981\n",
      "global step 4920, epoch: 12, batch: 102, loss: 0.00425, acc: 0.99020\n",
      "global step 4930, epoch: 12, batch: 112, loss: 0.00432, acc: 0.99023\n",
      "global step 4940, epoch: 12, batch: 122, loss: 0.00408, acc: 0.99052\n",
      "global step 4950, epoch: 12, batch: 132, loss: 0.02492, acc: 0.99077\n",
      "global step 4960, epoch: 12, batch: 142, loss: 0.00405, acc: 0.99076\n",
      "global step 4970, epoch: 12, batch: 152, loss: 0.00567, acc: 0.99075\n",
      "global step 4980, epoch: 12, batch: 162, loss: 0.00575, acc: 0.99074\n",
      "global step 4990, epoch: 12, batch: 172, loss: 0.08567, acc: 0.99073\n",
      "global step 5000, epoch: 12, batch: 182, loss: 0.00336, acc: 0.99056\n",
      "global step 5010, epoch: 12, batch: 192, loss: 0.00243, acc: 0.99089\n",
      "global step 5020, epoch: 12, batch: 202, loss: 0.01492, acc: 0.99103\n",
      "global step 5030, epoch: 12, batch: 212, loss: 0.00843, acc: 0.99130\n",
      "global step 5040, epoch: 12, batch: 222, loss: 0.00515, acc: 0.99127\n",
      "global step 5050, epoch: 12, batch: 232, loss: 0.01179, acc: 0.99111\n",
      "global step 5060, epoch: 12, batch: 242, loss: 0.01345, acc: 0.99096\n",
      "global step 5070, epoch: 12, batch: 252, loss: 0.01905, acc: 0.99107\n",
      "global step 5080, epoch: 12, batch: 262, loss: 0.01188, acc: 0.99117\n",
      "global step 5090, epoch: 12, batch: 272, loss: 0.07198, acc: 0.99138\n",
      "global step 5100, epoch: 12, batch: 282, loss: 0.03033, acc: 0.99125\n",
      "global step 5110, epoch: 12, batch: 292, loss: 0.00995, acc: 0.99122\n",
      "global step 5120, epoch: 12, batch: 302, loss: 0.00911, acc: 0.99120\n",
      "global step 5130, epoch: 12, batch: 312, loss: 0.01424, acc: 0.99149\n",
      "global step 5140, epoch: 12, batch: 322, loss: 0.07584, acc: 0.99156\n",
      "global step 5150, epoch: 12, batch: 332, loss: 0.04147, acc: 0.99143\n",
      "global step 5160, epoch: 12, batch: 342, loss: 0.03189, acc: 0.99150\n",
      "global step 5170, epoch: 12, batch: 352, loss: 0.00367, acc: 0.99157\n",
      "global step 5180, epoch: 12, batch: 362, loss: 0.00245, acc: 0.99171\n",
      "global step 5190, epoch: 12, batch: 372, loss: 0.01136, acc: 0.99168\n",
      "global step 5200, epoch: 12, batch: 382, loss: 0.03258, acc: 0.99149\n",
      "global step 5210, epoch: 12, batch: 392, loss: 0.15671, acc: 0.99147\n",
      "global step 5220, epoch: 12, batch: 402, loss: 0.02546, acc: 0.99160\n",
      "global step 5230, epoch: 12, batch: 412, loss: 0.00742, acc: 0.99166\n",
      "global step 5240, epoch: 12, batch: 422, loss: 0.00587, acc: 0.99171\n",
      "global step 5250, epoch: 12, batch: 432, loss: 0.01853, acc: 0.99161\n",
      "eval loss: 2.01732, accu: 0.64767\n",
      "global step 5260, epoch: 13, batch: 4, loss: 0.02405, acc: 1.00000\n",
      "global step 5270, epoch: 13, batch: 14, loss: 0.00914, acc: 0.99777\n",
      "global step 5280, epoch: 13, batch: 24, loss: 0.00229, acc: 0.99479\n",
      "global step 5290, epoch: 13, batch: 34, loss: 0.01191, acc: 0.99632\n",
      "global step 5300, epoch: 13, batch: 44, loss: 0.00463, acc: 0.99574\n",
      "global step 5310, epoch: 13, batch: 54, loss: 0.00212, acc: 0.99653\n",
      "global step 5320, epoch: 13, batch: 64, loss: 0.02597, acc: 0.99609\n",
      "global step 5330, epoch: 13, batch: 74, loss: 0.08362, acc: 0.99535\n",
      "global step 5340, epoch: 13, batch: 84, loss: 0.01105, acc: 0.99591\n",
      "global step 5350, epoch: 13, batch: 94, loss: 0.12199, acc: 0.99568\n",
      "global step 5360, epoch: 13, batch: 104, loss: 0.01276, acc: 0.99549\n",
      "global step 5370, epoch: 13, batch: 114, loss: 0.09492, acc: 0.99507\n",
      "global step 5380, epoch: 13, batch: 124, loss: 0.01022, acc: 0.99496\n",
      "global step 5390, epoch: 13, batch: 134, loss: 0.00199, acc: 0.99510\n",
      "global step 5400, epoch: 13, batch: 144, loss: 0.01800, acc: 0.99479\n",
      "global step 5410, epoch: 13, batch: 154, loss: 0.01261, acc: 0.99493\n",
      "global step 5420, epoch: 13, batch: 164, loss: 0.09250, acc: 0.99466\n",
      "global step 5430, epoch: 13, batch: 174, loss: 0.00656, acc: 0.99461\n",
      "global step 5440, epoch: 13, batch: 184, loss: 0.00591, acc: 0.99440\n",
      "global step 5450, epoch: 13, batch: 194, loss: 0.02318, acc: 0.99452\n",
      "global step 5460, epoch: 13, batch: 204, loss: 0.00892, acc: 0.99479\n",
      "global step 5470, epoch: 13, batch: 214, loss: 0.00507, acc: 0.99489\n",
      "global step 5480, epoch: 13, batch: 224, loss: 0.00302, acc: 0.99512\n",
      "global step 5490, epoch: 13, batch: 234, loss: 0.00633, acc: 0.99493\n",
      "global step 5500, epoch: 13, batch: 244, loss: 0.00518, acc: 0.99488\n",
      "global step 5510, epoch: 13, batch: 254, loss: 0.00553, acc: 0.99496\n",
      "global step 5520, epoch: 13, batch: 264, loss: 0.00643, acc: 0.99503\n",
      "global step 5530, epoch: 13, batch: 274, loss: 0.00914, acc: 0.99498\n",
      "global step 5540, epoch: 13, batch: 284, loss: 0.00577, acc: 0.99494\n",
      "global step 5550, epoch: 13, batch: 294, loss: 0.02554, acc: 0.99500\n",
      "global step 5560, epoch: 13, batch: 304, loss: 0.00400, acc: 0.99496\n",
      "global step 5570, epoch: 13, batch: 314, loss: 0.00659, acc: 0.99512\n",
      "global step 5580, epoch: 13, batch: 324, loss: 0.00631, acc: 0.99508\n",
      "global step 5590, epoch: 13, batch: 334, loss: 0.00348, acc: 0.99504\n",
      "global step 5600, epoch: 13, batch: 344, loss: 0.01505, acc: 0.99519\n",
      "global step 5610, epoch: 13, batch: 354, loss: 0.00492, acc: 0.99514\n",
      "global step 5620, epoch: 13, batch: 364, loss: 0.01024, acc: 0.99519\n",
      "global step 5630, epoch: 13, batch: 374, loss: 0.02578, acc: 0.99532\n",
      "global step 5640, epoch: 13, batch: 384, loss: 0.02332, acc: 0.99528\n",
      "global step 5650, epoch: 13, batch: 394, loss: 0.04971, acc: 0.99516\n",
      "global step 5660, epoch: 13, batch: 404, loss: 0.03311, acc: 0.99513\n",
      "global step 5670, epoch: 13, batch: 414, loss: 0.00256, acc: 0.99509\n",
      "global step 5680, epoch: 13, batch: 424, loss: 0.00811, acc: 0.99514\n",
      "global step 5690, epoch: 13, batch: 434, loss: 0.06167, acc: 0.99489\n",
      "eval loss: 2.09245, accu: 0.65067\n",
      "global step 5700, epoch: 14, batch: 6, loss: 0.01131, acc: 0.98438\n",
      "global step 5710, epoch: 14, batch: 16, loss: 0.00529, acc: 0.98828\n",
      "global step 5720, epoch: 14, batch: 26, loss: 0.00958, acc: 0.99159\n",
      "global step 5730, epoch: 14, batch: 36, loss: 0.13279, acc: 0.99219\n",
      "global step 5740, epoch: 14, batch: 46, loss: 0.00234, acc: 0.99321\n",
      "global step 5750, epoch: 14, batch: 56, loss: 0.00429, acc: 0.99386\n",
      "global step 5760, epoch: 14, batch: 66, loss: 0.04987, acc: 0.99432\n",
      "global step 5770, epoch: 14, batch: 76, loss: 0.01580, acc: 0.99507\n",
      "global step 5780, epoch: 14, batch: 86, loss: 0.00597, acc: 0.99528\n",
      "global step 5790, epoch: 14, batch: 96, loss: 0.01043, acc: 0.99544\n",
      "global step 5800, epoch: 14, batch: 106, loss: 0.00275, acc: 0.99587\n",
      "global step 5810, epoch: 14, batch: 116, loss: 0.00120, acc: 0.99569\n",
      "global step 5820, epoch: 14, batch: 126, loss: 0.00853, acc: 0.99603\n",
      "global step 5830, epoch: 14, batch: 136, loss: 0.00255, acc: 0.99586\n",
      "global step 5840, epoch: 14, batch: 146, loss: 0.00801, acc: 0.99615\n",
      "global step 5850, epoch: 14, batch: 156, loss: 0.00169, acc: 0.99639\n",
      "global step 5860, epoch: 14, batch: 166, loss: 0.00226, acc: 0.99623\n",
      "global step 5870, epoch: 14, batch: 176, loss: 0.07412, acc: 0.99592\n",
      "global step 5880, epoch: 14, batch: 186, loss: 0.00430, acc: 0.99597\n",
      "global step 5890, epoch: 14, batch: 196, loss: 0.00548, acc: 0.99601\n",
      "global step 5900, epoch: 14, batch: 206, loss: 0.00421, acc: 0.99621\n",
      "global step 5910, epoch: 14, batch: 216, loss: 0.00268, acc: 0.99638\n",
      "global step 5920, epoch: 14, batch: 226, loss: 0.00155, acc: 0.99654\n",
      "global step 5930, epoch: 14, batch: 236, loss: 0.01468, acc: 0.99669\n",
      "global step 5940, epoch: 14, batch: 246, loss: 0.00330, acc: 0.99644\n",
      "global step 5950, epoch: 14, batch: 256, loss: 0.01007, acc: 0.99622\n",
      "global step 5960, epoch: 14, batch: 266, loss: 0.00534, acc: 0.99636\n",
      "global step 5970, epoch: 14, batch: 276, loss: 0.00192, acc: 0.99638\n",
      "global step 5980, epoch: 14, batch: 286, loss: 0.00343, acc: 0.99650\n",
      "global step 5990, epoch: 14, batch: 296, loss: 0.00346, acc: 0.99641\n",
      "global step 6000, epoch: 14, batch: 306, loss: 0.00814, acc: 0.99643\n",
      "global step 6010, epoch: 14, batch: 316, loss: 0.01460, acc: 0.99634\n",
      "global step 6020, epoch: 14, batch: 326, loss: 0.02512, acc: 0.99636\n",
      "global step 6030, epoch: 14, batch: 336, loss: 0.00480, acc: 0.99628\n",
      "global step 6040, epoch: 14, batch: 346, loss: 0.00178, acc: 0.99612\n",
      "global step 6050, epoch: 14, batch: 356, loss: 0.00442, acc: 0.99605\n",
      "global step 6060, epoch: 14, batch: 366, loss: 0.00206, acc: 0.99599\n",
      "global step 6070, epoch: 14, batch: 376, loss: 0.07296, acc: 0.99593\n",
      "global step 6080, epoch: 14, batch: 386, loss: 0.00239, acc: 0.99595\n",
      "global step 6090, epoch: 14, batch: 396, loss: 0.01554, acc: 0.99582\n",
      "global step 6100, epoch: 14, batch: 406, loss: 0.01213, acc: 0.99561\n",
      "global step 6110, epoch: 14, batch: 416, loss: 0.04342, acc: 0.99542\n",
      "global step 6120, epoch: 14, batch: 426, loss: 0.02056, acc: 0.99538\n",
      "global step 6130, epoch: 14, batch: 436, loss: 0.00245, acc: 0.99534\n",
      "eval loss: 2.08401, accu: 0.65567\n",
      "global step 6140, epoch: 15, batch: 8, loss: 0.00702, acc: 1.00000\n",
      "global step 6150, epoch: 15, batch: 18, loss: 0.00550, acc: 1.00000\n",
      "global step 6160, epoch: 15, batch: 28, loss: 0.10511, acc: 0.99665\n",
      "global step 6170, epoch: 15, batch: 38, loss: 0.00247, acc: 0.99589\n",
      "global step 6180, epoch: 15, batch: 48, loss: 0.00759, acc: 0.99609\n",
      "global step 6190, epoch: 15, batch: 58, loss: 0.00356, acc: 0.99569\n",
      "global step 6200, epoch: 15, batch: 68, loss: 0.00163, acc: 0.99586\n",
      "global step 6210, epoch: 15, batch: 78, loss: 0.00340, acc: 0.99559\n",
      "global step 6220, epoch: 15, batch: 88, loss: 0.00492, acc: 0.99609\n",
      "global step 6230, epoch: 15, batch: 98, loss: 0.00290, acc: 0.99617\n",
      "global step 6240, epoch: 15, batch: 108, loss: 0.00486, acc: 0.99653\n",
      "global step 6250, epoch: 15, batch: 118, loss: 0.00243, acc: 0.99656\n",
      "global step 6260, epoch: 15, batch: 128, loss: 0.02787, acc: 0.99683\n",
      "global step 6270, epoch: 15, batch: 138, loss: 0.00362, acc: 0.99638\n",
      "global step 6280, epoch: 15, batch: 148, loss: 0.00313, acc: 0.99662\n",
      "global step 6290, epoch: 15, batch: 158, loss: 0.21318, acc: 0.99644\n",
      "global step 6300, epoch: 15, batch: 168, loss: 0.00397, acc: 0.99628\n",
      "global step 6310, epoch: 15, batch: 178, loss: 0.01281, acc: 0.99649\n",
      "global step 6320, epoch: 15, batch: 188, loss: 0.00119, acc: 0.99618\n",
      "global step 6330, epoch: 15, batch: 198, loss: 0.00940, acc: 0.99621\n",
      "global step 6340, epoch: 15, batch: 208, loss: 0.09522, acc: 0.99594\n",
      "global step 6350, epoch: 15, batch: 218, loss: 0.00125, acc: 0.99599\n",
      "global step 6360, epoch: 15, batch: 228, loss: 0.00196, acc: 0.99616\n",
      "global step 6370, epoch: 15, batch: 238, loss: 0.00798, acc: 0.99606\n",
      "global step 6380, epoch: 15, batch: 248, loss: 0.01180, acc: 0.99622\n",
      "global step 6390, epoch: 15, batch: 258, loss: 0.00379, acc: 0.99637\n",
      "global step 6400, epoch: 15, batch: 268, loss: 0.00518, acc: 0.99650\n",
      "global step 6410, epoch: 15, batch: 278, loss: 0.00530, acc: 0.99629\n",
      "global step 6420, epoch: 15, batch: 288, loss: 0.00232, acc: 0.99631\n",
      "global step 6430, epoch: 15, batch: 298, loss: 0.00229, acc: 0.99633\n",
      "global step 6440, epoch: 15, batch: 308, loss: 0.00399, acc: 0.99645\n",
      "global step 6450, epoch: 15, batch: 318, loss: 0.00387, acc: 0.99646\n",
      "global step 6460, epoch: 15, batch: 328, loss: 0.00377, acc: 0.99638\n",
      "global step 6470, epoch: 15, batch: 338, loss: 0.03471, acc: 0.99621\n",
      "global step 6480, epoch: 15, batch: 348, loss: 0.02148, acc: 0.99632\n",
      "global step 6490, epoch: 15, batch: 358, loss: 0.00751, acc: 0.99625\n",
      "global step 6500, epoch: 15, batch: 368, loss: 0.00143, acc: 0.99626\n",
      "global step 6510, epoch: 15, batch: 378, loss: 0.00189, acc: 0.99620\n",
      "global step 6520, epoch: 15, batch: 388, loss: 0.00391, acc: 0.99605\n",
      "global step 6530, epoch: 15, batch: 398, loss: 0.00906, acc: 0.99615\n",
      "global step 6540, epoch: 15, batch: 408, loss: 0.00290, acc: 0.99609\n",
      "global step 6550, epoch: 15, batch: 418, loss: 0.00971, acc: 0.99611\n",
      "global step 6560, epoch: 15, batch: 428, loss: 0.00362, acc: 0.99606\n",
      "global step 6570, epoch: 15, batch: 438, loss: 0.00188, acc: 0.99593\n",
      "eval loss: 2.16931, accu: 0.65083\n",
      "global step 6580, epoch: 16, batch: 10, loss: 0.00317, acc: 0.99687\n",
      "global step 6590, epoch: 16, batch: 20, loss: 0.01021, acc: 0.99844\n",
      "global step 6600, epoch: 16, batch: 30, loss: 0.00113, acc: 0.99896\n",
      "global step 6610, epoch: 16, batch: 40, loss: 0.00347, acc: 0.99844\n",
      "global step 6620, epoch: 16, batch: 50, loss: 0.00302, acc: 0.99875\n",
      "global step 6630, epoch: 16, batch: 60, loss: 0.00181, acc: 0.99792\n",
      "global step 6640, epoch: 16, batch: 70, loss: 0.01884, acc: 0.99821\n",
      "global step 6650, epoch: 16, batch: 80, loss: 0.00334, acc: 0.99844\n",
      "global step 6660, epoch: 16, batch: 90, loss: 0.00327, acc: 0.99826\n",
      "global step 6670, epoch: 16, batch: 100, loss: 0.00190, acc: 0.99844\n",
      "global step 6680, epoch: 16, batch: 110, loss: 0.00161, acc: 0.99858\n",
      "global step 6690, epoch: 16, batch: 120, loss: 0.00715, acc: 0.99870\n",
      "global step 6700, epoch: 16, batch: 130, loss: 0.00684, acc: 0.99880\n",
      "global step 6710, epoch: 16, batch: 140, loss: 0.00100, acc: 0.99888\n",
      "global step 6720, epoch: 16, batch: 150, loss: 0.00211, acc: 0.99896\n",
      "global step 6730, epoch: 16, batch: 160, loss: 0.00210, acc: 0.99883\n",
      "global step 6740, epoch: 16, batch: 170, loss: 0.00376, acc: 0.99871\n",
      "global step 6750, epoch: 16, batch: 180, loss: 0.00789, acc: 0.99878\n",
      "global step 6760, epoch: 16, batch: 190, loss: 0.00085, acc: 0.99885\n",
      "global step 6770, epoch: 16, batch: 200, loss: 0.00298, acc: 0.99891\n",
      "global step 6780, epoch: 16, batch: 210, loss: 0.00156, acc: 0.99896\n",
      "global step 6790, epoch: 16, batch: 220, loss: 0.00253, acc: 0.99901\n",
      "global step 6800, epoch: 16, batch: 230, loss: 0.01615, acc: 0.99891\n",
      "global step 6810, epoch: 16, batch: 240, loss: 0.00111, acc: 0.99896\n",
      "global step 6820, epoch: 16, batch: 250, loss: 0.00131, acc: 0.99900\n",
      "global step 6830, epoch: 16, batch: 260, loss: 0.00142, acc: 0.99892\n",
      "global step 6840, epoch: 16, batch: 270, loss: 0.02068, acc: 0.99896\n",
      "global step 6850, epoch: 16, batch: 280, loss: 0.00313, acc: 0.99900\n",
      "global step 6860, epoch: 16, batch: 290, loss: 0.00198, acc: 0.99892\n",
      "global step 6870, epoch: 16, batch: 300, loss: 0.00297, acc: 0.99896\n",
      "global step 6880, epoch: 16, batch: 310, loss: 0.00303, acc: 0.99889\n",
      "global step 6890, epoch: 16, batch: 320, loss: 0.00867, acc: 0.99873\n",
      "global step 6900, epoch: 16, batch: 330, loss: 0.00229, acc: 0.99858\n",
      "global step 6910, epoch: 16, batch: 340, loss: 0.00179, acc: 0.99853\n",
      "global step 6920, epoch: 16, batch: 350, loss: 0.00180, acc: 0.99830\n",
      "global step 6930, epoch: 16, batch: 360, loss: 0.00120, acc: 0.99835\n",
      "global step 6940, epoch: 16, batch: 370, loss: 0.00114, acc: 0.99840\n",
      "global step 6950, epoch: 16, batch: 380, loss: 0.00058, acc: 0.99844\n",
      "global step 6960, epoch: 16, batch: 390, loss: 0.00141, acc: 0.99848\n",
      "global step 6970, epoch: 16, batch: 400, loss: 0.00172, acc: 0.99844\n",
      "global step 6980, epoch: 16, batch: 410, loss: 0.00156, acc: 0.99848\n",
      "global step 6990, epoch: 16, batch: 420, loss: 0.00070, acc: 0.99851\n",
      "global step 7000, epoch: 16, batch: 430, loss: 0.00151, acc: 0.99855\n",
      "eval loss: 2.18028, accu: 0.65767\n",
      "global step 7010, epoch: 17, batch: 2, loss: 0.00608, acc: 1.00000\n",
      "global step 7020, epoch: 17, batch: 12, loss: 0.00654, acc: 1.00000\n",
      "global step 7030, epoch: 17, batch: 22, loss: 0.00161, acc: 1.00000\n",
      "global step 7040, epoch: 17, batch: 32, loss: 0.00137, acc: 1.00000\n",
      "global step 7050, epoch: 17, batch: 42, loss: 0.00054, acc: 1.00000\n",
      "global step 7060, epoch: 17, batch: 52, loss: 0.01690, acc: 1.00000\n",
      "global step 7070, epoch: 17, batch: 62, loss: 0.00170, acc: 1.00000\n",
      "global step 7080, epoch: 17, batch: 72, loss: 0.00065, acc: 0.99957\n",
      "global step 7090, epoch: 17, batch: 82, loss: 0.00389, acc: 0.99962\n",
      "global step 7100, epoch: 17, batch: 92, loss: 0.00169, acc: 0.99932\n",
      "global step 7110, epoch: 17, batch: 102, loss: 0.00187, acc: 0.99939\n",
      "global step 7120, epoch: 17, batch: 112, loss: 0.00148, acc: 0.99916\n",
      "global step 7130, epoch: 17, batch: 122, loss: 0.01580, acc: 0.99923\n",
      "global step 7140, epoch: 17, batch: 132, loss: 0.00217, acc: 0.99905\n",
      "global step 7150, epoch: 17, batch: 142, loss: 0.00677, acc: 0.99912\n",
      "global step 7160, epoch: 17, batch: 152, loss: 0.00260, acc: 0.99918\n",
      "global step 7170, epoch: 17, batch: 162, loss: 0.00262, acc: 0.99923\n",
      "global step 7180, epoch: 17, batch: 172, loss: 0.00228, acc: 0.99927\n",
      "global step 7190, epoch: 17, batch: 182, loss: 0.00104, acc: 0.99897\n",
      "global step 7200, epoch: 17, batch: 192, loss: 0.00127, acc: 0.99886\n",
      "global step 7210, epoch: 17, batch: 202, loss: 0.00923, acc: 0.99892\n",
      "global step 7220, epoch: 17, batch: 212, loss: 0.03477, acc: 0.99867\n",
      "global step 7230, epoch: 17, batch: 222, loss: 0.00170, acc: 0.99873\n",
      "global step 7240, epoch: 17, batch: 232, loss: 0.00128, acc: 0.99879\n",
      "global step 7250, epoch: 17, batch: 242, loss: 0.00789, acc: 0.99871\n",
      "global step 7260, epoch: 17, batch: 252, loss: 0.02269, acc: 0.99864\n",
      "global step 7270, epoch: 17, batch: 262, loss: 0.00233, acc: 0.99869\n",
      "global step 7280, epoch: 17, batch: 272, loss: 0.00415, acc: 0.99874\n",
      "global step 7290, epoch: 17, batch: 282, loss: 0.00140, acc: 0.99867\n",
      "global step 7300, epoch: 17, batch: 292, loss: 0.00249, acc: 0.99872\n",
      "global step 7310, epoch: 17, batch: 302, loss: 0.00828, acc: 0.99865\n",
      "global step 7320, epoch: 17, batch: 312, loss: 0.00133, acc: 0.99860\n",
      "global step 7330, epoch: 17, batch: 322, loss: 0.01277, acc: 0.99864\n",
      "global step 7340, epoch: 17, batch: 332, loss: 0.00176, acc: 0.99868\n",
      "global step 7350, epoch: 17, batch: 342, loss: 0.00214, acc: 0.99872\n",
      "global step 7360, epoch: 17, batch: 352, loss: 0.00275, acc: 0.99876\n",
      "global step 7370, epoch: 17, batch: 362, loss: 0.00233, acc: 0.99871\n",
      "global step 7380, epoch: 17, batch: 372, loss: 0.00511, acc: 0.99874\n",
      "global step 7390, epoch: 17, batch: 382, loss: 0.00216, acc: 0.99869\n",
      "global step 7400, epoch: 17, batch: 392, loss: 0.00115, acc: 0.99872\n",
      "global step 7410, epoch: 17, batch: 402, loss: 0.00211, acc: 0.99876\n",
      "global step 7420, epoch: 17, batch: 412, loss: 0.00137, acc: 0.99863\n",
      "global step 7430, epoch: 17, batch: 422, loss: 0.04929, acc: 0.99852\n",
      "global step 7440, epoch: 17, batch: 432, loss: 0.00165, acc: 0.99855\n",
      "eval loss: 2.22781, accu: 0.65883\n",
      "global step 7450, epoch: 18, batch: 4, loss: 0.00163, acc: 1.00000\n",
      "global step 7460, epoch: 18, batch: 14, loss: 0.00128, acc: 1.00000\n",
      "global step 7470, epoch: 18, batch: 24, loss: 0.00173, acc: 1.00000\n",
      "global step 7480, epoch: 18, batch: 34, loss: 0.00146, acc: 0.99908\n",
      "global step 7490, epoch: 18, batch: 44, loss: 0.00230, acc: 0.99858\n",
      "global step 7500, epoch: 18, batch: 54, loss: 0.00258, acc: 0.99884\n",
      "global step 7510, epoch: 18, batch: 64, loss: 0.00183, acc: 0.99902\n",
      "global step 7520, epoch: 18, batch: 74, loss: 0.01275, acc: 0.99916\n",
      "global step 7530, epoch: 18, batch: 84, loss: 0.00069, acc: 0.99926\n",
      "global step 7540, epoch: 18, batch: 94, loss: 0.00109, acc: 0.99934\n",
      "global step 7550, epoch: 18, batch: 104, loss: 0.00084, acc: 0.99940\n",
      "global step 7560, epoch: 18, batch: 114, loss: 0.00247, acc: 0.99945\n",
      "global step 7570, epoch: 18, batch: 124, loss: 0.00740, acc: 0.99924\n",
      "global step 7580, epoch: 18, batch: 134, loss: 0.00112, acc: 0.99930\n",
      "global step 7590, epoch: 18, batch: 144, loss: 0.00393, acc: 0.99935\n",
      "global step 7600, epoch: 18, batch: 154, loss: 0.00120, acc: 0.99939\n",
      "global step 7610, epoch: 18, batch: 164, loss: 0.00077, acc: 0.99943\n",
      "global step 7620, epoch: 18, batch: 174, loss: 0.00177, acc: 0.99946\n",
      "global step 7630, epoch: 18, batch: 184, loss: 0.00105, acc: 0.99949\n",
      "global step 7640, epoch: 18, batch: 194, loss: 0.00250, acc: 0.99952\n",
      "global step 7650, epoch: 18, batch: 204, loss: 0.00074, acc: 0.99954\n",
      "global step 7660, epoch: 18, batch: 214, loss: 0.01117, acc: 0.99956\n",
      "global step 7670, epoch: 18, batch: 224, loss: 0.00428, acc: 0.99958\n",
      "global step 7680, epoch: 18, batch: 234, loss: 0.00101, acc: 0.99960\n",
      "global step 7690, epoch: 18, batch: 244, loss: 0.00151, acc: 0.99962\n",
      "global step 7700, epoch: 18, batch: 254, loss: 0.01035, acc: 0.99963\n",
      "global step 7710, epoch: 18, batch: 264, loss: 0.00160, acc: 0.99964\n",
      "global step 7720, epoch: 18, batch: 274, loss: 0.00078, acc: 0.99966\n",
      "global step 7730, epoch: 18, batch: 284, loss: 0.00348, acc: 0.99967\n",
      "global step 7740, epoch: 18, batch: 294, loss: 0.00083, acc: 0.99957\n",
      "global step 7750, epoch: 18, batch: 304, loss: 0.00125, acc: 0.99959\n",
      "global step 7760, epoch: 18, batch: 314, loss: 0.00150, acc: 0.99960\n",
      "global step 7770, epoch: 18, batch: 324, loss: 0.00149, acc: 0.99961\n",
      "global step 7780, epoch: 18, batch: 334, loss: 0.00082, acc: 0.99963\n",
      "global step 7790, epoch: 18, batch: 344, loss: 0.00102, acc: 0.99964\n",
      "global step 7800, epoch: 18, batch: 354, loss: 0.00216, acc: 0.99965\n",
      "global step 7810, epoch: 18, batch: 364, loss: 0.00102, acc: 0.99966\n",
      "global step 7820, epoch: 18, batch: 374, loss: 0.00109, acc: 0.99967\n",
      "global step 7830, epoch: 18, batch: 384, loss: 0.00092, acc: 0.99967\n",
      "global step 7840, epoch: 18, batch: 394, loss: 0.00119, acc: 0.99968\n",
      "global step 7850, epoch: 18, batch: 404, loss: 0.00100, acc: 0.99969\n",
      "global step 7860, epoch: 18, batch: 414, loss: 0.00073, acc: 0.99970\n",
      "global step 7870, epoch: 18, batch: 424, loss: 0.00097, acc: 0.99971\n",
      "global step 7880, epoch: 18, batch: 434, loss: 0.00087, acc: 0.99971\n",
      "eval loss: 2.25834, accu: 0.65517\n",
      "global step 7890, epoch: 19, batch: 6, loss: 0.00226, acc: 1.00000\n",
      "global step 7900, epoch: 19, batch: 16, loss: 0.00090, acc: 1.00000\n",
      "global step 7910, epoch: 19, batch: 26, loss: 0.00103, acc: 1.00000\n",
      "global step 7920, epoch: 19, batch: 36, loss: 0.00129, acc: 1.00000\n",
      "global step 7930, epoch: 19, batch: 46, loss: 0.00118, acc: 1.00000\n",
      "global step 7940, epoch: 19, batch: 56, loss: 0.00195, acc: 1.00000\n",
      "global step 7950, epoch: 19, batch: 66, loss: 0.00143, acc: 1.00000\n",
      "global step 7960, epoch: 19, batch: 76, loss: 0.00109, acc: 0.99959\n",
      "global step 7970, epoch: 19, batch: 86, loss: 0.00094, acc: 0.99964\n",
      "global step 7980, epoch: 19, batch: 96, loss: 0.00105, acc: 0.99967\n",
      "global step 7990, epoch: 19, batch: 106, loss: 0.00079, acc: 0.99971\n",
      "global step 8000, epoch: 19, batch: 116, loss: 0.00062, acc: 0.99946\n",
      "global step 8010, epoch: 19, batch: 126, loss: 0.00080, acc: 0.99950\n",
      "global step 8020, epoch: 19, batch: 136, loss: 0.00117, acc: 0.99954\n",
      "global step 8030, epoch: 19, batch: 146, loss: 0.00061, acc: 0.99957\n",
      "global step 8040, epoch: 19, batch: 156, loss: 0.00069, acc: 0.99960\n",
      "global step 8050, epoch: 19, batch: 166, loss: 0.00151, acc: 0.99944\n",
      "global step 8060, epoch: 19, batch: 176, loss: 0.00108, acc: 0.99947\n",
      "global step 8070, epoch: 19, batch: 186, loss: 0.01505, acc: 0.99950\n",
      "global step 8080, epoch: 19, batch: 196, loss: 0.00145, acc: 0.99952\n",
      "global step 8090, epoch: 19, batch: 206, loss: 0.02077, acc: 0.99954\n",
      "global step 8100, epoch: 19, batch: 216, loss: 0.00089, acc: 0.99957\n",
      "global step 8110, epoch: 19, batch: 226, loss: 0.00226, acc: 0.99959\n",
      "global step 8120, epoch: 19, batch: 236, loss: 0.00081, acc: 0.99960\n",
      "global step 8130, epoch: 19, batch: 246, loss: 0.00269, acc: 0.99962\n",
      "global step 8140, epoch: 19, batch: 256, loss: 0.00145, acc: 0.99963\n",
      "global step 8150, epoch: 19, batch: 266, loss: 0.00102, acc: 0.99953\n",
      "global step 8160, epoch: 19, batch: 276, loss: 0.00233, acc: 0.99955\n",
      "global step 8170, epoch: 19, batch: 286, loss: 0.00095, acc: 0.99956\n",
      "global step 8180, epoch: 19, batch: 296, loss: 0.00092, acc: 0.99958\n",
      "global step 8190, epoch: 19, batch: 306, loss: 0.00144, acc: 0.99959\n",
      "global step 8200, epoch: 19, batch: 316, loss: 0.00063, acc: 0.99960\n",
      "global step 8210, epoch: 19, batch: 326, loss: 0.00091, acc: 0.99962\n",
      "global step 8220, epoch: 19, batch: 336, loss: 0.00125, acc: 0.99963\n",
      "global step 8230, epoch: 19, batch: 346, loss: 0.00178, acc: 0.99964\n",
      "global step 8240, epoch: 19, batch: 356, loss: 0.00135, acc: 0.99965\n",
      "global step 8250, epoch: 19, batch: 366, loss: 0.00084, acc: 0.99966\n",
      "global step 8260, epoch: 19, batch: 376, loss: 0.00168, acc: 0.99967\n",
      "global step 8270, epoch: 19, batch: 386, loss: 0.00108, acc: 0.99968\n",
      "global step 8280, epoch: 19, batch: 396, loss: 0.00114, acc: 0.99968\n",
      "global step 8290, epoch: 19, batch: 406, loss: 0.00122, acc: 0.99969\n",
      "global step 8300, epoch: 19, batch: 416, loss: 0.00285, acc: 0.99970\n",
      "global step 8310, epoch: 19, batch: 426, loss: 0.00122, acc: 0.99971\n",
      "global step 8320, epoch: 19, batch: 436, loss: 0.00319, acc: 0.99964\n",
      "eval loss: 2.27448, accu: 0.65783\n",
      "global step 8330, epoch: 20, batch: 8, loss: 0.00061, acc: 1.00000\n",
      "global step 8340, epoch: 20, batch: 18, loss: 0.00965, acc: 1.00000\n",
      "global step 8350, epoch: 20, batch: 28, loss: 0.00095, acc: 1.00000\n",
      "global step 8360, epoch: 20, batch: 38, loss: 0.00128, acc: 1.00000\n",
      "global step 8370, epoch: 20, batch: 48, loss: 0.00111, acc: 1.00000\n",
      "global step 8380, epoch: 20, batch: 58, loss: 0.00160, acc: 1.00000\n",
      "global step 8390, epoch: 20, batch: 68, loss: 0.00261, acc: 0.99954\n",
      "global step 8400, epoch: 20, batch: 78, loss: 0.00113, acc: 0.99960\n",
      "global step 8410, epoch: 20, batch: 88, loss: 0.00112, acc: 0.99964\n",
      "global step 8420, epoch: 20, batch: 98, loss: 0.00091, acc: 0.99968\n",
      "global step 8430, epoch: 20, batch: 108, loss: 0.00092, acc: 0.99971\n",
      "global step 8440, epoch: 20, batch: 118, loss: 0.00127, acc: 0.99974\n",
      "global step 8450, epoch: 20, batch: 128, loss: 0.00456, acc: 0.99976\n",
      "global step 8460, epoch: 20, batch: 138, loss: 0.00075, acc: 0.99977\n",
      "global step 8470, epoch: 20, batch: 148, loss: 0.00575, acc: 0.99979\n",
      "global step 8480, epoch: 20, batch: 158, loss: 0.00162, acc: 0.99980\n",
      "global step 8490, epoch: 20, batch: 168, loss: 0.00108, acc: 0.99981\n",
      "global step 8500, epoch: 20, batch: 178, loss: 0.00100, acc: 0.99982\n",
      "global step 8510, epoch: 20, batch: 188, loss: 0.00184, acc: 0.99983\n",
      "global step 8520, epoch: 20, batch: 198, loss: 0.00107, acc: 0.99984\n",
      "global step 8530, epoch: 20, batch: 208, loss: 0.00245, acc: 0.99985\n",
      "global step 8540, epoch: 20, batch: 218, loss: 0.00056, acc: 0.99986\n",
      "global step 8550, epoch: 20, batch: 228, loss: 0.00182, acc: 0.99986\n",
      "global step 8560, epoch: 20, batch: 238, loss: 0.00042, acc: 0.99987\n",
      "global step 8570, epoch: 20, batch: 248, loss: 0.00063, acc: 0.99975\n",
      "global step 8580, epoch: 20, batch: 258, loss: 0.00508, acc: 0.99976\n",
      "global step 8590, epoch: 20, batch: 268, loss: 0.00329, acc: 0.99977\n",
      "global step 8600, epoch: 20, batch: 278, loss: 0.00078, acc: 0.99978\n",
      "global step 8610, epoch: 20, batch: 288, loss: 0.00104, acc: 0.99978\n",
      "global step 8620, epoch: 20, batch: 298, loss: 0.00091, acc: 0.99969\n",
      "global step 8630, epoch: 20, batch: 308, loss: 0.00291, acc: 0.99970\n",
      "global step 8640, epoch: 20, batch: 318, loss: 0.00052, acc: 0.99971\n",
      "global step 8650, epoch: 20, batch: 328, loss: 0.00568, acc: 0.99971\n",
      "global step 8660, epoch: 20, batch: 338, loss: 0.00081, acc: 0.99972\n",
      "global step 8670, epoch: 20, batch: 348, loss: 0.00713, acc: 0.99973\n",
      "global step 8680, epoch: 20, batch: 358, loss: 0.00606, acc: 0.99974\n",
      "global step 8690, epoch: 20, batch: 368, loss: 0.00227, acc: 0.99975\n",
      "global step 8700, epoch: 20, batch: 378, loss: 0.00115, acc: 0.99975\n",
      "global step 8710, epoch: 20, batch: 388, loss: 0.00080, acc: 0.99976\n",
      "global step 8720, epoch: 20, batch: 398, loss: 0.00060, acc: 0.99976\n",
      "global step 8730, epoch: 20, batch: 408, loss: 0.00100, acc: 0.99977\n",
      "global step 8740, epoch: 20, batch: 418, loss: 0.00075, acc: 0.99978\n",
      "global step 8750, epoch: 20, batch: 428, loss: 0.00231, acc: 0.99978\n",
      "global step 8760, epoch: 20, batch: 438, loss: 0.00224, acc: 0.99979\n",
      "eval loss: 2.27844, accu: 0.65917\n"
     ]
    }
   ],
   "source": [
    "import paddle.nn.functional as F\n",
    "\n",
    "ckpt_dir = \"ernie_ckpt\"\n",
    "global_step = 0\n",
    "for epoch in range(1, epochs + 1):\n",
    "    for step, batch in enumerate(train_data_loader, start=1):\n",
    "        input_ids, segment_ids, labels = batch\n",
    "        logits = model(input_ids, segment_ids)\n",
    "        loss = criterion(logits, labels)\n",
    "        probs = F.softmax(logits, axis=1)\n",
    "        correct = metric.compute(probs, labels)\n",
    "        metric.update(correct)\n",
    "        acc = metric.accumulate()\n",
    "\n",
    "        global_step += 1\n",
    "        if global_step % 10 == 0 :\n",
    "            print(\"global step %d, epoch: %d, batch: %d, loss: %.5f, acc: %.5f\" % (global_step, epoch, step, loss, acc))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.clear_grad()\n",
    "    if epoch % 1 == 0:\n",
    "        save_dir = os.path.join(ckpt_dir, \"model_%d\" % global_step)\n",
    "        if not os.path.exists(save_dir):\n",
    "             os.makedirs(save_dir)\n",
    "        # evaluate model\n",
    "        evaluate(model, criterion, metric, dev_data_loader)\n",
    "        # save\n",
    "        model.save_pretrained(save_dir)\n",
    "        # save\n",
    "        tokenizer.save_pretrained(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "02ea523f-1d0b-482f-ab21-10fb16712bc0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-25T08:06:52.171120Z",
     "iopub.status.busy": "2021-10-25T08:06:52.170668Z",
     "iopub.status.idle": "2021-10-25T08:06:52.181680Z",
     "shell.execute_reply": "2021-10-25T08:06:52.180610Z",
     "shell.execute_reply.started": "2021-10-25T08:06:52.171059Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict(model, data, tokenizer, label_map, batch_size=1):\n",
    "    \"\"\"\n",
    "    Predicts the data labels.\n",
    "\n",
    "    Args:\n",
    "        model (obj:`paddle.nn.Layer`): A model to classify texts.\n",
    "        data (obj:`List(Example)`): The processed data whose each element is a Example (numedtuple) object.\n",
    "            A Example object contains `text`(word_ids) and `se_len`(sequence length).\n",
    "        tokenizer(obj:`PretrainedTokenizer`): This tokenizer inherits from :class:`~paddlenlp.transformers.PretrainedTokenizer` \n",
    "            which contains most of the methods. Users should refer to the superclass for more information regarding methods.\n",
    "        label_map(obj:`dict`): The label id (key) to label str (value) map.\n",
    "        batch_size(obj:`int`, defaults to 1): The number of batch.\n",
    "\n",
    "    Returns:\n",
    "        results(obj:`dict`): All the predictions labels.\n",
    "    \"\"\"\n",
    "    examples = []\n",
    "    for text in data:\n",
    "        input_ids, segment_ids = convert_example(\n",
    "            text,\n",
    "            tokenizer,\n",
    "            max_seq_length=128,\n",
    "            is_test=True)\n",
    "        examples.append((input_ids, segment_ids))\n",
    "\n",
    "    batchify_fn = lambda samples, fn=Tuple(\n",
    "        Pad(axis=0, pad_val=tokenizer.pad_token_id),  # input id\n",
    "        Pad(axis=0, pad_val=tokenizer.pad_token_id),  # segment id\n",
    "    ): fn(samples)\n",
    "\n",
    "    # Seperates data into some batches.\n",
    "    batches = []\n",
    "    one_batch = []\n",
    "    for example in examples:\n",
    "        one_batch.append(example)\n",
    "        if len(one_batch) == batch_size:\n",
    "            batches.append(one_batch)\n",
    "            one_batch = []\n",
    "    if one_batch:\n",
    "        # The last batch whose size is less than the config batch_size setting.\n",
    "        batches.append(one_batch)\n",
    "\n",
    "    results = []\n",
    "    model.eval()\n",
    "    for batch in batches:\n",
    "        input_ids, segment_ids = batchify_fn(batch)\n",
    "        input_ids = paddle.to_tensor(input_ids)\n",
    "        segment_ids = paddle.to_tensor(segment_ids)\n",
    "        logits = model(input_ids, segment_ids)\n",
    "        probs = F.softmax(logits, axis=1)\n",
    "        idx = paddle.argmax(probs, axis=1).numpy()\n",
    "        idx = idx.tolist()\n",
    "        labels = [label_map[i] for i in idx]\n",
    "        results.extend(labels)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6a3ef33e-5c42-4d3c-84d8-df84f8465351",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-25T08:07:53.623944Z",
     "iopub.status.busy": "2021-10-25T08:07:53.623355Z",
     "iopub.status.idle": "2021-10-25T08:07:53.681635Z",
     "shell.execute_reply": "2021-10-25T08:07:53.680944Z",
     "shell.execute_reply.started": "2021-10-25T08:07:53.623688Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '低自尊', 1: '其他', 2: '失眠', 3: '情感关系问题', 4: '青春期问题', 5: '压力', 6: '自我探索', 7: '家庭问题和矛盾', 8: '强迫症', 9: '人际关系', 10: '事业和工作烦恼', 11: '学业烦恼、对未来规划的迷茫', 12: '离婚', 13: '分手', 14: '物质滥用', 15: '悲恸', 16: '性问题', 17: 'LGBT', 18: '亲子关系', 19: '尚未到达S2', 20: '忧郁症', 21: '焦虑症', 22: '其他疾病', 23: '躁郁症', 24: '创伤后应激反应', 25: '恐慌症', 26: '厌食症和暴食症', 27: '进行的人身伤害', 28: '策划及逆行的自杀行为', 29: '自残', 30: '无伤害身体倾向', 31: '计划的人身伤害'}\n",
      "Data: {'text': '我感觉我好垃圾', 'qid': ''} \t Lable: 其他\n",
      "Data: {'text': '我被那个坏女人伤透了心', 'qid': ''} \t Lable: 情感关系问题\n",
      "Data: {'text': '太难了学不下去了', 'qid': ''} \t Lable: 学业烦恼、对未来规划的迷茫\n",
      "Data: {'text': '哎我晚上一直睡不着怎么办呀', 'qid': ''} \t Lable: 失眠\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    {\"text\":'我感觉我好垃圾','qid':''},\n",
    "    {\"text\":'我被那个坏女人伤透了心','qid':''},\n",
    "    {\"text\":'太难了学不下去了','qid':''},\n",
    "     {\"text\":'哎我晚上一直睡不着怎么办呀','qid':''},\n",
    "]\n",
    "label_map = {'低自尊': 0, '其他': 1, '失眠': 2, '情感关系问题': 3, '青春期问题': 4, '压力': 5, '自我探索': 6, '家庭问题和矛盾': 7, '强迫症': 8, '人际关系': 9, '事业和工作烦恼': 10,\n",
    " '学业烦恼、对未来规划的迷茫': 11, '离婚': 12, '分手': 13, '物质滥用': 14, '悲恸': 15, '性问题': 16, 'LGBT': 17, '亲子关系': 18, '尚未到达S2': 19, '忧郁症': 20, \n",
    " '焦虑症': 21, '其他疾病': 22, '躁郁症': 23, '创伤后应激反应': 24, '恐慌症': 25, '厌食症和暴食症': 26, '进行的人身伤害': 27, '策划及逆行的自杀行为': 28, '自残': 29, '无伤害身体倾向': 30, '计划的人身伤害': 31}\n",
    "label_map = {value:key for key,value in label_map.items()}\n",
    "print(label_map)\n",
    "\n",
    "results = predict(\n",
    "    model, data, tokenizer, label_map, batch_size=batch_size)\n",
    "for idx, text in enumerate(data):\n",
    "    print('Data: {} \\t Lable: {}'.format(text, results[idx]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
