{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e15e82f0-046b-4010-a488-72e56c3f1c82",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-27T13:24:06.627001Z",
     "iopub.status.busy": "2021-10-27T13:24:06.626669Z",
     "iopub.status.idle": "2021-10-27T13:24:06.645087Z",
     "shell.execute_reply": "2021-10-27T13:24:06.644278Z",
     "shell.execute_reply.started": "2021-10-27T13:24:06.626955Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'type_check' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-c5e8d51f15c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpaddle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpaddlenlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTokenEmbedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist_embedding_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpaddle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_embedding_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# ['w2v.baidu_encyclopedia.target.word-word.dim300']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbatch\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfluid\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmonkey_patch_variable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfluid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdygraph\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmonkey_patch_math_varbase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mmonkey_patch_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;31m# import all class inside framework into fluid module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mframework\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;31m# import all class inside executor into fluid module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/framework.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMethodType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFunctionType\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmultiprocessing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m     \u001b[0;31m# NOTE: to be revisited following future namespace cleanup.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0;31m# See gh-14454 and gh-15672 for discussion.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/numpy/lib/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0m__all__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'emath'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'math'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'tracemalloc_domain'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Arrayterator'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0m__all__\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtype_check\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__all__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0m__all__\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mindex_tricks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__all__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0m__all__\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mfunction_base\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__all__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'type_check' is not defined"
     ]
    }
   ],
   "source": [
    "import paddle\n",
    "from paddlenlp.embeddings import TokenEmbedding, list_embedding_name\n",
    "paddle.set_device(\"cpu\")\n",
    "\n",
    "print(list_embedding_name()) # ['w2v.baidu_encyclopedia.target.word-word.dim300']\n",
    "token_embedding = TokenEmbedding(embedding_name=\"w2v.baidu_encyclopedia.target.word-word.dim300\")\n",
    "print(token_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0b6aa366-6d2a-421b-928b-f4ebcd4b6f2f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-24T13:56:04.658270Z",
     "iopub.status.busy": "2021-10-24T13:56:04.657918Z",
     "iopub.status.idle": "2021-10-24T13:56:04.661610Z",
     "shell.execute_reply": "2021-10-24T13:56:04.660873Z",
     "shell.execute_reply.started": "2021-10-24T13:56:04.658216Z"
    }
   },
   "outputs": [],
   "source": [
    "#text similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2c8a686-6af8-4a39-a454-9ab70889be19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-24T13:46:17.108632Z",
     "iopub.status.busy": "2021-10-24T13:46:17.108285Z",
     "iopub.status.idle": "2021-10-24T13:46:17.119146Z",
     "shell.execute_reply": "2021-10-24T13:46:17.118370Z",
     "shell.execute_reply.started": "2021-10-24T13:46:17.108578Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'中国': 1.0000001, '中国人': 0.50604516, '你好': 0.118777126, '美国汽车': 0.040412646, '中国是一个国家': 0.040412646}\n"
     ]
    }
   ],
   "source": [
    "test_token_embedding = token_embedding.search(\"中国\")\n",
    "from collections import Counter\n",
    "\n",
    "def get_order_dict_N(_dict, N):\n",
    "    result = Counter(_dict).most_common(N)\n",
    "    d = {}\n",
    "    for k,v in result:\n",
    "        d[k] = v\n",
    "    return d\n",
    "\n",
    "txt_list=['中国人','美国汽车','中国','中国是一个国家','你好']\n",
    "\n",
    "dic={}\n",
    "def search_list(input_txt,target_list,method='cosine',topk=5):\n",
    "    score=[]\n",
    "    for txt in target_list:\n",
    "        dic[txt]=token_embedding.cosine_sim(input_txt, txt)  \n",
    "    return get_order_dict_N(dic,topk)\n",
    "\n",
    "print(search_list('中国',txt_list,method='cosine',topk=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d4ddb9-810c-48c2-abec-1e348bda0510",
   "metadata": {},
   "outputs": [],
   "source": [
    "#key words extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "613cc359-2ffb-4078-81d5-c313b1dd0dc5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-27T08:59:29.801363Z",
     "iopub.status.busy": "2021-10-27T08:59:29.800973Z",
     "iopub.status.idle": "2021-10-27T08:59:30.160743Z",
     "shell.execute_reply": "2021-10-27T08:59:30.159457Z",
     "shell.execute_reply.started": "2021-10-27T08:59:29.801302Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF\n",
      "焦虑 4.1189\n",
      "感到 0.8770\n",
      "今天 0.8320\n",
      "原因 0.8253\n",
      "textrank\n"
     ]
    }
   ],
   "source": [
    "import jieba.analyse\n",
    "\n",
    "sent = '我今天感到很焦虑，焦虑的原因是我很焦虑'\n",
    "print('TF-IDF')\n",
    "t = jieba.analyse.TFIDF()\n",
    "kws = t.extract_tags(sent, topK=10, withWeight=True)\n",
    "for word, weight in kws:\n",
    "    print('%s %.4f'%(word, weight))\n",
    "\n",
    "print('textrank')\n",
    "t = jieba.analyse.TextRank()\n",
    "kws = t.extract_tags(sent, topK=10, withWeight=True, allowPOS=('ns', 'n', 'vn', 'v'))\n",
    "for word, weight in kws:\n",
    "    print('%s %.4f'%(word, weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea2f6f1e-f82a-4d51-9a2b-b4e251e01b49",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-27T13:28:04.147498Z",
     "iopub.status.busy": "2021-10-27T13:28:04.147160Z",
     "iopub.status.idle": "2021-10-27T13:28:10.870160Z",
     "shell.execute_reply": "2021-10-27T13:28:10.869426Z",
     "shell.execute_reply.started": "2021-10-27T13:28:04.147451Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-10-27 21:28:04,166] [    INFO] - Already cached /home/aistudio/.paddlenlp/models/skep_ernie_1.0_large_ch/skep_ernie_1.0_large_ch.pdparams\n",
      "[2021-10-27 21:28:08,282] [    INFO] - Already cached /home/aistudio/.paddlenlp/models/skep_ernie_1.0_large_ch/skep_ernie_1.0_large_ch.vocab.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded parameters from ../model_state.pdparams\n",
      "init done\n",
      "你可能有点关于失眠的问题,你可以查看下这个网站来获取更多信息 https://zh.wikihow.com/wikiHowTo?search=失眠\n"
     ]
    }
   ],
   "source": [
    "import paddlenlp\n",
    "from paddlenlp.transformers import SkepForSequenceClassification, SkepTokenizer\n",
    "import os\n",
    "import paddle\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import paddle.nn.functional as F\n",
    "from paddlenlp.data import Stack, Tuple, Pad\n",
    "\n",
    "def convert_example(example,\n",
    "                    tokenizer,\n",
    "                    max_seq_length=512,\n",
    "                    is_test=False):\n",
    "    encoded_inputs = tokenizer(\n",
    "        text=example[\"text\"], max_seq_len=max_seq_length)\n",
    "\n",
    "\n",
    "    input_ids = encoded_inputs[\"input_ids\"]\n",
    "    # token_type_ids\n",
    "    token_type_ids = encoded_inputs[\"token_type_ids\"]\n",
    "\n",
    "    if not is_test:\n",
    "        # label\n",
    "        label = np.array([example[\"label\"]], dtype=\"int64\")\n",
    "        return input_ids, token_type_ids, label\n",
    "    else:\n",
    "        # qid = np.array([example[\"qid\"]], dtype=\"int64\")\n",
    "        return input_ids, token_type_ids\n",
    "\n",
    "def create_dataloader(dataset,\n",
    "                      trans_fn=None,\n",
    "                      mode='train',\n",
    "                      batch_size=1,\n",
    "                      batchify_fn=None):\n",
    "    if trans_fn:\n",
    "        dataset = dataset.map(trans_fn)\n",
    "\n",
    "    shuffle = True if mode == 'train' else False\n",
    "    if mode == \"train\":\n",
    "        sampler = paddle.io.DistributedBatchSampler(\n",
    "            dataset=dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    else:\n",
    "        sampler = paddle.io.BatchSampler(\n",
    "            dataset=dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    dataloader = paddle.io.DataLoader(\n",
    "        dataset, batch_sampler=sampler, collate_fn=batchify_fn)\n",
    "    return dataloader\n",
    "def predict(model, data, tokenizer, label_map, batch_size=1):\n",
    "    \"\"\"\n",
    "    Predicts the data labels.\n",
    "\n",
    "    Args:\n",
    "        model (obj:`paddle.nn.Layer`): A model to classify texts.\n",
    "        data (obj:`List(Example)`): The processed data whose each element is a Example (numedtuple) object.\n",
    "            A Example object contains `text`(word_ids) and `se_len`(sequence length).\n",
    "        tokenizer(obj:`PretrainedTokenizer`): This tokenizer inherits from :class:`~paddlenlp.transformers.PretrainedTokenizer` \n",
    "            which contains most of the methods. Users should refer to the superclass for more information regarding methods.\n",
    "        label_map(obj:`dict`): The label id (key) to label str (value) map.\n",
    "        batch_size(obj:`int`, defaults to 1): The number of batch.\n",
    "\n",
    "    Returns:\n",
    "        results(obj:`dict`): All the predictions labels.\n",
    "    \"\"\"\n",
    "    examples = []\n",
    "    for text in data:\n",
    "        input_ids, segment_ids = convert_example(\n",
    "            text,\n",
    "            tokenizer,\n",
    "            max_seq_length=128,\n",
    "            is_test=True)\n",
    "        examples.append((input_ids, segment_ids))\n",
    "\n",
    "    batchify_fn = lambda samples, fn=Tuple(\n",
    "        Pad(axis=0, pad_val=tokenizer.pad_token_id),  # input id\n",
    "        Pad(axis=0, pad_val=tokenizer.pad_token_id),  # segment id\n",
    "    ): fn(samples)\n",
    "\n",
    "    # Seperates data into some batches.\n",
    "    batches = []\n",
    "    one_batch = []\n",
    "    for example in examples:\n",
    "        one_batch.append(example)\n",
    "        if len(one_batch) == batch_size:\n",
    "            batches.append(one_batch)\n",
    "            one_batch = []\n",
    "    if one_batch:\n",
    "        # The last batch whose size is less than the config batch_size setting.\n",
    "        batches.append(one_batch)\n",
    "\n",
    "    results = []\n",
    "    model.eval()\n",
    "    for batch in batches:\n",
    "        input_ids, segment_ids = batchify_fn(batch)\n",
    "        input_ids = paddle.to_tensor(input_ids)\n",
    "        segment_ids = paddle.to_tensor(segment_ids)\n",
    "        logits = model(input_ids, segment_ids)\n",
    "        probs = F.softmax(logits, axis=1)\n",
    "        idx = paddle.argmax(probs, axis=1).numpy()\n",
    "        idx = idx.tolist()\n",
    "        labels = [label_map[i] for i in idx]\n",
    "        results.extend(labels)\n",
    "    return results\n",
    "\n",
    "\n",
    "model = SkepForSequenceClassification.from_pretrained(pretrained_model_name_or_path=\"skep_ernie_1.0_large_ch\", num_classes=19)\n",
    "tokenizer = SkepTokenizer.from_pretrained(pretrained_model_name_or_path=\"skep_ernie_1.0_large_ch\")\n",
    "\n",
    "batch_size=1\n",
    "params_path = '../model_state.pdparams'\n",
    "vocab_path='skep_ckpt/model_800/vocab.txt'\n",
    "\n",
    "if params_path and os.path.isfile(params_path):\n",
    "    # load\n",
    "    state_dict = paddle.load(params_path)\n",
    "    model.set_dict(state_dict)\n",
    "    # tokenizer.load_vocabulary(vocab_path)\n",
    "    print(\"Loaded parameters from %s\" % params_path)\n",
    "\n",
    "print('init done')\n",
    "ch_txt=\"哎我晚上一直睡不着怎么办呀\"\n",
    "data=[{'text':str(ch_txt),'qid':''}]\n",
    "# data = [\n",
    "#     {\"text\":'我担心我和同学和舍友的关系','qid':''},\n",
    "#     {\"text\":'我挺开心的没事 哈哈哈啊哈','qid':''},\n",
    "#     {\"text\":'我女朋友对我太坏了，我被那个坏女人伤透了心','qid':''},\n",
    "#     {\"text\":'太难了学不下去了','qid':''},\n",
    "#      {\"text\":'哎我晚上一直睡不着怎么办呀','qid':''},\n",
    "# ]\n",
    "\n",
    "label_map = {'低自尊': 0, '其他': 1, '失眠': 2, '情感关系问题': 3, '青春期问题': 4, '压力': 5, '自我探索': 6, '家庭问题和矛盾': 7, '强迫症': 8, '人际关系': 9, '事业和工作烦恼': 10,\n",
    " '学业烦恼、对未来规划的迷茫': 11, '离婚': 12, '分手': 13, '物质滥用': 14, '悲恸': 15, '性问题': 16, 'LGBT': 17, '亲子关系': 18, '尚未到达S2': 19, '忧郁症': 20, \n",
    " '焦虑症': 21, '其他疾病': 22, '躁郁症': 23, '创伤后应激反应': 24, '恐慌症': 25, '厌食症和暴食症': 26, '进行的人身伤害': 27, '策划及逆行的自杀行为': 28, '自残': 29, '无伤害身体倾向': 30, '计划的人身伤害': 31}\n",
    "label_map = {value:key for key,value in label_map.items()}\n",
    "# print(label_map)\n",
    "\n",
    "results = predict(\n",
    "    model, data, tokenizer, label_map, batch_size=batch_size)\n",
    "# for idx, text in enumerate(data):\n",
    "#     print('Data: {} \\t Label: {}'.format(text, results[idx]))\n",
    "ans=str(results[0])\n",
    "if(ans==\"其他\"):\n",
    "    ans=\"你看起来还行呀 没事的 明天会更好\"\n",
    "else:\n",
    "    ans=\"你可能有点关于\"+ans+\"的问题,你可以查看下这个网站来获取更多信息 https://zh.wikihow.com/wikiHowTo?search=\"+ans\n",
    "print(ans)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
